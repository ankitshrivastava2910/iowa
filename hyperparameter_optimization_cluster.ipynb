{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning with XGBoost, Ray Tune, Hyperopt and Optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "\n",
    "In this post we are going to demonstrate how we can speed up hyperparameter tuning with:\n",
    "\n",
    "1) Bayesian optimization tuning algos like HyperOpt and Optuna, running on…\n",
    "\n",
    "2) the [Ray](https://ray.io/) distributed ML framework, with a [unified API to many hyperparameter search algos](https://medium.com/riselab/cutting-edge-hyperparameter-tuning-with-ray-tune-be6c0447afdf) and…\n",
    "\n",
    "3) a distributed cluster of cloud instances for even more speedup.\n",
    "\n",
    "### Outline:\n",
    "- Overview of hyperparameter tuning\n",
    "- Baseline linear regression with no hyperparameters\n",
    "- ElasticNet with L1 and L2 regularization using ElasticNetCV hyperparameter optimization\n",
    "- ElasticNet with GridSearchCV hyperparameter optimization\n",
    "- XGBoost: sequential grid search over hyperparameter subsets with early stopping \n",
    "- XGBoost: with HyperOpt and Optuna search algorithms\n",
    "- LightGBM: with HyperOpt and Optuna search algorithms\n",
    "- XGBoost: HyperOpt on a Ray cluster\n",
    "- LightGBM: HyperOpt on a Ray cluster\n",
    "- Conclusions\n",
    "\n",
    "But first, here are results on the Ames housing data set, predicting Iowa home prices:\n",
    "\n",
    "| ML Algo           | Hyperparameter search algo   | CV Error (RMSE in $)  | Time     |\n",
    "|-------------------|------------------------------|-----------------------|----------|\n",
    "| XGB               | Sequential Grid Search       | $18783                |   36:09  |\n",
    "| XGB               | HyperOpt (128 samples)       | $18770                |   14:08  |\n",
    "| LightGBM          | HyperOpt (128 samples)       | $18770                |   14:08  |\n",
    "| XGB               | Optuna                       | $18618\n",
    "| LightGBM          | Optuna                       | $18618\n",
    "| XGB               | Optuna - 16-instance cluster | $18618\n",
    "| LightGBM          | Optuna - 16-instance cluster | $18618\n",
    "| Linear Regression | --                           | $18192                |   0:01s  |\n",
    "| ElasticNet        | ElasticNetCV (Grid Search)   | $18122                |   0:02s  |          \n",
    "| ElasticNet        | GridSearchCV                 | $18061                |   0:05s  |          \n",
    "\n",
    "We see both speedup and RMSE improvement when using HyperOpt and Optuna, and the cluster. But our feature engineering was quite good and our simple linear model still outperforms boosting. (Not shown, SVR and KR are high-performing and an ensemble improves over all individual algos)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning Overview\n",
    "\n",
    "Here are [the principal approaches to hyperparameter tuning](https://en.wikipedia.org/wiki/Hyperparameter_optimization)\n",
    "\n",
    "- Grid search: given a finite set of discrete values for each hyperparameter, exhaustively cross-validate all combinations\n",
    "\n",
    "- Random search: given a discrete or continuous distribution for each hyperparameter, randomly sample from the joint distribution. Generally [more efficient than exhaustive grid search.](https://dl.acm.org/doi/10.5555/2188385.2188395 ) \n",
    "\n",
    "- Bayesian optimization: update the search space as you go based on outcomes of prior searches.\n",
    "\n",
    "- Gradient-based optimization: attempt to estimate the gradient of the CV metric with respect to the hyperparameter and ascend/descend the gradient.\n",
    "\n",
    "- Evolutionary optimization: sample the search space, discard combinations with poor metrics, and genetically evolve new combinations to try based on the successful combinations.\n",
    "\n",
    "- Population-based: A method of performing hyperparameter optimization at the same time as training.\n",
    "\n",
    "In this post we focus on Bayesian optimization with HyperOpt and Optuna. What is Bayesian optimization? When we perform a grid search, the search space can be considered a prior belief that the best hyperparameter vector is in the search space, and the combinations have equal probability of being the best combination. So we try them all and pick the best one.\n",
    "\n",
    "Perhaps we might do two passes of grid search. After an initial search on a broad, coarsely spaced grid, we might do a deeper dive in a smaller area around the best metric from the first pass, with a more finely-spaced grid. In Bayesian terminology we updated our prior belief.\n",
    "\n",
    "Bayesian optimization first samples randomly, e.g. 30 combinations, and computes the cross-validation metric for each combination. Then the algorithm updates the distribution it samples from, so it is more likely to sample combinations near the good metrics, and less likely to sample combinations near the poor metrics. As it continues to sample, it continues to update the search distribution based on the metrics it finds.\n",
    "\n",
    "Early stopping may also highly beneficial: often we can discard a combination without fully training it. In this post we use [ASHA](https://arxiv.org/abs/1810.05934). \n",
    "\n",
    "We use 4 regression algorithms:\n",
    "- LinearRegression: baseline with no hyperparameters\n",
    "- ElasticNet: Linear regression with L1 and L2 regularization (2 hyperparameters).\n",
    "- XGBoost\n",
    "- LightGBM\n",
    "\n",
    "We use 5 approaches :\n",
    "- *Native CV*: In sklearn if an algo has hyperparameters it will often have an xxxCV version which performs automated hyperparameter tuning over a search space with specified kfolds.\n",
    "- *GridSearchCV*: Abstracts CV for any sklearn algo, running multithreaded trials over specified folds. \n",
    "- *Manual sequential grid search*: What we typically do with XGBoost, which doesn't play well with GridSearchCV and has too many hyperparameters to tune in one pass.\n",
    "- *Ray on local machine*: HyperOpt and Optuna with early stopping.\n",
    "- *Ray on cluster*: Additionally scale out to run a single hyperparameter optimization task over many instances.\n",
    "\n",
    "We use data from the Ames Housing Dataset https://www.kaggle.com/c/house-prices-advanced-regression-techniques . The original data has 79 raw features. The data we will use has 100 features with a fair amount of feature engineering from [my own attempt at modeling](https://github.com/druce/iowa), which was in the top 5% or so when I submitted it to Kaggle.\n",
    "\n",
    "### Further reading: \n",
    " - [Hyper-Parameter Optimization: A Review of Algorithms and Applications](https://arxiv.org/abs/2003.05689) Tong Yu, Hong Zhu (2020)\n",
    " - [Hyperparameter Search in Machine Learning](https://arxiv.org/abs/1502.02127v2), Marc Claesen, Bart De Moor (2015)\n",
    " - [Hyperparameter Optimization](https://link.springer.com/chapter/10.1007/978-3-030-05318-5_1), Matthias Feurer, Frank Hutter (2019) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-18 16:25:23.231900\n",
      "numpy                1.19.1\n",
      "pandas               1.1.3\n",
      "sklearn              0.23.2\n",
      "xgboost              1.2.0\n",
      "lightgbm             2.3.0\n",
      "ray                  1.0.0\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import random\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression, ElasticNet, ElasticNetCV, Ridge, RidgeCV\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, GridSearchCV, KFold\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "#!conda install -y -c conda-forge  xgboost \n",
    "import xgboost\n",
    "from xgboost import XGBRegressor\n",
    "from xgboost import plot_importance\n",
    "\n",
    "import lightgbm\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.suggest import ConcurrencyLimiter\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "from ray.tune.suggest.bayesopt import BayesOptSearch\n",
    "from ray.tune.suggest.hyperopt import HyperOptSearch\n",
    "from ray.tune.suggest.optuna import OptunaSearch\n",
    "from ray.tune.logger import DEFAULT_LOGGERS\n",
    "from ray.tune.integration.wandb import WandbLogger\n",
    "\n",
    "# pip install hyperopt\n",
    "# pip install optuna\n",
    "\n",
    "# import wandb\n",
    "# os.environ['WANDB_NOTEBOOK_NAME']='hyperparameter_optimization.ipynb'\n",
    "\n",
    "print(datetime.now())\n",
    "\n",
    "print (\"%-20s %s\"% (\"numpy\", np.__version__))\n",
    "print (\"%-20s %s\"% (\"pandas\", pd.__version__))\n",
    "print (\"%-20s %s\"% (\"sklearn\", sklearn.__version__))\n",
    "print (\"%-20s %s\"% (\"xgboost\", xgboost.__version__))\n",
    "print (\"%-20s %s\"% (\"lightgbm\", lightgbm.__version__))\n",
    "print (\"%-20s %s\"% (\"ray\", ray.__version__))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed for reproducibility\n",
    "RANDOMSTATE = 42\n",
    "np.random.seed(RANDOMSTATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>BsmtFullBath</th>\n",
       "      <th>FullBath</th>\n",
       "      <th>KitchenAbvGr</th>\n",
       "      <th>GarageYrBlt</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>1stFlrSF</th>\n",
       "      <th>GrLivArea</th>\n",
       "      <th>GarageArea</th>\n",
       "      <th>...</th>\n",
       "      <th>SaleCondition_1</th>\n",
       "      <th>SaleCondition_2</th>\n",
       "      <th>SaleCondition_5</th>\n",
       "      <th>SaleType_4</th>\n",
       "      <th>BedroomAbvGr_1</th>\n",
       "      <th>BedroomAbvGr_4</th>\n",
       "      <th>BedroomAbvGr_5</th>\n",
       "      <th>HalfBath_1</th>\n",
       "      <th>TotalBath_1.0</th>\n",
       "      <th>TotalBath_2.5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>65.0</td>\n",
       "      <td>196.0</td>\n",
       "      <td>856</td>\n",
       "      <td>1710</td>\n",
       "      <td>548.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1262</td>\n",
       "      <td>1262</td>\n",
       "      <td>460.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>68.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>920</td>\n",
       "      <td>1786</td>\n",
       "      <td>608.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>95</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>961</td>\n",
       "      <td>1717</td>\n",
       "      <td>642.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>84.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>1145</td>\n",
       "      <td>2198</td>\n",
       "      <td>836.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    YearBuilt  BsmtFullBath  FullBath  KitchenAbvGr  GarageYrBlt  LotFrontage  \\\n",
       "Id                                                                              \n",
       "1           7             1         2             1            7         65.0   \n",
       "2          34             0         2             1           34         80.0   \n",
       "3           9             1         2             1            9         68.0   \n",
       "4          95             1         1             1           12         60.0   \n",
       "5          10             1         2             1           10         84.0   \n",
       "\n",
       "    MasVnrArea  1stFlrSF  GrLivArea  GarageArea  ...  SaleCondition_1  \\\n",
       "Id                                               ...                    \n",
       "1        196.0       856       1710       548.0  ...                0   \n",
       "2          0.0      1262       1262       460.0  ...                0   \n",
       "3        162.0       920       1786       608.0  ...                0   \n",
       "4          0.0       961       1717       642.0  ...                1   \n",
       "5        350.0      1145       2198       836.0  ...                0   \n",
       "\n",
       "    SaleCondition_2  SaleCondition_5  SaleType_4  BedroomAbvGr_1  \\\n",
       "Id                                                                 \n",
       "1                 0                0           1               0   \n",
       "2                 0                0           1               0   \n",
       "3                 0                0           1               0   \n",
       "4                 0                0           1               0   \n",
       "5                 0                0           1               0   \n",
       "\n",
       "    BedroomAbvGr_4  BedroomAbvGr_5  HalfBath_1  TotalBath_1.0  TotalBath_2.5  \n",
       "Id                                                                            \n",
       "1                0               0           1              0              0  \n",
       "2                0               0           0              0              1  \n",
       "3                0               0           1              0              0  \n",
       "4                0               0           0              0              0  \n",
       "5                1               0           1              0              0  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.247699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.109016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12.317171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11.849405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12.429220</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    SalePrice\n",
       "Id           \n",
       "1   12.247699\n",
       "2   12.109016\n",
       "3   12.317171\n",
       "4   11.849405\n",
       "5   12.429220"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import train data\n",
    "df = pd.read_pickle('df_train.pickle')\n",
    "\n",
    "response = 'SalePrice'\n",
    "predictors = ['YearBuilt',\n",
    "              'BsmtFullBath',\n",
    "              'FullBath',\n",
    "              'KitchenAbvGr',\n",
    "              'GarageYrBlt',\n",
    "              'LotFrontage',\n",
    "              'MasVnrArea',\n",
    "              '1stFlrSF',\n",
    "              'GrLivArea',\n",
    "              'GarageArea',\n",
    "              'WoodDeckSF',\n",
    "              'PorchSF',\n",
    "              'AvgBltRemod',\n",
    "              'FireBathRatio',\n",
    "              'TotalSF x OverallQual x OverallCond',\n",
    "              'AvgBltRemod x Functional x TotalFinSF',\n",
    "              'Functional x OverallQual',\n",
    "              'KitchenAbvGr x KitchenQual',\n",
    "              'GarageCars x GarageYrBlt',\n",
    "              'GarageQual x GarageCond x GarageCars',\n",
    "              'HeatingQC x Heating',\n",
    "              'monthnum',\n",
    "              'log_YearBuilt',\n",
    "              'log_LotArea',\n",
    "              'log_TotalFinSF',\n",
    "              'log_GarageRatio',\n",
    "              'log_TotalSF x OverallQual x OverallCond',\n",
    "              'log_TotalSF x OverallCond',\n",
    "              'log_AvgBltRemod x TotalFinSF',\n",
    "              'sq_2ndFlrSF',\n",
    "              'sq_BsmtFinSF',\n",
    "              'sq_BsmtFinSF x BsmtQual',\n",
    "              'sq_BsmtFinSF x BsmtBath',\n",
    "              'BldgType_4',\n",
    "              'BsmtExposure_1',\n",
    "              'BsmtExposure_4',\n",
    "              'BsmtFinType1_1',\n",
    "              'BsmtFinType1_2',\n",
    "              'BsmtFinType1_4',\n",
    "              'BsmtFinType1_5',\n",
    "              'BsmtFinType1_6',\n",
    "              'CentralAir_0',\n",
    "              'CentralAir_1',\n",
    "              'Condition1_1',\n",
    "              'Condition1_3',\n",
    "              'ExterCond_2',\n",
    "              'ExterQual_2',\n",
    "              'Exterior1st_4',\n",
    "              'Exterior1st_5',\n",
    "              'Exterior1st_10',\n",
    "              'Fence_0',\n",
    "              'Fence_2',\n",
    "              'Foundation_1',\n",
    "              'Foundation_5',\n",
    "              'GarageCars_1',\n",
    "              'GarageFinish_2',\n",
    "              'GarageFinish_3',\n",
    "              'GarageType_2',\n",
    "              'HouseStyle_2',\n",
    "              'KitchenQual_4',\n",
    "              'LotConfig_0',\n",
    "              'LotConfig_4',\n",
    "              'MSSubClass_30',\n",
    "              'MSSubClass_70',\n",
    "              'MSZoning_0',\n",
    "              'MSZoning_1',\n",
    "              'MSZoning_4',\n",
    "              'MasVnrType_2',\n",
    "              'MasVnrType_3',\n",
    "              'MoSold_1',\n",
    "              'MoSold_5',\n",
    "              'MoSold_6',\n",
    "              'MoSold_11',\n",
    "              'Neighborhood_3',\n",
    "              'Neighborhood_4',\n",
    "              'Neighborhood_5',\n",
    "              'Neighborhood_10',\n",
    "              'Neighborhood_11',\n",
    "              'Neighborhood_16',\n",
    "              'Neighborhood_17',\n",
    "              'Neighborhood_19',\n",
    "              'Neighborhood_22',\n",
    "              'Neighborhood_24',\n",
    "              'OverallCond_7',\n",
    "              'OverallQual_5',\n",
    "              'OverallQual_6',\n",
    "              'OverallQual_7',\n",
    "              'OverallQual_9',\n",
    "              'PavedDrive_0',\n",
    "              'PavedDrive_2',\n",
    "              'SaleCondition_1',\n",
    "              'SaleCondition_2',\n",
    "              'SaleCondition_5',\n",
    "              'SaleType_4',\n",
    "              'BedroomAbvGr_1',\n",
    "              'BedroomAbvGr_4',\n",
    "              'BedroomAbvGr_5',\n",
    "              'HalfBath_1',\n",
    "              'TotalBath_1.0',\n",
    "              'TotalBath_2.5']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, df[response], test_size=.25)\n",
    "\n",
    "display(df[predictors].head())\n",
    "display(df[[response]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are training on a response which is the log of 1 + the sale price\n",
    "# transform prediction back to original basis with expm1 and evaluate vs. original\n",
    "\n",
    "MEAN_RESPONSE=df[response].mean()\n",
    "def cv_to_raw(cv_val, mean_response=MEAN_RESPONSE):\n",
    "    \"\"\"convert log1p rmse to underlying SalePrice error\"\"\"\n",
    "    # MEAN_RESPONSE assumes folds have same mean response, which is true in expectation but not in each fold\n",
    "    # we can also pass the actual response for each fold\n",
    "    # but we're usually looking to consistently convert the log value to a more meaningful unit\n",
    "    return np.expm1(mean_response+cv_val) - np.expm1(mean_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# always use same k-folds for reproducibility\n",
    "kfolds = KFold(n_splits=10, shuffle=True, random_state=RANDOMSTATE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ray Cluster\n",
    "\n",
    "- Cluster config is in `ray1.1.yaml`\n",
    "- Edit `ray1.1.yaml` file with your region, availability zone, subnet, imageid information\n",
    "    - to get those variables launch the latest Deep Learning AMI (Ubuntu 18.04) Version 35.0 into a small instance in your favorite region/zone\n",
    "    - test that it works\n",
    "    - note those 4 variables: region, availability zone, subnet, AMI imageid\n",
    "    - terminate the instance and edit `ray1.1.yaml` accordingly\n",
    "    - in future you can create your own image with everything pre-installed and specify its AMI imageid, instead of using the generic image and installing everything at launch.\n",
    "- To run the cluster: \n",
    "`ray up ray1.1.yaml`\n",
    "    - Creates head instance using image specified.\n",
    "    - Installs ray and related requirements\n",
    "    - Clones this Iowa repo\n",
    "    - Launches worker nodes per auto-scaling parameters (currently we fix the number of nodes because we're not benching the time the cluster will take to auto-scale)\n",
    "- After cluster starts you can check AWS console and note that several instances launched.\n",
    "- Check `ray monitor ray1.1.yaml` for any error messages\n",
    "- Run Jupyter on the cluster with port forwarding\n",
    " `ray exec ray1.1.yaml --port-forward=8899 'jupyter notebook --port=8899'`\n",
    "- Open the notebook on the generated URL e.g. http://localhost:8899/?token=5f46d4355ae7174524ba71f30ef3f0633a20b19a204b93b4\n",
    "- Make sure to hoose the default kernel to make sure it runs in the conda environment with all installs\n",
    "- Make sure to use the ray.init() command given in the startup messages.\n",
    "- You can also run a terminal on the head node of the cluster with\n",
    " `ray attach /Users/drucev/projects/iowa/ray1.1.yaml`\n",
    "- You can also ssh explicitly with the IP address and the generated private key\n",
    " `ssh -o IdentitiesOnly=yes -i ~/.ssh/ray-autoscaler_1_us-east-1.pem ubuntu@54.161.200.54`\n",
    "- run port forwarding to the Ray dashboard with   \n",
    "`ray dashboard ray1.1.yaml`\n",
    "and then open\n",
    " http://localhost:8265/\n",
    "\n",
    "see https://docs.ray.io/en/latest/cluster/launcher.html for additional info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure local ray service is shutdown\n",
    "ray.shutdown()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-18 16:26:40,752\tINFO worker.py:634 -- Connecting to existing Ray cluster at address: 172.30.1.105:6379\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '172.30.1.105',\n",
       " 'raylet_ip_address': '172.30.1.105',\n",
       " 'redis_address': '172.30.1.105:6379',\n",
       " 'object_store_address': '/tmp/ray/session_2020-10-18_16-14-46_162480_30793/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2020-10-18_16-14-46_162480_30793/sockets/raylet',\n",
       " 'webui_url': 'localhost:8265',\n",
       " 'session_dir': '/tmp/ray/session_2020-10-18_16-14-46_162480_30793',\n",
       " 'metrics_export_port': 53720}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# launch cluster in terminal with ray up ray1.1.yaml\n",
    "# initialize ray on cluster\n",
    "ray.init(address='localhost:6379', _redis_password='5241590000000000')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refactor to give ray.tune a single function of hyperparameters to optimize\n",
    "\n",
    "# @wandb_mixin\n",
    "def my_xgb(config):\n",
    "    \n",
    "    # fix these configs to match calling convention\n",
    "    config['max_depth'] = int(config['max_depth']) + 2   # hyperopt needs left to start at 0 but we want to start at 2\n",
    "#    config['max_leaves'] = int(config['max_leaves'])\n",
    "    config['n_estimators'] = int(config['n_estimators'])   # pass float eg loguniform distribution, use int\n",
    "    \n",
    "    xgb = XGBRegressor(\n",
    "        objective='reg:squarederror',\n",
    "        n_jobs=1,\n",
    "        random_state=RANDOMSTATE,\n",
    "        booster='gbtree',   \n",
    "        scale_pos_weight=1, \n",
    "        **config,\n",
    "    )\n",
    "    scores = -cross_val_score(xgb, df[predictors], df[response],\n",
    "                                      scoring=\"neg_root_mean_squared_error\",\n",
    "                                      cv=kfolds)\n",
    "    rmse = np.mean(scores)\n",
    "    tune.report(rmse=rmse)\n",
    "    # wandb.log({\"rmse\": rmse})\n",
    "    \n",
    "    return {\"rmse\": rmse}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['n_estimators',\n",
       " 'max_depth',\n",
       " 'subsample',\n",
       " 'colsample_bytree',\n",
       " 'colsample_bylevel',\n",
       " 'learning_rate']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_tune_kwargs = {\n",
    "    \"n_estimators\": tune.loguniform(100, 10000),\n",
    "    \"max_depth\": tune.randint(0, 5),\n",
    "#    'max_leaves': tune.loguniform(1, 1000),    \n",
    "    \"subsample\": tune.quniform(0.25, 0.75, 0.01),\n",
    "    \"colsample_bytree\": tune.quniform(0.05, 0.5, 0.01),\n",
    "    \"colsample_bylevel\": tune.quniform(0.05, 0.5, 0.01),    \n",
    "    \"learning_rate\": tune.loguniform(0.001, 0.1),\n",
    "#     \"wandb\": {\n",
    "#         \"project\": \"iowa2\",\n",
    "#        \"api_key_file\": \"secrets/wandb.txt\",\n",
    "#    }    \n",
    "}\n",
    "\n",
    "xgb_tune_params = [k for k in xgb_tune_kwargs.keys() if k != 'wandb']\n",
    "xgb_tune_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 1.3/7.6 GiB<br>Using AsyncHyperBand: num_stopped=36\n",
       "Bracket: Iter 64.000: None | Iter 16.000: None | Iter 4.000: None | Iter 1.000: -0.1097674763107903<br>Resources requested: 31/32 CPUs, 0/0 GPUs, 0.0/80.03 GiB heap, 0.0/23.44 GiB objects<br>Result logdir: /home/ubuntu/ray_results/hyperopt_xgb<br>Number of trials: 1024 (935 PENDING, 31 RUNNING, 58 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name     </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  colsample_bylevel</th><th style=\"text-align: right;\">  colsample_bytree</th><th style=\"text-align: right;\">  learning_rate</th><th style=\"text-align: right;\">  max_depth</th><th style=\"text-align: right;\">  n_estimators</th><th style=\"text-align: right;\">  subsample</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    rmse</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>my_xgb_1e07187e</td><td>PENDING   </td><td>     </td><td style=\"text-align: right;\">               0.1 </td><td style=\"text-align: right;\">              0.08</td><td style=\"text-align: right;\">     0.0554069 </td><td style=\"text-align: right;\">          4</td><td style=\"text-align: right;\">       115.954</td><td style=\"text-align: right;\">       0.57</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td></tr>\n",
       "<tr><td>my_xgb_1e0c4a92</td><td>PENDING   </td><td>     </td><td style=\"text-align: right;\">               0.06</td><td style=\"text-align: right;\">              0.25</td><td style=\"text-align: right;\">     0.00426332</td><td style=\"text-align: right;\">          3</td><td style=\"text-align: right;\">      1679.06 </td><td style=\"text-align: right;\">       0.47</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td></tr>\n",
       "<tr><td>my_xgb_1e13431a</td><td>PENDING   </td><td>     </td><td style=\"text-align: right;\">               0.16</td><td style=\"text-align: right;\">              0.3 </td><td style=\"text-align: right;\">     0.0148267 </td><td style=\"text-align: right;\">          0</td><td style=\"text-align: right;\">      8535.26 </td><td style=\"text-align: right;\">       0.49</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td></tr>\n",
       "<tr><td>my_xgb_1e184126</td><td>PENDING   </td><td>     </td><td style=\"text-align: right;\">               0.12</td><td style=\"text-align: right;\">              0.45</td><td style=\"text-align: right;\">     0.0365982 </td><td style=\"text-align: right;\">          0</td><td style=\"text-align: right;\">      2841.64 </td><td style=\"text-align: right;\">       0.43</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td></tr>\n",
       "<tr><td>my_xgb_1e1d8852</td><td>PENDING   </td><td>     </td><td style=\"text-align: right;\">               0.08</td><td style=\"text-align: right;\">              0.48</td><td style=\"text-align: right;\">     0.0782704 </td><td style=\"text-align: right;\">          0</td><td style=\"text-align: right;\">      7205.77 </td><td style=\"text-align: right;\">       0.4 </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td></tr>\n",
       "<tr><td>my_xgb_1e246c3a</td><td>PENDING   </td><td>     </td><td style=\"text-align: right;\">               0.12</td><td style=\"text-align: right;\">              0.5 </td><td style=\"text-align: right;\">     0.0988203 </td><td style=\"text-align: right;\">          0</td><td style=\"text-align: right;\">      5321.66 </td><td style=\"text-align: right;\">       0.33</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td></tr>\n",
       "<tr><td>my_xgb_1e2a4114</td><td>PENDING   </td><td>     </td><td style=\"text-align: right;\">               0.35</td><td style=\"text-align: right;\">              0.23</td><td style=\"text-align: right;\">     0.0539004 </td><td style=\"text-align: right;\">          0</td><td style=\"text-align: right;\">      1122.9  </td><td style=\"text-align: right;\">       0.4 </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td></tr>\n",
       "<tr><td>my_xgb_1c868dd6</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">               0.08</td><td style=\"text-align: right;\">              0.44</td><td style=\"text-align: right;\">     0.0798818 </td><td style=\"text-align: right;\">          1</td><td style=\"text-align: right;\">      5842.93 </td><td style=\"text-align: right;\">       0.67</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td></tr>\n",
       "<tr><td>my_xgb_1c8ec3de</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">               0.49</td><td style=\"text-align: right;\">              0.3 </td><td style=\"text-align: right;\">     0.0400602 </td><td style=\"text-align: right;\">          2</td><td style=\"text-align: right;\">      7563.79 </td><td style=\"text-align: right;\">       0.62</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td></tr>\n",
       "<tr><td>my_xgb_1c9dd7ca</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">               0.27</td><td style=\"text-align: right;\">              0.33</td><td style=\"text-align: right;\">     0.0388854 </td><td style=\"text-align: right;\">          0</td><td style=\"text-align: right;\">      6949.66 </td><td style=\"text-align: right;\">       0.57</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td></tr>\n",
       "<tr><td>my_xgb_1cb15a16</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">               0.3 </td><td style=\"text-align: right;\">              0.17</td><td style=\"text-align: right;\">     0.0101945 </td><td style=\"text-align: right;\">          0</td><td style=\"text-align: right;\">      9229.48 </td><td style=\"text-align: right;\">       0.41</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td></tr>\n",
       "<tr><td>my_xgb_1cbce5c0</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">               0.46</td><td style=\"text-align: right;\">              0.17</td><td style=\"text-align: right;\">     0.0078466 </td><td style=\"text-align: right;\">          3</td><td style=\"text-align: right;\">      9205.81 </td><td style=\"text-align: right;\">       0.4 </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td></tr>\n",
       "<tr><td>my_xgb_1ce3dd60</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">               0.41</td><td style=\"text-align: right;\">              0.26</td><td style=\"text-align: right;\">     0.00467664</td><td style=\"text-align: right;\">          3</td><td style=\"text-align: right;\">      4844.64 </td><td style=\"text-align: right;\">       0.48</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td></tr>\n",
       "<tr><td>my_xgb_1ce91744</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">               0.14</td><td style=\"text-align: right;\">              0.49</td><td style=\"text-align: right;\">     0.0224775 </td><td style=\"text-align: right;\">          0</td><td style=\"text-align: right;\">      9510.88 </td><td style=\"text-align: right;\">       0.44</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td></tr>\n",
       "<tr><td>my_xgb_1c8096c4</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">               0.36</td><td style=\"text-align: right;\">              0.16</td><td style=\"text-align: right;\">     0.00149561</td><td style=\"text-align: right;\">          4</td><td style=\"text-align: right;\">      4626.62 </td><td style=\"text-align: right;\">       0.48</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        70.5378 </td><td style=\"text-align: right;\">0.115363</td></tr>\n",
       "<tr><td>my_xgb_1c831f2a</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">               0.07</td><td style=\"text-align: right;\">              0.47</td><td style=\"text-align: right;\">     0.00455539</td><td style=\"text-align: right;\">          3</td><td style=\"text-align: right;\">      2853.98 </td><td style=\"text-align: right;\">       0.46</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        39.1695 </td><td style=\"text-align: right;\">0.107704</td></tr>\n",
       "<tr><td>my_xgb_1c89e756</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">               0.09</td><td style=\"text-align: right;\">              0.11</td><td style=\"text-align: right;\">     0.00110121</td><td style=\"text-align: right;\">          2</td><td style=\"text-align: right;\">      6877.73 </td><td style=\"text-align: right;\">       0.47</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        82.051  </td><td style=\"text-align: right;\">0.149682</td></tr>\n",
       "<tr><td>my_xgb_1c8b7864</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">               0.09</td><td style=\"text-align: right;\">              0.29</td><td style=\"text-align: right;\">     0.0403208 </td><td style=\"text-align: right;\">          3</td><td style=\"text-align: right;\">       220.67 </td><td style=\"text-align: right;\">       0.66</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.46888</td><td style=\"text-align: right;\">0.116273</td></tr>\n",
       "<tr><td>my_xgb_1c8d0f6c</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">               0.37</td><td style=\"text-align: right;\">              0.46</td><td style=\"text-align: right;\">     0.0270039 </td><td style=\"text-align: right;\">          1</td><td style=\"text-align: right;\">       751.724</td><td style=\"text-align: right;\">       0.65</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        15.3014 </td><td style=\"text-align: right;\">0.109189</td></tr>\n",
       "<tr><td>my_xgb_1c905618</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">               0.22</td><td style=\"text-align: right;\">              0.35</td><td style=\"text-align: right;\">     0.0027847 </td><td style=\"text-align: right;\">          0</td><td style=\"text-align: right;\">      3554.09 </td><td style=\"text-align: right;\">       0.56</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        42.6324 </td><td style=\"text-align: right;\">0.115962</td></tr>\n",
       "<tr><td>my_xgb_1c91f0ae</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">               0.07</td><td style=\"text-align: right;\">              0.2 </td><td style=\"text-align: right;\">     0.0473492 </td><td style=\"text-align: right;\">          4</td><td style=\"text-align: right;\">      6918.6  </td><td style=\"text-align: right;\">       0.52</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        83.881  </td><td style=\"text-align: right;\">0.108683</td></tr>\n",
       "</tbody>\n",
       "</table><br>... 1004 more trials not shown (928 PENDING, 24 RUNNING, 51 TERMINATED)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "NUM_SAMPLES=1024\n",
    "\n",
    "start_time = datetime.now()\n",
    "print(\"%-20s %s\" % (\"Start Time\", start_time))\n",
    "\n",
    "algo = HyperOptSearch(random_state_seed=RANDOMSTATE)\n",
    "# to limit number of cores, uncomment and set max_concurrent \n",
    "# algo = ConcurrencyLimiter(algo, max_concurrent=10)\n",
    "scheduler = AsyncHyperBandScheduler()\n",
    "\n",
    "analysis = tune.run(my_xgb,\n",
    "                    num_samples=NUM_SAMPLES,\n",
    "                    config=xgb_tune_kwargs,                    \n",
    "                    name=\"hyperopt_xgb\",\n",
    "                    metric=\"rmse\",\n",
    "                    mode=\"min\",\n",
    "                    search_alg=algo,\n",
    "                    scheduler=scheduler,\n",
    "                    verbose=1,\n",
    "#                    loggers=DEFAULT_LOGGERS + (WandbLogger, ),\n",
    "                   )\n",
    "\n",
    "end_time = datetime.now()\n",
    "print(\"%-20s %s\" % (\"Start Time\", start_time))\n",
    "print(\"%-20s %s\" % (\"End Time\", end_time))\n",
    "print(str(timedelta(seconds=(end_time-start_time).seconds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_cols = ['config.' + k for k in xgb_tune_params]\n",
    "analysis_results_df = analysis.results_df[['rmse', 'date', 'time_this_iter_s'] + param_cols].sort_values('rmse')\n",
    "analysis_results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_config = {z: analysis_results_df.iloc[0]['config.' + z] for z in xgb_tune_params}\n",
    "\n",
    "xgb = XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    random_state=RANDOMSTATE,    \n",
    "    verbosity=1,\n",
    "    n_jobs=-1,\n",
    "    **best_config\n",
    ")\n",
    "print(xgb)\n",
    "\n",
    "scores = -cross_val_score(xgb, df[predictors], df[response],\n",
    "                          scoring=\"neg_root_mean_squared_error\",\n",
    "                          cv=kfolds)\n",
    "\n",
    "raw_scores = [cv_to_raw(x) for x in scores]\n",
    "print()\n",
    "print(\"Log1p CV RMSE %.06f (STD %.04f)\" % (np.mean(scores), np.std(scores)))\n",
    "print(\"Raw CV RMSE %.0f (STD %.0f)\" % (np.mean(raw_scores), np.std(raw_scores)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_tune_kwargs = {\n",
    "    \"n_estimators\": tune.loguniform(100, 10000),\n",
    "    \"max_depth\": tune.randint(0, 5),\n",
    "    'num_leaves': tune.loguniform(2, 1000),               # max_leaves\n",
    "    \"bagging_fraction\": tune.quniform(0.5, 0.8, 0.01),    # subsample\n",
    "    \"feature_fraction\": tune.quniform(0.05, 0.5, 0.01),   # colsample_bytree\n",
    "    \"learning_rate\": tune.loguniform(0.001, 0.1),\n",
    "#     \"wandb\": {\n",
    "#         \"project\": \"iowa\",\n",
    "#     }        \n",
    "}\n",
    "\n",
    "#print(\"wandb name:\", lgbm_tune_kwargs['wandb']['name'])\n",
    "lgbm_tune_params = [k for k in lgbm_tune_kwargs.keys() if k != 'wandb']\n",
    "print(lgbm_tune_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_lgbm(config):\n",
    "    \n",
    "    # fix these configs \n",
    "    config['n_estimators'] = int(config['n_estimators'])   # pass float eg loguniform distribution, use int\n",
    "    config['num_leaves'] = int(config['num_leaves'])\n",
    "    \n",
    "    lgbm = LGBMRegressor(objective='regression',\n",
    "                         max_bin=200,\n",
    "                         feature_fraction_seed=7,\n",
    "                         min_data_in_leaf=2,\n",
    "                         verbose=-1,\n",
    "                         n_jobs=1,\n",
    "                         # these are specified to suppress warnings\n",
    "                         colsample_bytree=None,\n",
    "                         min_child_samples=None,\n",
    "                         subsample=None,\n",
    "                         **config,\n",
    "                         # early stopping params, maybe in fit\n",
    "                         #early_stopping_rounds=early_stopping_rounds,\n",
    "                         #valid_sets=[xgtrain, xgvalid], valid_names=['train','valid'], evals_result=evals_results\n",
    "                         #num_boost_round=num_boost_round,\n",
    "                         )\n",
    "    \n",
    "    scores = -cross_val_score(lgbm, df[predictors], df[response],\n",
    "                              scoring=\"neg_root_mean_squared_error\",\n",
    "                              cv=kfolds)\n",
    "    rmse=np.mean(scores)  \n",
    "    tune.report(rmse=rmse)\n",
    "    # wandb.log({\"rmse\": rmse})\n",
    "    \n",
    "    return {'rmse': np.mean(scores)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tune LightGBM\n",
    "print(\"LightGBM\")\n",
    "\n",
    "NUM_SAMPLES=256\n",
    "\n",
    "start_time = datetime.now()\n",
    "print(\"%-20s %s\" % (\"Start Time\", start_time))\n",
    "\n",
    "algo = HyperOptSearch(random_state_seed=RANDOMSTATE)\n",
    "# uncomment and set max_concurrent to limit number of cores\n",
    "# algo = ConcurrencyLimiter(algo, max_concurrent=10)\n",
    "scheduler = AsyncHyperBandScheduler()\n",
    "\n",
    "# lgbm_tune_kwargs['wandb']['name'] = 'hyperopt_' + xgb_tune_kwargs['wandb']['name']\n",
    "\n",
    "analysis = tune.run(my_lgbm,\n",
    "                    num_samples=NUM_SAMPLES,\n",
    "                    config = lgbm_tune_kwargs,\n",
    "                    name=\"hyperopt_lgbm\",\n",
    "                    metric=\"rmse\",\n",
    "                    mode=\"min\",\n",
    "                    search_alg=algo,\n",
    "                    scheduler=scheduler,\n",
    "                    verbose=1,\n",
    "#                     loggers=DEFAULT_LOGGERS + (WandbLogger, ),\n",
    "                   )\n",
    "\n",
    "end_time = datetime.now()\n",
    "print(\"%-20s %s\" % (\"Start Time\", start_time))\n",
    "print(\"%-20s %s\" % (\"End Time\", end_time))\n",
    "print(str(timedelta(seconds=(end_time-start_time).seconds)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_cols = ['config.' + k for k in lgbm_tune_params]\n",
    "analysis_results_df = analysis.results_df[['rmse', 'date', 'time_this_iter_s'] + param_cols].sort_values('rmse')\n",
    "analysis_results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_config = {z: analysis_results_df.iloc[0]['config.' + z] for z in lgbm_tune_params}\n",
    "\n",
    "lgbm = LGBMRegressor(objective='regression',\n",
    "                     max_bin=200,\n",
    "                     feature_fraction_seed=7,\n",
    "                     min_data_in_leaf=2,\n",
    "                     verbose=-1,\n",
    "                     **best_config,\n",
    "                     # early stopping params, maybe in fit\n",
    "                     #early_stopping_rounds=early_stopping_rounds,\n",
    "                     #valid_sets=[xgtrain, xgvalid], valid_names=['train','valid'], evals_result=evals_results\n",
    "                     #num_boost_round=num_boost_round,\n",
    "                     )\n",
    " \n",
    "print(lgbm)\n",
    "\n",
    "scores = -cross_val_score(lgbm, df[predictors], df[response],\n",
    "                          scoring=\"neg_root_mean_squared_error\",\n",
    "                          cv=kfolds)\n",
    "\n",
    "raw_scores = [cv_to_raw(x) for x in scores]\n",
    "print()\n",
    "print(\"Log1p CV RMSE %.06f (STD %.04f)\" % (np.mean(scores), np.std(scores)))\n",
    "print(\"Raw CV RMSE %.0f (STD %.0f)\" % (np.mean(raw_scores), np.std(raw_scores)))\n",
    "raw_scores = [cv_to_raw(x) for x in scores]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_anaconda3)",
   "language": "python",
   "name": "conda_anaconda3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
