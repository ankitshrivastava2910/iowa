{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning with XGBoost, Ray Tune, Hyperopt and BayesOpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complex machine learning algorithms like XGBoost, LightGBM, neural networks have many tuning parameters, so finding the most appropriate combination via a grid search is time-consuming. This is an example of speeding hyperparameter tuning using Ray Tune, advanced search algorithms, and clusters to significantly accelerate tuning.\n",
    "\n",
    "Outline: I will use this Housing Prices Competition for Kaggle Learn Users: https://www.kaggle.com/c/house-prices-advanced-regression-techniques . The response we are predicting is the log-transformed SalePrice based on house features like square feet, neighborhood, features like pool, condition. I already did some feature engineering and feature selection and my submission was top 5% when I submitted it. link to github\n",
    "\n",
    "- Baseline linear regression with no hyperparameters\n",
    "- ElasticNet with L1 and L2 regularization using ElasticNetCV hyperparameter optimization\n",
    "- ElasticNet with GridSearchCV hyperparameter optimization\n",
    "- XGBoost with a semi-manual hyperparameter optimization using early stopping and looping over a grid\n",
    "- XGBoost with Ray, HyperOpt and BayesOpt search algorithms\n",
    "- Accelerate advanced algorithms with a Ray cluster\n",
    "\n",
    "Results table with cv error and timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, ElasticNet, ElasticNetCV, Ridge, RidgeCV\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, GridSearchCV, KFold\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "#!conda install -y -c conda-forge  xgboost \n",
    "import xgboost\n",
    "from xgboost import XGBRegressor\n",
    "from xgboost import plot_importance\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.suggest import ConcurrencyLimiter\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "from ray.tune.suggest.bayesopt import BayesOptSearch\n",
    "from ray.tune.suggest.hyperopt import HyperOptSearch\n",
    "# pip install bayesian-optimization\n",
    "# pip install hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed for reproducibility\n",
    "RANDOMSTATE = 42\n",
    "np.random.seed(RANDOMSTATE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is from Iowa House prices\n",
    "Housing Prices Competition for Kaggle Learn Users\n",
    "https://www.kaggle.com/c/house-prices-advanced-regression-techniques\n",
    "\n",
    "Already did some feature engineering and feature selection\n",
    "predicting log of the SalePrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>BsmtFullBath</th>\n",
       "      <th>FullBath</th>\n",
       "      <th>KitchenAbvGr</th>\n",
       "      <th>GarageYrBlt</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>1stFlrSF</th>\n",
       "      <th>GrLivArea</th>\n",
       "      <th>GarageArea</th>\n",
       "      <th>...</th>\n",
       "      <th>SaleCondition_1</th>\n",
       "      <th>SaleCondition_2</th>\n",
       "      <th>SaleCondition_5</th>\n",
       "      <th>SaleType_4</th>\n",
       "      <th>BedroomAbvGr_1</th>\n",
       "      <th>BedroomAbvGr_4</th>\n",
       "      <th>BedroomAbvGr_5</th>\n",
       "      <th>HalfBath_1</th>\n",
       "      <th>TotalBath_1.0</th>\n",
       "      <th>TotalBath_2.5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>65.0</td>\n",
       "      <td>196.0</td>\n",
       "      <td>856</td>\n",
       "      <td>1710</td>\n",
       "      <td>548.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1262</td>\n",
       "      <td>1262</td>\n",
       "      <td>460.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>68.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>920</td>\n",
       "      <td>1786</td>\n",
       "      <td>608.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>95</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>961</td>\n",
       "      <td>1717</td>\n",
       "      <td>642.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>84.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>1145</td>\n",
       "      <td>2198</td>\n",
       "      <td>836.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    YearBuilt  BsmtFullBath  FullBath  KitchenAbvGr  GarageYrBlt  LotFrontage  \\\n",
       "Id                                                                              \n",
       "1           7             1         2             1            7         65.0   \n",
       "2          34             0         2             1           34         80.0   \n",
       "3           9             1         2             1            9         68.0   \n",
       "4          95             1         1             1           12         60.0   \n",
       "5          10             1         2             1           10         84.0   \n",
       "\n",
       "    MasVnrArea  1stFlrSF  GrLivArea  GarageArea  ...  SaleCondition_1  \\\n",
       "Id                                               ...                    \n",
       "1        196.0       856       1710       548.0  ...                0   \n",
       "2          0.0      1262       1262       460.0  ...                0   \n",
       "3        162.0       920       1786       608.0  ...                0   \n",
       "4          0.0       961       1717       642.0  ...                1   \n",
       "5        350.0      1145       2198       836.0  ...                0   \n",
       "\n",
       "    SaleCondition_2  SaleCondition_5  SaleType_4  BedroomAbvGr_1  \\\n",
       "Id                                                                 \n",
       "1                 0                0           1               0   \n",
       "2                 0                0           1               0   \n",
       "3                 0                0           1               0   \n",
       "4                 0                0           1               0   \n",
       "5                 0                0           1               0   \n",
       "\n",
       "    BedroomAbvGr_4  BedroomAbvGr_5  HalfBath_1  TotalBath_1.0  TotalBath_2.5  \n",
       "Id                                                                            \n",
       "1                0               0           1              0              0  \n",
       "2                0               0           0              0              1  \n",
       "3                0               0           1              0              0  \n",
       "4                0               0           0              0              0  \n",
       "5                1               0           1              0              0  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.247699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.109016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12.317171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11.849405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12.429220</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    SalePrice\n",
       "Id           \n",
       "1   12.247699\n",
       "2   12.109016\n",
       "3   12.317171\n",
       "4   11.849405\n",
       "5   12.429220"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import train data\n",
    "df = pd.read_pickle('df_train.pickle')\n",
    "\n",
    "response = 'SalePrice'\n",
    "predictors = ['YearBuilt',\n",
    "              'BsmtFullBath',\n",
    "              'FullBath',\n",
    "              'KitchenAbvGr',\n",
    "              'GarageYrBlt',\n",
    "              'LotFrontage',\n",
    "              'MasVnrArea',\n",
    "              '1stFlrSF',\n",
    "              'GrLivArea',\n",
    "              'GarageArea',\n",
    "              'WoodDeckSF',\n",
    "              'PorchSF',\n",
    "              'AvgBltRemod',\n",
    "              'FireBathRatio',\n",
    "              'TotalSF x OverallQual x OverallCond',\n",
    "              'AvgBltRemod x Functional x TotalFinSF',\n",
    "              'Functional x OverallQual',\n",
    "              'KitchenAbvGr x KitchenQual',\n",
    "              'GarageCars x GarageYrBlt',\n",
    "              'GarageQual x GarageCond x GarageCars',\n",
    "              'HeatingQC x Heating',\n",
    "              'monthnum',\n",
    "              'log_YearBuilt',\n",
    "              'log_LotArea',\n",
    "              'log_TotalFinSF',\n",
    "              'log_GarageRatio',\n",
    "              'log_TotalSF x OverallQual x OverallCond',\n",
    "              'log_TotalSF x OverallCond',\n",
    "              'log_AvgBltRemod x TotalFinSF',\n",
    "              'sq_2ndFlrSF',\n",
    "              'sq_BsmtFinSF',\n",
    "              'sq_BsmtFinSF x BsmtQual',\n",
    "              'sq_BsmtFinSF x BsmtBath',\n",
    "              'BldgType_4',\n",
    "              'BsmtExposure_1',\n",
    "              'BsmtExposure_4',\n",
    "              'BsmtFinType1_1',\n",
    "              'BsmtFinType1_2',\n",
    "              'BsmtFinType1_4',\n",
    "              'BsmtFinType1_5',\n",
    "              'BsmtFinType1_6',\n",
    "              'CentralAir_0',\n",
    "              'CentralAir_1',\n",
    "              'Condition1_1',\n",
    "              'Condition1_3',\n",
    "              'ExterCond_2',\n",
    "              'ExterQual_2',\n",
    "              'Exterior1st_4',\n",
    "              'Exterior1st_5',\n",
    "              'Exterior1st_10',\n",
    "              'Fence_0',\n",
    "              'Fence_2',\n",
    "              'Foundation_1',\n",
    "              'Foundation_5',\n",
    "              'GarageCars_1',\n",
    "              'GarageFinish_2',\n",
    "              'GarageFinish_3',\n",
    "              'GarageType_2',\n",
    "              'HouseStyle_2',\n",
    "              'KitchenQual_4',\n",
    "              'LotConfig_0',\n",
    "              'LotConfig_4',\n",
    "              'MSSubClass_30',\n",
    "              'MSSubClass_70',\n",
    "              'MSZoning_0',\n",
    "              'MSZoning_1',\n",
    "              'MSZoning_4',\n",
    "              'MasVnrType_2',\n",
    "              'MasVnrType_3',\n",
    "              'MoSold_1',\n",
    "              'MoSold_5',\n",
    "              'MoSold_6',\n",
    "              'MoSold_11',\n",
    "              'Neighborhood_3',\n",
    "              'Neighborhood_4',\n",
    "              'Neighborhood_5',\n",
    "              'Neighborhood_10',\n",
    "              'Neighborhood_11',\n",
    "              'Neighborhood_16',\n",
    "              'Neighborhood_17',\n",
    "              'Neighborhood_19',\n",
    "              'Neighborhood_22',\n",
    "              'Neighborhood_24',\n",
    "              'OverallCond_7',\n",
    "              'OverallQual_5',\n",
    "              'OverallQual_6',\n",
    "              'OverallQual_7',\n",
    "              'OverallQual_9',\n",
    "              'PavedDrive_0',\n",
    "              'PavedDrive_2',\n",
    "              'SaleCondition_1',\n",
    "              'SaleCondition_2',\n",
    "              'SaleCondition_5',\n",
    "              'SaleType_4',\n",
    "              'BedroomAbvGr_1',\n",
    "              'BedroomAbvGr_4',\n",
    "              'BedroomAbvGr_5',\n",
    "              'HalfBath_1',\n",
    "              'TotalBath_1.0',\n",
    "              'TotalBath_2.5']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, df[response], test_size=.25)\n",
    "\n",
    "display(df[predictors].head())\n",
    "display(df[[response]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are training on a response which is the log of 1 + the sale price\n",
    "# transform prediction back to original basis with expm1 and evaluate vs. original\n",
    "\n",
    "def evaluate(y_train, y_pred_train, y_test, y_pred_test):\n",
    "    \"\"\"evaluate in train_test split\"\"\"\n",
    "    print('Train RMSE', np.sqrt(mean_squared_error(np.expm1(y_train), np.expm1(y_pred_train))))\n",
    "    print('Train R-squared', r2_score(np.expm1(y_train), np.expm1(y_pred_train)))\n",
    "    print('Train MAE', mean_absolute_error(np.expm1(y_train), np.expm1(y_pred_train)))\n",
    "    print()\n",
    "    print('Test RMSE', np.sqrt(mean_squared_error(np.expm1(y_test), np.expm1(y_pred_test))))\n",
    "    print('Test R-squared', r2_score(np.expm1(y_test), np.expm1(y_pred_test)))\n",
    "    print('Test MAE', mean_absolute_error(np.expm1(y_test), np.expm1(y_pred_test)))\n",
    "\n",
    "MEAN_RESPONSE=df[response].mean()\n",
    "def cv_to_raw(cv_val):\n",
    "    \"\"\"convert log1p rmse to underlying SalePrice error\"\"\"\n",
    "    return np.expm1(MEAN_RESPONSE+cv_val) - np.expm1(MEAN_RESPONSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# always use same k-folds\n",
    "kfolds = KFold(n_splits=10, shuffle=True, random_state=RANDOMSTATE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline linear regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression\n",
      "100 predictors\n",
      "Train RMSE 16551.163619379957\n",
      "Train R-squared 0.955449013094547\n",
      "Train MAE 10998.860386226794\n",
      "\n",
      "Test RMSE 17846.1719186747\n",
      "Test R-squared 0.9364301769500323\n",
      "Test MAE 12755.395673617797\n",
      "\n",
      "Log1p CV RMSE 0.1037 (STD 0.0099)\n",
      "Raw CV RMSE 18191.9791 (STD 1838.6678)\n",
      "CPU times: user 265 ms, sys: 184 ms, total: 449 ms\n",
      "Wall time: 1.62 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Tune lr search space for alphas and l1_ratio\n",
    "print(\"LinearRegression\")\n",
    "\n",
    "print(len(predictors), \"predictors\")\n",
    "\n",
    "lr = LinearRegression()\n",
    "\n",
    "#train and evaluate in train/test split\n",
    "lr.fit(X_train[predictors], y_train)\n",
    "\n",
    "y_pred_train = lr.predict(X_train[predictors])\n",
    "y_pred_test = lr.predict(X_test[predictors])\n",
    "evaluate(y_train, y_pred_train, y_test, y_pred_test)\n",
    "\n",
    "# evaluate using kfolds, same process as train/test split but average results over 10 folds\n",
    "# more sample-efficient, less CPU-efficient\n",
    "\n",
    "scores = -cross_val_score(lr, df[predictors], df[response],\n",
    "                          scoring=\"neg_root_mean_squared_error\",\n",
    "                          cv=kfolds,\n",
    "                          n_jobs=-1)\n",
    "raw_scores = [cv_to_raw(x) for x in scores]\n",
    "print()\n",
    "print(\"Log1p CV RMSE %.04f (STD %.04f)\" % (np.mean(scores), np.std(scores)))\n",
    "print(\"Raw CV RMSE %.04f (STD %.04f)\" % (np.mean(raw_scores), np.std(raw_scores)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Native Sklearn xxxCV\n",
    "- LogisticRegressionCV, LassoCV, RidgeCV, ElasticNetCV, etc.\n",
    "- Test many hyperparameters in parallel with multithreading\n",
    "- Note improvement vs. LinearRegression due to controlling overfitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElasticnetCV\n",
      "Train RMSE 16749.841578148156\n",
      "Train R-squared 0.9543730254176759\n",
      "Train MAE 11014.427829330485\n",
      "\n",
      "Test RMSE 17479.444684207574\n",
      "Test R-squared 0.9390159700504748\n",
      "Test MAE 12415.248148857769\n",
      "l1_ratio 0.01\n",
      "alpha 0.005011872336272725\n",
      "\n",
      "Log1p CV RMSE 0.1032 (STD 0.0112)\n",
      "Raw CV RMSE 18103.4794 (STD 2060.3546)\n",
      "CPU times: user 24.4 s, sys: 5.64 s, total: 30 s\n",
      "Wall time: 4.61 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Tune elasticnet search space for alphas and L1_ratio\n",
    "# predictor selection used to create the training set used lasso\n",
    "# so l1 parameter is close to 0\n",
    "# could use ridge (eg elasticnet with 0 L1 regularization)\n",
    "# but then only 1 param, more general and useful to do this with elasticnet\n",
    "print(\"ElasticnetCV\")\n",
    "\n",
    "# make pipeline\n",
    "# with regularization must scale predictors\n",
    "elasticnetcv = make_pipeline(RobustScaler(),\n",
    "                             ElasticNetCV(max_iter=100000, \n",
    "                                          #l1_ratio=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.99],\n",
    "                                          l1_ratio=np.linspace(0.01, 0.21, 21),\n",
    "                                          alphas=np.logspace(-4, -2, 21),\n",
    "                                          cv=kfolds))\n",
    "\n",
    "#train and evaluate in train/test split\n",
    "elasticnetcv.fit(X_train[predictors], y_train)\n",
    "\n",
    "y_pred_train = elasticnetcv.predict(X_train[predictors])\n",
    "y_pred_test = elasticnetcv.predict(X_test[predictors])\n",
    "evaluate(y_train, y_pred_train, y_test, y_pred_test)\n",
    "l1_ratio = elasticnetcv._final_estimator.l1_ratio_\n",
    "alpha = elasticnetcv._final_estimator.alpha_\n",
    "print('l1_ratio', l1_ratio)\n",
    "print('alpha', alpha)\n",
    "\n",
    "# evaluate using kfolds on full dataset\n",
    "# I don't see API to get CV error from elasticnetcv, so we use cross_val_score\n",
    "elasticnet = ElasticNet(alpha=alpha,\n",
    "                        l1_ratio=l1_ratio,\n",
    "                        max_iter=10000)\n",
    "\n",
    "scores = -cross_val_score(elasticnet, df[predictors], df[response],\n",
    "                          scoring=\"neg_root_mean_squared_error\",\n",
    "                          cv=kfolds,\n",
    "                          n_jobs=-1)\n",
    "raw_scores = [cv_to_raw(x) for x in scores]\n",
    "print()\n",
    "print(\"Log1p CV RMSE %.04f (STD %.04f)\" % (np.mean(scores), np.std(scores)))\n",
    "print(\"Raw CV RMSE %.04f (STD %.04f)\" % (np.mean(raw_scores), np.std(raw_scores)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearchCV\n",
    "- Useful for algos with no native multithreaded xxxCV\n",
    "- Test many hyperparameter combinations in parallel with multithreading\n",
    "- Same result vs ElasticNetCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CV on full dataset\n",
      "Fitting 10 folds for each of 441 candidates, totalling 4410 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 305 tasks      | elapsed:    2.0s\n",
      "[Parallel(n_jobs=-1)]: Done 804 tasks      | elapsed:    4.7s\n",
      "[Parallel(n_jobs=-1)]: Done 1504 tasks      | elapsed:    7.8s\n",
      "[Parallel(n_jobs=-1)]: Done 2404 tasks      | elapsed:   11.2s\n",
      "[Parallel(n_jobs=-1)]: Done 4000 tasks      | elapsed:   16.6s\n",
      "[Parallel(n_jobs=-1)]: Done 4410 out of 4410 | elapsed:   17.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params {'alpha': 0.002511886431509582, 'l1_ratio': 0.01}\n",
      "best score 0.010632767727426403\n",
      "ElasticNet(alpha=0.002511886431509582, l1_ratio=0.01, max_iter=100000)\n",
      "\n",
      "Log1p CV RMSE 0.102973 (STD 0.0108)\n",
      "Raw CV RMSE 18054.997021 (STD 1984.3274)\n",
      "weighted average 0.102993\n"
     ]
    }
   ],
   "source": [
    "#%%time\n",
    "gs = make_pipeline(RobustScaler(),\n",
    "                   GridSearchCV(ElasticNet(max_iter=100000),\n",
    "                                param_grid={'l1_ratio': np.linspace(0.01, 0.21, 21),\n",
    "                                            'alpha': np.logspace(-4, -2, 21),\n",
    "                                           },\n",
    "                                scoring='neg_mean_squared_error',\n",
    "                                refit=True,\n",
    "                                cv=kfolds,\n",
    "                                n_jobs=-1,\n",
    "                                verbose=1\n",
    "                               ))\n",
    "\n",
    "# do cv using kfolds on full dataset\n",
    "print(\"\\nCV on full dataset\")\n",
    "gs.fit(df[predictors], df[response])\n",
    "print('best params', gs._final_estimator.best_params_)\n",
    "print('best score', -gs._final_estimator.best_score_)\n",
    "l1_ratio = gs._final_estimator.best_params_['l1_ratio']\n",
    "alpha = gs._final_estimator.best_params_['alpha']\n",
    "\n",
    "elasticnet = ElasticNet(alpha=alpha,\n",
    "                        l1_ratio=l1_ratio,\n",
    "                        max_iter=100000)\n",
    "print(elasticnet)\n",
    "\n",
    "scores = -cross_val_score(elasticnet, df[predictors], df[response],\n",
    "                          scoring=\"neg_root_mean_squared_error\",\n",
    "                          cv=kfolds,\n",
    "                          n_jobs=-1)\n",
    "raw_scores = [cv_to_raw(x) for x in scores]\n",
    "print()\n",
    "print(\"Log1p CV RMSE %.06f (STD %.04f)\" % (np.mean(scores), np.std(scores)))\n",
    "print(\"Raw CV RMSE %.06f (STD %.04f)\" % (np.mean(raw_scores), np.std(raw_scores)))\n",
    "\n",
    "# difference in average CV scores reported by GridSearchCV and cross_val_score\n",
    "# with same alpha, l1_ratio, kfolds\n",
    "# one reason could be that we used simple average, GridSearchCV is weighted by # of samples per fold?\n",
    "nsamples = [len(z[1]) for z in kfolds.split(df)]\n",
    "print(\"weighted average %.06f\" % np.average(scores, weights=nsamples))\n",
    "# not sure why \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElasticNet(alpha=0.002511886431509582, l1_ratio=0.01, max_iter=10000)\n",
      "0.102973\n"
     ]
    }
   ],
   "source": [
    "# roll-our-own CV \n",
    "# matches cross_val_score\n",
    "alpha = 0.002511886431509582\n",
    "l1_ratio = 0.01\n",
    "regressor = ElasticNet(alpha=alpha,\n",
    "                       l1_ratio=l1_ratio,\n",
    "                       max_iter=10000)\n",
    "print(regressor)\n",
    "cverrors = []\n",
    "for train_fold, cv_fold in kfolds.split(df): \n",
    "    fold_X_train=df[predictors].values[train_fold]\n",
    "    fold_y_train=df[response].values[train_fold]\n",
    "    fold_X_test=df[predictors].values[cv_fold]\n",
    "    fold_y_test=df[response].values[cv_fold]\n",
    "    regressor.fit(fold_X_train, fold_y_train)\n",
    "    y_pred_test=regressor.predict(fold_X_test)\n",
    "    cverrors.append(np.sqrt(mean_squared_error(fold_y_test, y_pred_test)))\n",
    "    \n",
    "print(\"%.06f\" % np.average(cverrors))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost CV \n",
    "- XGBoost is a powerful gradient boost algo with built-in multithreading, native CV\n",
    "- XGBoost has many tuning parameters so a complete grid search has an unreasonable number of combinations\n",
    "- We tune reduced sets sequentially and use early stopping. \n",
    "\n",
    "### Tuning methodology\n",
    "- Set an initial set of starting parameters\n",
    "- Do 10-fold CV\n",
    "- Use early stopping in each fold to halt training if no improvement after eg 100 rounds, average error over kfolds\n",
    "- Tune max_depth and min_child_weight that result in smallest CV error\n",
    "- Tune subsample and colsample_bytree\n",
    "- Tune alpha, lambda and gamma (regularization)\n",
    "- Tune learning rate: lower learning rate will need more rounds/n_estimators\n",
    "- Retrain on full dataset with best learning rate and best n_estimators (average stopping point over kfolds)\n",
    "\n",
    "### Notes\n",
    "- It doesn't seem possible to get XGBoost early stopping and also use GridSearchCV. GridSearchCV doesn't pass the kfolds in a way that XGboost understands for early stopping\n",
    "- 2 alternative approaches \n",
    "    - use native xgboost .cv which understands early stopping but doesn't use sklearn API (uses DMatrix, not np array or dataframe)\n",
    "    - use sklearn API and roll our own grid search instead of GridSearchCV (used below)\n",
    "- XGboost terminology differs from sklearn\n",
    "    - boost_rounds = n_estimators\n",
    "    - eta = learning_rate\n",
    "- parameter reference: https://xgboost.readthedocs.io/en/latest/parameter.html\n",
    "- training reference: https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11:29:18 params    0: {'max_depth': 3, 'min_child_weight': 2, 'subsample': 0.65, 'colsample_bytree': 0.05, 'reg_alpha': 0.001, 'reg_lambda': 0.01, 'gamma': 0, 'learning_rate': 0.1}\n",
      "11:29:22   0 result mean: 0.108778 std: 0.012633, iter: 434.00\n",
      "11:29:22 params    1: {'max_depth': 3, 'min_child_weight': 2, 'subsample': 0.65, 'colsample_bytree': 0.05, 'reg_alpha': 0.001, 'reg_lambda': 0.01, 'gamma': 0, 'learning_rate': 0.03162277660168379}\n",
      "11:29:31   1 result mean: 0.106535 std: 0.014412, iter: 843.80\n",
      "11:29:31 params    2: {'max_depth': 3, 'min_child_weight': 2, 'subsample': 0.65, 'colsample_bytree': 0.05, 'reg_alpha': 0.001, 'reg_lambda': 0.01, 'gamma': 0, 'learning_rate': 0.01}\n",
      "11:29:52   2 result mean: 0.105469 std: 0.013154, iter: 2296.30\n",
      "11:29:52 params    3: {'max_depth': 3, 'min_child_weight': 2, 'subsample': 0.65, 'colsample_bytree': 0.05, 'reg_alpha': 0.001, 'reg_lambda': 0.01, 'gamma': 0, 'learning_rate': 0.0031622776601683794}\n",
      "11:30:37   3 result mean: 0.106649 std: 0.013061, iter: 5185.00\n",
      "11:30:37 params    4: {'max_depth': 3, 'min_child_weight': 2, 'subsample': 0.65, 'colsample_bytree': 0.05, 'reg_alpha': 0.001, 'reg_lambda': 0.01, 'gamma': 0, 'learning_rate': 0.001}\n",
      "11:32:58   4 result mean: 0.106623 std: 0.013864, iter: 16281.00\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_child_weight</th>\n",
       "      <th>subsample</th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>reg_alpha</th>\n",
       "      <th>reg_lambda</th>\n",
       "      <th>reg_gamma</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>rmse</th>\n",
       "      <th>std</th>\n",
       "      <th>best_iter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.105469</td>\n",
       "      <td>0.013154</td>\n",
       "      <td>2296.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0</td>\n",
       "      <td>0.031623</td>\n",
       "      <td>0.106535</td>\n",
       "      <td>0.014412</td>\n",
       "      <td>843.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.106623</td>\n",
       "      <td>0.013864</td>\n",
       "      <td>16281.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0</td>\n",
       "      <td>0.003162</td>\n",
       "      <td>0.106649</td>\n",
       "      <td>0.013061</td>\n",
       "      <td>5185.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.108778</td>\n",
       "      <td>0.012633</td>\n",
       "      <td>434.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   max_depth  min_child_weight  subsample  colsample_bytree  reg_alpha  \\\n",
       "2          3                 2       0.65              0.05      0.001   \n",
       "1          3                 2       0.65              0.05      0.001   \n",
       "4          3                 2       0.65              0.05      0.001   \n",
       "3          3                 2       0.65              0.05      0.001   \n",
       "0          3                 2       0.65              0.05      0.001   \n",
       "\n",
       "   reg_lambda  reg_gamma  learning_rate      rmse       std  best_iter  \n",
       "2        0.01          0       0.010000  0.105469  0.013154     2296.3  \n",
       "1        0.01          0       0.031623  0.106535  0.014412      843.8  \n",
       "4        0.01          0       0.001000  0.106623  0.013864    16281.0  \n",
       "3        0.01          0       0.003162  0.106649  0.013061     5185.0  \n",
       "0        0.01          0       0.100000  0.108778  0.012633      434.0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initial XGboost parameters\n",
    "\n",
    "max_depth = 5\n",
    "min_child_weight=5\n",
    "colsample_bytree = 0.5\n",
    "subsample = 0.5\n",
    "reg_alpha = 1e-05\n",
    "reg_lambda = 1\n",
    "reg_gamma = 0\n",
    "learning_rate = 0.01\n",
    "\n",
    "BOOST_ROUNDS=50000   # we use early stopping so make this arbitrarily high\n",
    "EARLY_STOPPING_ROUNDS=100 # stop if no improvement after 100 rounds\n",
    "\n",
    "# round 1: tune depth and min_child_weight\n",
    "max_depths = list(range(1,5))\n",
    "min_child_weights = list(range(1,5))\n",
    "gridsearch_params_1 = product(max_depths, min_child_weights)\n",
    "\n",
    "# round 2: tune subsample and colsample_bytree\n",
    "subsamples = np.linspace(0.1, 1.0, 10)\n",
    "colsample_bytrees = np.linspace(0.1, 1.0, 10)\n",
    "gridsearch_params_2 = product(subsamples, colsample_bytrees)\n",
    "\n",
    "# round 2 (refined): tune subsample and colsample_bytree\n",
    "subsamples = np.linspace(0.6, 0.8, 5)\n",
    "colsample_bytrees = np.linspace(0.05, 0.25, 5)\n",
    "gridsearch_params_2 = product(subsamples, colsample_bytrees)\n",
    "\n",
    "# round 3: tune alpha, lambda, gamma\n",
    "reg_alphas = np.logspace(-3, -2, 3)\n",
    "reg_lambdas = np.logspace(-2, 1, 4)\n",
    "reg_gammas = [0]\n",
    "#reg_gammas = np.linspace(0, 5, 6)\n",
    "gridsearch_params_3 = product(reg_alphas, reg_lambdas, reg_gammas)\n",
    "\n",
    "# round 4: learning rate\n",
    "learning_rates = reversed(np.logspace(-3, -1, 5).tolist())\n",
    "gridsearch_params_4 = learning_rates\n",
    "\n",
    "# override initial parameters after search\n",
    "# round 1:\n",
    "max_depth=3\n",
    "min_child_weight=2\n",
    "# # round 2:\n",
    "subsample=0.65\n",
    "colsample_bytree=0.05\n",
    "# # round 3: \n",
    "reg_alpha = 0.001000\n",
    "reg_lambda = 0.01\n",
    "reg_gamma = 0\n",
    "\n",
    "def my_cv(df, predictors, response, kfolds, regressor, verbose=False):\n",
    "    \"\"\"Roll our own CV over kfolds with early stopping\"\"\"\n",
    "    metrics = []\n",
    "    best_iterations = []\n",
    "\n",
    "    for train_fold, cv_fold in kfolds.split(df): \n",
    "        fold_X_train=df[predictors].values[train_fold]\n",
    "        fold_y_train=df[response].values[train_fold]\n",
    "        fold_X_test=df[predictors].values[cv_fold]\n",
    "        fold_y_test=df[response].values[cv_fold]\n",
    "        regressor.fit(fold_X_train, fold_y_train,\n",
    "                      early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n",
    "                      eval_set=[(fold_X_test, fold_y_test)],\n",
    "                      eval_metric='rmse',\n",
    "                      verbose=verbose\n",
    "                     )\n",
    "        y_pred_test=regressor.predict(fold_X_test)\n",
    "        metrics.append(np.sqrt(mean_squared_error(fold_y_test, y_pred_test)))\n",
    "        best_iterations.append(xgb.best_iteration)\n",
    "    return np.average(metrics), np.std(metrics), np.average(best_iterations)\n",
    "\n",
    "results = []\n",
    "best_iterations = []\n",
    "\n",
    "# for i, (max_depth, min_child_weight) in enumerate(gridsearch_params_1): # round 1\n",
    "# for i, (subsample, colsample_bytree) in enumerate(gridsearch_params_2): # round 2\n",
    "# for i, (reg_alpha, reg_lambda, reg_gamma) in enumerate(gridsearch_params_3): # round 3\n",
    "for i, learning_rate in enumerate(gridsearch_params_4): # round 4\n",
    "\n",
    "    params = {\n",
    "        'max_depth': max_depth,\n",
    "        'min_child_weight': min_child_weight,\n",
    "        'subsample': subsample,\n",
    "        'colsample_bytree': colsample_bytree,\n",
    "        'reg_alpha': reg_alpha,\n",
    "        'reg_lambda': reg_lambda,\n",
    "        'gamma': reg_gamma,\n",
    "        'learning_rate': learning_rate,\n",
    "    }\n",
    "    print(\"%s params  %3d: %s\" % (datetime.strftime(datetime.now(), \"%T\"), i, params))\n",
    "    \n",
    "    xgb = XGBRegressor(\n",
    "        objective='reg:squarederror',\n",
    "        n_estimators=BOOST_ROUNDS,\n",
    "        early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n",
    "        random_state=RANDOMSTATE,    \n",
    "        verbosity=1,\n",
    "        n_jobs=-1,\n",
    "        **params\n",
    "    )\n",
    "    \n",
    "    metric_rmse, metric_std, best_iteration = my_cv(df, predictors, response, kfolds, xgb, verbose=False)    \n",
    "    results.append([max_depth, min_child_weight, subsample, colsample_bytree, reg_alpha, reg_lambda, reg_gamma, \n",
    "                   learning_rate, metric_rmse, metric_std, best_iteration])\n",
    "    \n",
    "    print(\"%s %3d result mean: %.6f std: %.6f, iter: %.2f\" % (datetime.strftime(datetime.now(), \"%T\"), i, metric_rmse, metric_std, best_iteration))\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame(results, columns=['max_depth', 'min_child_weight', 'subsample', 'colsample_bytree', \n",
    "                               'reg_alpha', 'reg_lambda', 'reg_gamma', 'learning_rate', 'rmse', 'std', 'best_iter']).sort_values('rmse')\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 3, 'min_child_weight': 2.0, 'subsample': 0.65, 'colsample_bytree': 0.05, 'reg_alpha': 0.001, 'reg_lambda': 0.01, 'gamma': 0.0, 'learning_rate': 0.01, 'n_estimators': 2296}\n"
     ]
    }
   ],
   "source": [
    "max_depth = int(results_df.iloc[0]['max_depth'])\n",
    "min_child_weight = results_df.iloc[0]['min_child_weight']\n",
    "subsample = results_df.iloc[0]['subsample']\n",
    "colsample_bytree = results_df.iloc[0]['colsample_bytree']\n",
    "reg_alpha = results_df.iloc[0]['reg_alpha']\n",
    "reg_lambda = results_df.iloc[0]['reg_lambda']\n",
    "reg_gamma = results_df.iloc[0]['reg_gamma']\n",
    "learning_rate = results_df.iloc[0]['learning_rate']\n",
    "N_ESTIMATORS = int(results_df.iloc[0]['best_iter'])\n",
    "\n",
    "params = {\n",
    "    'max_depth': int(max_depth),\n",
    "    'min_child_weight': min_child_weight,\n",
    "    'subsample': subsample,\n",
    "    'colsample_bytree': colsample_bytree,\n",
    "    'reg_alpha': reg_alpha,\n",
    "    'reg_lambda': reg_lambda,\n",
    "    'gamma': reg_gamma,\n",
    "    'learning_rate': learning_rate,\n",
    "    'n_estimators': N_ESTIMATORS,    \n",
    "}\n",
    "\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBRegressor(base_score=None, booster=None, colsample_bylevel=None,\n",
      "             colsample_bynode=None, colsample_bytree=0.05, gamma=0.0,\n",
      "             gpu_id=None, importance_type='gain', interaction_constraints=None,\n",
      "             learning_rate=0.01, max_delta_step=None, max_depth=3,\n",
      "             min_child_weight=2.0, missing=nan, monotone_constraints=None,\n",
      "             n_estimators=2296, n_jobs=-1, num_parallel_tree=None,\n",
      "             random_state=42, reg_alpha=0.001, reg_lambda=0.01,\n",
      "             scale_pos_weight=None, subsample=0.65, tree_method=None,\n",
      "             validate_parameters=False, verbosity=1)\n",
      "\n",
      "Log1p CV RMSE 0.102973 (STD 0.0108)\n",
      "Raw CV RMSE 18054.997021 (STD 1984.3274)\n"
     ]
    }
   ],
   "source": [
    "# evaluate without early stopping\n",
    "\n",
    "xgb = XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    random_state=RANDOMSTATE,    \n",
    "    verbosity=1,\n",
    "    n_jobs=-1,\n",
    "    **params\n",
    ")\n",
    "print(xgb)\n",
    "\n",
    "scores = -cross_val_score(elasticnet, df[predictors], df[response],\n",
    "                          scoring=\"neg_root_mean_squared_error\",\n",
    "                          cv=kfolds,\n",
    "                          n_jobs=-1)\n",
    "raw_scores = [cv_to_raw(x) for x in scores]\n",
    "print()\n",
    "print(\"Log1p CV RMSE %.06f (STD %.04f)\" % (np.mean(scores), np.std(scores)))\n",
    "print(\"Raw CV RMSE %.06f (STD %.04f)\" % (np.mean(raw_scores), np.std(raw_scores)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refactor for ray.tune\n",
    "def my_xgb(config):\n",
    "    \n",
    "    # fix these configs for hyperopt\n",
    "    config['max_depth'] += 2   # hyperopt needs left to start at 0 but we want to start at 2\n",
    "    config['n_estimators'] = int(config['n_estimators'])   # pass float eg loguniform distribution, use int\n",
    "    \n",
    "    xgb = XGBRegressor(\n",
    "        objective='reg:squarederror',\n",
    "        n_jobs=1,\n",
    "        **config,\n",
    "    )\n",
    "    scores = np.sqrt(-cross_val_score(xgb, df[predictors], df[response],\n",
    "                                      scoring=\"neg_mean_squared_error\",\n",
    "                                      cv=kfolds))\n",
    "    tune.report(mse=np.mean(scores))\n",
    "    return xgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Session not detected. You should not be calling this function outside `tune.run` or while using the class API. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBRegressor(base_score=None, booster=None, colsample_bylevel=None,\n",
      "             colsample_bynode=None, colsample_bytree=0.05, gamma=0.0,\n",
      "             gpu_id=None, importance_type='gain', interaction_constraints=None,\n",
      "             learning_rate=0.01, max_delta_step=None, max_depth=3,\n",
      "             min_child_weight=2.0, missing=nan, monotone_constraints=None,\n",
      "             n_estimators=2296, n_jobs=1, num_parallel_tree=None,\n",
      "             random_state=None, reg_alpha=0.001, reg_lambda=0.01,\n",
      "             scale_pos_weight=None, subsample=0.65, tree_method=None,\n",
      "             validate_parameters=False, verbosity=None)\n",
      "\n",
      "Log1p CV RMSE 0.102973 (STD 0.0108)\n",
      "Raw CV RMSE 18054.997021 (STD 1984.3274)\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    'max_depth': max_depth-2,\n",
    "    'min_child_weight': min_child_weight,\n",
    "    'subsample': subsample,\n",
    "    'colsample_bytree': colsample_bytree,\n",
    "    'reg_alpha': reg_alpha,\n",
    "    'reg_lambda': reg_lambda,\n",
    "    'gamma': reg_gamma,\n",
    "    'learning_rate': learning_rate,\n",
    "    'n_estimators': N_ESTIMATORS,    \n",
    "}\n",
    "\n",
    "xgb = my_xgb(config)\n",
    "\n",
    "print(xgb)\n",
    "\n",
    "scores = -cross_val_score(elasticnet, df[predictors], df[response],\n",
    "                          scoring=\"neg_root_mean_squared_error\",\n",
    "                          cv=kfolds,\n",
    "                          n_jobs=-1)\n",
    "raw_scores = [cv_to_raw(x) for x in scores]\n",
    "print()\n",
    "print(\"Log1p CV RMSE %.06f (STD %.04f)\" % (np.mean(scores), np.std(scores)))\n",
    "print(\"Raw CV RMSE %.06f (STD %.04f)\" % (np.mean(raw_scores), np.std(raw_scores)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 19.2/32.0 GiB<br>Using AsyncHyperBand: num_stopped=214\n",
       "Bracket: Iter 64.000: None | Iter 16.000: None | Iter 4.000: None | Iter 1.000: -0.10988067765831336<br>Resources requested: 15/16 CPUs, 0/0 GPUs, 0.0/9.62 GiB heap, 0.0/3.32 GiB objects<br>Result logdir: /Users/drucev/ray_results/my_xgb<br>Number of trials: 512 (196 PENDING, 15 RUNNING, 301 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name     </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  colsample_bytree</th><th style=\"text-align: right;\">  gamma</th><th style=\"text-align: right;\">  learning_rate</th><th style=\"text-align: right;\">  max_depth</th><th style=\"text-align: right;\">  min_child_weight</th><th style=\"text-align: right;\">  n_estimators</th><th style=\"text-align: right;\">  reg_alpha</th><th style=\"text-align: right;\">  reg_lambda</th><th style=\"text-align: right;\">  subsample</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     mse</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>my_xgb_c7a07c9e</td><td>PENDING   </td><td>     </td><td style=\"text-align: right;\">              0.4 </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.0186291 </td><td style=\"text-align: right;\">          1</td><td style=\"text-align: right;\">                 4</td><td style=\"text-align: right;\">      2722.49 </td><td style=\"text-align: right;\">0.0568616  </td><td style=\"text-align: right;\"> 0.000219101</td><td style=\"text-align: right;\">       0.85</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td></tr>\n",
       "<tr><td>my_xgb_c7a711c6</td><td>PENDING   </td><td>     </td><td style=\"text-align: right;\">              0.45</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.025078  </td><td style=\"text-align: right;\">          1</td><td style=\"text-align: right;\">                 4</td><td style=\"text-align: right;\">      2312.5  </td><td style=\"text-align: right;\">0.0690756  </td><td style=\"text-align: right;\"> 0.000511991</td><td style=\"text-align: right;\">       0.85</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td></tr>\n",
       "<tr><td>my_xgb_c7add0b0</td><td>PENDING   </td><td>     </td><td style=\"text-align: right;\">              0.4 </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.0294749 </td><td style=\"text-align: right;\">          1</td><td style=\"text-align: right;\">                 4</td><td style=\"text-align: right;\">      1639.88 </td><td style=\"text-align: right;\">0.0330019  </td><td style=\"text-align: right;\"> 0.00071687 </td><td style=\"text-align: right;\">       0.85</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td></tr>\n",
       "<tr><td>my_xgb_c7b46a56</td><td>PENDING   </td><td>     </td><td style=\"text-align: right;\">              0.4 </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.0104948 </td><td style=\"text-align: right;\">          1</td><td style=\"text-align: right;\">                 4</td><td style=\"text-align: right;\">      1216.53 </td><td style=\"text-align: right;\">0.0488063  </td><td style=\"text-align: right;\"> 1.68645    </td><td style=\"text-align: right;\">       0.85</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td></tr>\n",
       "<tr><td>my_xgb_c7bae53e</td><td>PENDING   </td><td>     </td><td style=\"text-align: right;\">              0.55</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.00971409</td><td style=\"text-align: right;\">          1</td><td style=\"text-align: right;\">                 4</td><td style=\"text-align: right;\">      1490.05 </td><td style=\"text-align: right;\">0.0537389  </td><td style=\"text-align: right;\"> 0.373096   </td><td style=\"text-align: right;\">       0.8 </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td></tr>\n",
       "<tr><td>my_xgb_c7c13948</td><td>PENDING   </td><td>     </td><td style=\"text-align: right;\">              0.55</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.00768961</td><td style=\"text-align: right;\">          1</td><td style=\"text-align: right;\">                 4</td><td style=\"text-align: right;\">      2265.4  </td><td style=\"text-align: right;\">0.082557   </td><td style=\"text-align: right;\"> 1.24952    </td><td style=\"text-align: right;\">       0.8 </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td></tr>\n",
       "<tr><td>my_xgb_c7c7f3aa</td><td>PENDING   </td><td>     </td><td style=\"text-align: right;\">              0.55</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.00808841</td><td style=\"text-align: right;\">          1</td><td style=\"text-align: right;\">                 4</td><td style=\"text-align: right;\">      1084.78 </td><td style=\"text-align: right;\">0.000344957</td><td style=\"text-align: right;\"> 2.13606    </td><td style=\"text-align: right;\">       0.6 </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td></tr>\n",
       "<tr><td>my_xgb_c64f2d7c</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">              0.15</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.00170248</td><td style=\"text-align: right;\">          3</td><td style=\"text-align: right;\">                 3</td><td style=\"text-align: right;\">       734.384</td><td style=\"text-align: right;\">0.203226   </td><td style=\"text-align: right;\"> 0.0107159  </td><td style=\"text-align: right;\">       0.6 </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td></tr>\n",
       "<tr><td>my_xgb_c6ac7590</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">              0.8 </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.00905756</td><td style=\"text-align: right;\">          4</td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">      4412.05 </td><td style=\"text-align: right;\">0.0102789  </td><td style=\"text-align: right;\">36.6868     </td><td style=\"text-align: right;\">       0.8 </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td></tr>\n",
       "<tr><td>my_xgb_c6b2a988</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">              0.75</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.0110132 </td><td style=\"text-align: right;\">          4</td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">      6063.41 </td><td style=\"text-align: right;\">0.0111333  </td><td style=\"text-align: right;\"> 0.012667   </td><td style=\"text-align: right;\">       0.85</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td></tr>\n",
       "<tr><td>my_xgb_c6bf5728</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">              0.75</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.0200842 </td><td style=\"text-align: right;\">          4</td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">      4318.71 </td><td style=\"text-align: right;\">0.00499309 </td><td style=\"text-align: right;\"> 0.032126   </td><td style=\"text-align: right;\">       0.7 </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td></tr>\n",
       "<tr><td>my_xgb_c74c9552</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">              0.2 </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.0993951 </td><td style=\"text-align: right;\">          2</td><td style=\"text-align: right;\">                 0</td><td style=\"text-align: right;\">      6712.79 </td><td style=\"text-align: right;\">0.725906   </td><td style=\"text-align: right;\"> 0.000484159</td><td style=\"text-align: right;\">       0.5 </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td></tr>\n",
       "<tr><td>my_xgb_c75331dc</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">              0.2 </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.0629943 </td><td style=\"text-align: right;\">          2</td><td style=\"text-align: right;\">                 0</td><td style=\"text-align: right;\">      7370.05 </td><td style=\"text-align: right;\">0.79521    </td><td style=\"text-align: right;\"> 0.00147183 </td><td style=\"text-align: right;\">       0.5 </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td></tr>\n",
       "<tr><td>my_xgb_c759b0b6</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">              0.15</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.0521523 </td><td style=\"text-align: right;\">          2</td><td style=\"text-align: right;\">                 0</td><td style=\"text-align: right;\">      9965.83 </td><td style=\"text-align: right;\">0.881857   </td><td style=\"text-align: right;\"> 0.0010441  </td><td style=\"text-align: right;\">       0.55</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td></tr>\n",
       "<tr><td>my_xgb_c19196c6</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">              0.1 </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.0107186 </td><td style=\"text-align: right;\">          3</td><td style=\"text-align: right;\">                 2</td><td style=\"text-align: right;\">      4195.53 </td><td style=\"text-align: right;\">0.269628   </td><td style=\"text-align: right;\"> 0.00822327 </td><td style=\"text-align: right;\">       0.85</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        147.652 </td><td style=\"text-align: right;\">0.10721 </td></tr>\n",
       "<tr><td>my_xgb_c194c684</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">              0.75</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.0162881 </td><td style=\"text-align: right;\">          4</td><td style=\"text-align: right;\">                 1</td><td style=\"text-align: right;\">      2151.51 </td><td style=\"text-align: right;\">0.000100402</td><td style=\"text-align: right;\"> 0.00482285 </td><td style=\"text-align: right;\">       0.6 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        332.378 </td><td style=\"text-align: right;\">0.110641</td></tr>\n",
       "<tr><td>my_xgb_c1978888</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">              0.5 </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.0397744 </td><td style=\"text-align: right;\">          1</td><td style=\"text-align: right;\">                 3</td><td style=\"text-align: right;\">      1280.8  </td><td style=\"text-align: right;\">0.179607   </td><td style=\"text-align: right;\"> 0.0135869  </td><td style=\"text-align: right;\">       0.45</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         66.6766</td><td style=\"text-align: right;\">0.108522</td></tr>\n",
       "<tr><td>my_xgb_c199b28e</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">              0.2 </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.00139427</td><td style=\"text-align: right;\">          0</td><td style=\"text-align: right;\">                 1</td><td style=\"text-align: right;\">      4258.59 </td><td style=\"text-align: right;\">0.0017747  </td><td style=\"text-align: right;\"> 0.00431635 </td><td style=\"text-align: right;\">       0.75</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        120.71  </td><td style=\"text-align: right;\">0.12618 </td></tr>\n",
       "<tr><td>my_xgb_c19bbb24</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">              0.75</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.0401715 </td><td style=\"text-align: right;\">          4</td><td style=\"text-align: right;\">                 0</td><td style=\"text-align: right;\">       770.645</td><td style=\"text-align: right;\">0.140043   </td><td style=\"text-align: right;\"> 0.0208237  </td><td style=\"text-align: right;\">       0.8 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        117.464 </td><td style=\"text-align: right;\">0.114311</td></tr>\n",
       "<tr><td>my_xgb_c19db8a2</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">              0.4 </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.0207636 </td><td style=\"text-align: right;\">          5</td><td style=\"text-align: right;\">                 3</td><td style=\"text-align: right;\">      5258.03 </td><td style=\"text-align: right;\">0.000904173</td><td style=\"text-align: right;\"> 0.0167774  </td><td style=\"text-align: right;\">       0.8 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        361.758 </td><td style=\"text-align: right;\">0.112231</td></tr>\n",
       "<tr><td>my_xgb_c19f66ca</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">              0.2 </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.0661937 </td><td style=\"text-align: right;\">          0</td><td style=\"text-align: right;\">                 3</td><td style=\"text-align: right;\">       545.424</td><td style=\"text-align: right;\">0.00122106 </td><td style=\"text-align: right;\"> 0.00209417 </td><td style=\"text-align: right;\">       0.45</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         12.5132</td><td style=\"text-align: right;\">0.110437</td></tr>\n",
       "</tbody>\n",
       "</table><br>... 492 more trials not shown (189 PENDING, 8 RUNNING, 294 TERMINATED)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m E1009 12:19:00.761010  8339 378379712 node_manager.cc:3307] Failed to send get core worker stats request: IOError: 2: Stream removed\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m E1009 12:19:01.774693  8339 378379712 node_manager.cc:3307] Failed to send get core worker stats request: IOError: 14: Transport closed\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m E1009 12:19:02.786226  8339 378379712 node_manager.cc:3307] Failed to send get core worker stats request: IOError: 2: Stream removed\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m E1009 12:19:03.797698  8339 378379712 node_manager.cc:3307] Failed to send get core worker stats request: IOError: 2: Stream removed\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m E1009 12:19:04.813293  8339 378379712 node_manager.cc:3307] Failed to send get core worker stats request: IOError: 2: Stream removed\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m E1009 12:19:05.828049  8339 378379712 node_manager.cc:3307] Failed to send get core worker stats request: IOError: 2: Stream removed\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m E1009 12:19:06.840543  8339 378379712 node_manager.cc:3307] Failed to send get core worker stats request: IOError: 14: Transport closed\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m E1009 12:19:07.853204  8339 378379712 node_manager.cc:3307] Failed to send get core worker stats request: IOError: 2: Stream removed\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m E1009 12:19:08.864473  8339 378379712 node_manager.cc:3307] Failed to send get core worker stats request: IOError: 14: Protocol wrong type for socket\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m E1009 12:19:09.878178  8339 378379712 node_manager.cc:3307] Failed to send get core worker stats request: IOError: 2: Stream removed\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m E1009 12:19:10.892254  8339 378379712 node_manager.cc:3307] Failed to send get core worker stats request: IOError: 2: Stream removed\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m E1009 12:19:11.907222  8339 378379712 node_manager.cc:3307] Failed to send get core worker stats request: IOError: 14: Transport closed\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-e55d8ee6371a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m }\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m analysis = tune.run(my_xgb,\n\u001b[0m\u001b[1;32m     24\u001b[0m                     \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"my_xgb\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                     \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"mse\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.8/site-packages/ray/tune/tune.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, local_dir, search_alg, scheduler, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, verbose, progress_reporter, loggers, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, server_port, resume, queue_trials, reuse_actors, trial_executor, raise_on_failed_trial, ray_auto_init, run_errored_only, global_checkpoint_period, with_server, upload_dir, sync_to_cloud, sync_to_driver, sync_on_checkpoint)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_finished\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m         \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m             \u001b[0m_report_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrunner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress_reporter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.8/site-packages/ray/tune/trial_runner.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    373\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrial_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_trial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrial_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_running_trials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# blocking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    376\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrial_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_no_available_trials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.8/site-packages/ray/tune/trial_runner.py\u001b[0m in \u001b[0;36m_process_events\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    473\u001b[0m             \u001b[0;31m# TODO(ujvl): Consider combining get_next_available_trial and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m             \u001b[0;31m#  fetch_result functionality so that we don't timeout on fetch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m             \u001b[0mtrial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrial_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_next_available_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# blocking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_restoring\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mwarn_if_slow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"process_trial_restore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.8/site-packages/ray/tune/ray_trial_executor.py\u001b[0m in \u001b[0;36mget_next_available_trial\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    461\u001b[0m         \u001b[0;31m# See https://github.com/ray-project/ray/issues/4211 for details.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;34m[\u001b[0m\u001b[0mresult_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshuffled_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m         \u001b[0mwait_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait_time\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mNONTRIVIAL_WAIT_TIME_THRESHOLD_S\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.8/site-packages/ray/worker.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_refs, num_returns, timeout)\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1553\u001b[0m         \u001b[0mtimeout_milliseconds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1554\u001b[0;31m         ready_ids, remaining_ids = worker.core_worker.wait(\n\u001b[0m\u001b[1;32m   1555\u001b[0m             \u001b[0mobject_refs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1556\u001b[0m             \u001b[0mnum_returns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m E1009 12:19:12.917996  8339 378379712 node_manager.cc:3307] Failed to send get core worker stats request: IOError: 2: Stream removed\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.now()\n",
    "print(\"%-20s %s\" % (\"Start Time\", start_time))\n",
    "\n",
    "algo = HyperOptSearch()\n",
    "# algo = ConcurrencyLimiter(algo, max_concurrent=10)\n",
    "scheduler = AsyncHyperBandScheduler()\n",
    "\n",
    "tune_kwargs = {\n",
    "    \"num_samples\": 512,\n",
    "    \"config\": {\n",
    "        \"n_estimators\": tune.loguniform(100, 10000),\n",
    "        \"max_depth\": tune.randint(0, 6),\n",
    "        'min_child_weight': tune.randint(0, 6),\n",
    "        \"subsample\": tune.quniform(0.4, 0.9, 0.05),\n",
    "        \"colsample_bytree\": tune.quniform(0.05, 0.8, 0.05),\n",
    "        \"reg_alpha\": tune.loguniform(1e-04, 1),\n",
    "        \"reg_lambda\": tune.loguniform(1e-04, 100),\n",
    "        \"gamma\": 0,\n",
    "        \"learning_rate\": tune.loguniform(0.001, 0.1)\n",
    "    }\n",
    "}\n",
    "\n",
    "analysis = tune.run(my_xgb,\n",
    "                    name=\"my_xgb\",\n",
    "                    metric=\"mse\",\n",
    "                    mode=\"min\",\n",
    "                    search_alg=algo,\n",
    "                    scheduler=scheduler,\n",
    "                    verbose=1,\n",
    "                    **tune_kwargs)\n",
    "\n",
    "print(\"%-20s %s\" % (\"Start Time\", start_time))\n",
    "print(\"%-20s %s\" % (\"End Time\", datetime.now()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.results_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_results_df = analysis.results_df[['mse', 'date', 'time_this_iter_s',\n",
    "       'config.n_estimators', 'config.max_depth', 'config.min_child_weight', 'config.subsample',\n",
    "       'config.colsample_bytree', 'config.reg_alpha', 'config.reg_lambda', 'config.gamma',\n",
    "       'config.learning_rate']].sort_values('mse')\n",
    "analysis_results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = analysis_results_df.iloc[0]['config.max_depth']\n",
    "min_child_weight = analysis_results_df.iloc[0]['config.min_child_weight']\n",
    "subsample = analysis_results_df.iloc[0]['config.subsample']\n",
    "colsample_bytree = analysis_results_df.iloc[0]['config.colsample_bytree']\n",
    "reg_alpha = analysis_results_df.iloc[0]['config.reg_alpha']\n",
    "reg_lambda = analysis_results_df.iloc[0]['config.reg_lambda']\n",
    "reg_gamma = analysis_results_df.iloc[0]['config.gamma']\n",
    "learning_rate = analysis_results_df.iloc[0]['config.learning_rate']\n",
    "N_ESTIMATORS = analysis_results_df.iloc[0]['config.n_estimators']    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_config = {\n",
    "    'max_depth': 3,\n",
    "    'min_child_weight': 0,\n",
    "    'subsample': 0.9,\n",
    "    'colsample_bytree': 0.1,\n",
    "    'reg_alpha': 0.0734501,\n",
    "    'reg_lambda': 0.0247377,\n",
    "    'gamma': 0,\n",
    "    'learning_rate': 0.00994503,\n",
    "    'n_estimators':  5555\n",
    "}\n",
    "\n",
    "xgb = XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    random_state=RANDOMSTATE,    \n",
    "    verbosity=1,\n",
    "    n_jobs=-1,\n",
    "    **best_config\n",
    ")\n",
    "print(xgb)\n",
    "\n",
    "scores = -cross_val_score(xgb, df[predictors], df[response],\n",
    "                          scoring=\"neg_root_mean_squared_error\",\n",
    "                          cv=kfolds)\n",
    "print(\"CV Score %.04f (STD %.04f)\" % (np.mean(scores), np.std(scores)))\n",
    "print()\n",
    "\n",
    "xgb.fit(X_train[predictors], y_train)\n",
    "y_pred_train = xgb.predict(X_train[predictors])\n",
    "y_pred_test = xgb.predict(X_test[predictors])\n",
    "evaluate(y_train, y_pred_train, y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.results_df['config.max_depth'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bayesopt\n",
    "start_time = datetime.now()\n",
    "print(\"%-20s %s\" % (\"Start Time\", start_time))\n",
    "\n",
    "algo = BayesOptSearch(utility_kwargs={\n",
    "    \"kind\": \"ucb\",\n",
    "    \"kappa\": 2.5,\n",
    "    \"xi\": 0.0\n",
    "})\n",
    "    \n",
    "# algo = ConcurrencyLimiter(algo, max_concurrent=10)\n",
    "scheduler = AsyncHyperBandScheduler()\n",
    "\n",
    "tune_kwargs = {\n",
    "    \"num_samples\": 512,\n",
    "    \"config\": {\n",
    "        \"n_estimators\": tune.loguniform(100, 10000),\n",
    "        \"max_depth\": tune.randint(0, 6),\n",
    "        'min_child_weight': tune.randint(0, 6),\n",
    "        \"subsample\": tune.quniform(0.4, 0.9, 0.05),\n",
    "        \"colsample_bytree\": tune.quniform(0.05, 0.8, 0.05),\n",
    "        \"reg_alpha\": tune.loguniform(1e-04, 1),\n",
    "        \"reg_lambda\": tune.loguniform(1e-04, 100),\n",
    "        \"gamma\": 0,\n",
    "        \"learning_rate\": tune.loguniform(0.001, 0.1)\n",
    "    }\n",
    "}\n",
    "\n",
    "analysis = tune.run(my_xgb,\n",
    "                    name=\"my_xgb\",\n",
    "                    metric=\"mse\",\n",
    "                    mode=\"min\",\n",
    "                    search_alg=algo,\n",
    "                    scheduler=scheduler,\n",
    "                    verbose=1,\n",
    "                    **tune_kwargs)\n",
    "\n",
    "print(\"%-20s %s\" % (\"Start Time\", start_time))\n",
    "print(\"%-20s %s\" % (\"End Time\", datetime.now()))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf] *",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
