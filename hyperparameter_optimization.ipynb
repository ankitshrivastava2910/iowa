{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning with XGBoost, Ray Tune, Hyperopt and Optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "\n",
    "In this post we are going to demonstrate how we can speed up hyperparameter tuning with:\n",
    "\n",
    "1) Bayesian optimization tuning algos like HyperOpt and Optuna, running on…\n",
    "\n",
    "2) the [Ray](https://ray.io/) distributed ML framework, with a [unified API to many hyperparameter search algos](https://medium.com/riselab/cutting-edge-hyperparameter-tuning-with-ray-tune-be6c0447afdf) with early stopping and…\n",
    "\n",
    "3) a distributed cluster of cloud instances for even more speedup.\n",
    "\n",
    "### Outline:\n",
    "- Overview of hyperparameter tuning\n",
    "- Baseline linear regression with no hyperparameters\n",
    "- ElasticNet with L1 and L2 regularization using ElasticNetCV hyperparameter optimization\n",
    "- ElasticNet with GridSearchCV hyperparameter optimization\n",
    "- XGBoost: sequential grid search over hyperparameter subsets with early stopping \n",
    "- XGBoost: with HyperOpt and Optuna search algorithms\n",
    "- LightGBM: with HyperOpt and Optuna search algorithms\n",
    "- XGBoost: HyperOpt on a Ray cluster\n",
    "- LightGBM: HyperOpt on a Ray cluster\n",
    "- Concluding remarks\n",
    "18735\n",
    "\n",
    "But first, here are results on the Ames housing data set, predicting Iowa home prices:\n",
    "\n",
    "| ML Algo           | Hyperparameter search algo   | CV Error (RMSE in $)  | Time     |\n",
    "|-------------------|------------------------------|-----------------------|----------|\n",
    "| XGB               | Sequential Grid Search       | 18783                |   36:09  |\n",
    "| XGB               | HyperOpt (128 samples)       | 18770                | 13:36 |\n",
    "| XGB               | Optuna (256 samples)         | 18722                | 43:21 |\n",
    "| LightGBM          | HyperOpt (256 samples)       | 18612              |   45:08  |\n",
    "| LightGBM          | Optuna                       |  18534               |  | 34:54\n",
    "| XGB               | Optuna - 16-instance cluster | 18770                |   14:23  |\n",
    "| LightGBM          | Optuna - 16-instance cluster | 18612                |    4:22  |\n",
    "| Baseline:         |                              |\n",
    "| Linear Regression | --                           | 18192                |   0:01s  |\n",
    "| ElasticNet        | ElasticNetCV (Grid Search)   | 18122                |   0:02s  |          \n",
    "| ElasticNet        | GridSearchCV                 | 18061                |   0:05s  |          \n",
    "\n",
    "We see both speedup and RMSE improvement when using HyperOpt and Optuna, and the cluster. But our feature engineering was quite good and our simple linear model still outperforms boosting. (Not shown, SVR and KR are high-performing and an ensemble improves over all individual algos)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning Overview\n",
    "\n",
    "Here are [the principal approaches to hyperparameter tuning](https://en.wikipedia.org/wiki/Hyperparameter_optimization)\n",
    "\n",
    "- Grid search: given a finite set of discrete values for each hyperparameter, exhaustively cross-validate all combinations\n",
    "\n",
    "- Random search: given a discrete or continuous distribution for each hyperparameter, randomly sample from the joint distribution. Generally [more efficient than exhaustive grid search.](https://dl.acm.org/doi/10.5555/2188385.2188395 ) \n",
    "\n",
    "- Bayesian optimization: update the search space as you go based on outcomes of prior searches.\n",
    "\n",
    "- Gradient-based optimization: attempt to estimate the gradient of the CV metric with respect to the hyperparameter and ascend/descend the gradient.\n",
    "\n",
    "- Evolutionary optimization: sample the search space, discard combinations with poor metrics, and genetically evolve new combinations to try based on the successful combinations.\n",
    "\n",
    "- Population-based: A method of performing hyperparameter optimization at the same time as training.\n",
    "\n",
    "In this post we focus on Bayesian optimization with HyperOpt and Optuna. What is Bayesian optimization? When we perform a grid search, the search space can be considered a prior belief that the best hyperparameter vector is in the search space, and the combinations have equal probability of being the best combination. So we try them all and pick the best one.\n",
    "\n",
    "Perhaps we might do two passes of grid search. After an initial search on a broad, coarsely spaced grid, we might do a deeper dive in a smaller area around the best metric from the first pass, with a more finely-spaced grid. In Bayesian terminology we updated our prior belief.\n",
    "\n",
    "Bayesian optimization first samples randomly, e.g. 30 combinations, and computes the cross-validation metric for each combination. Then the algorithm updates the distribution it samples from, so it is more likely to sample combinations near the good metrics, and less likely to sample combinations near the poor metrics. As it continues to sample, it continues to update the search distribution based on the metrics it finds.\n",
    "\n",
    "Early stopping may also highly beneficial: often we can discard a combination without fully training it. In this post we use [ASHA](https://arxiv.org/abs/1810.05934). \n",
    "\n",
    "We use 4 regression algorithms:\n",
    "- LinearRegression: baseline with no hyperparameters\n",
    "- ElasticNet: Linear regression with L1 and L2 regularization (2 hyperparameters).\n",
    "- XGBoost\n",
    "- LightGBM\n",
    "\n",
    "We use 5 approaches :\n",
    "- *Native CV*: In sklearn if an algo has hyperparameters it will often have an xxxCV version which performs automated hyperparameter tuning over a search space with specified kfolds.\n",
    "- *GridSearchCV*: Abstracts CV for any sklearn algo, running multithreaded trials over specified folds. \n",
    "- *Manual sequential grid search*: What we typically do with XGBoost, which doesn't play well with GridSearchCV and has too many hyperparameters to tune in one pass.\n",
    "- *Ray on local machine*: HyperOpt and Optuna with early stopping.\n",
    "- *Ray on cluster*: Additionally scale out to run a single hyperparameter optimization task over many instances.\n",
    "\n",
    "We use data from the Ames Housing Dataset https://www.kaggle.com/c/house-prices-advanced-regression-techniques . The original data has 79 raw features. The data we will use has 100 features with a fair amount of feature engineering from [my own attempt at modeling](https://github.com/druce/iowa), which was in the top 5% or so when I submitted it to Kaggle.\n",
    "\n",
    "### Further reading: \n",
    " - [Hyper-Parameter Optimization: A Review of Algorithms and Applications](https://arxiv.org/abs/2003.05689) Tong Yu, Hong Zhu (2020)\n",
    " - [Hyperparameter Search in Machine Learning](https://arxiv.org/abs/1502.02127v2), Marc Claesen, Bart De Moor (2015)\n",
    " - [Hyperparameter Optimization](https://link.springer.com/chapter/10.1007/978-3-030-05318-5_1), Matthias Feurer, Frank Hutter (2019) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-18 10:51:17.716648\n",
      "numpy                1.19.1\n",
      "pandas               1.1.3\n",
      "sklearn              0.23.2\n",
      "xgboost              1.2.0\n",
      "lightgbm             2.3.0\n",
      "ray                  1.0.0\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import random\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression, ElasticNet, ElasticNetCV, Ridge, RidgeCV\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, GridSearchCV, KFold\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#!conda install -y -c conda-forge  xgboost \n",
    "import xgboost\n",
    "from xgboost import XGBRegressor\n",
    "from xgboost import plot_importance\n",
    "\n",
    "import lightgbm\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.suggest import ConcurrencyLimiter\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "from ray.tune.suggest.hyperopt import HyperOptSearch\n",
    "from ray.tune.suggest.optuna import OptunaSearch\n",
    "from ray.tune.logger import DEFAULT_LOGGERS\n",
    "# from ray.tune.integration.wandb import WandbLogger\n",
    "\n",
    "# wandb is great for real-time tracking but not necessary\n",
    "# import wandb\n",
    "# from ray.tune.integration.wandb import wandb_mixin\n",
    "# from wandb.lightgbm import wandb_callback\n",
    "# os.environ['WANDB_NOTEBOOK_NAME']='hyperparameter_optimization'\n",
    "# os.environ[\"WANDB_RUN_GROUP\"] = \"experiment-\" + wandb.util.generate_id()\n",
    "# wandb.init(group=\"experiment_1\", job_type=\"eval\")\n",
    "\n",
    "print(datetime.now())\n",
    "\n",
    "print (\"%-20s %s\"% (\"numpy\", np.__version__))\n",
    "print (\"%-20s %s\"% (\"pandas\", pd.__version__))\n",
    "print (\"%-20s %s\"% (\"sklearn\", sklearn.__version__))\n",
    "print (\"%-20s %s\"% (\"xgboost\", xgboost.__version__))\n",
    "print (\"%-20s %s\"% (\"lightgbm\", lightgbm.__version__))\n",
    "print (\"%-20s %s\"% (\"ray\", ray.__version__))\n",
    "\n",
    "pd.set_option('max_colwidth', 1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed for reproducibility\n",
    "RANDOMSTATE = 42\n",
    "np.random.seed(RANDOMSTATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>BsmtFullBath</th>\n",
       "      <th>FullBath</th>\n",
       "      <th>KitchenAbvGr</th>\n",
       "      <th>GarageYrBlt</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>1stFlrSF</th>\n",
       "      <th>GrLivArea</th>\n",
       "      <th>GarageArea</th>\n",
       "      <th>...</th>\n",
       "      <th>SaleCondition_1</th>\n",
       "      <th>SaleCondition_2</th>\n",
       "      <th>SaleCondition_5</th>\n",
       "      <th>SaleType_4</th>\n",
       "      <th>BedroomAbvGr_1</th>\n",
       "      <th>BedroomAbvGr_4</th>\n",
       "      <th>BedroomAbvGr_5</th>\n",
       "      <th>HalfBath_1</th>\n",
       "      <th>TotalBath_1.0</th>\n",
       "      <th>TotalBath_2.5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>65.0</td>\n",
       "      <td>196.0</td>\n",
       "      <td>856</td>\n",
       "      <td>1710</td>\n",
       "      <td>548.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1262</td>\n",
       "      <td>1262</td>\n",
       "      <td>460.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>68.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>920</td>\n",
       "      <td>1786</td>\n",
       "      <td>608.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>95</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>961</td>\n",
       "      <td>1717</td>\n",
       "      <td>642.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>84.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>1145</td>\n",
       "      <td>2198</td>\n",
       "      <td>836.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    YearBuilt  BsmtFullBath  FullBath  KitchenAbvGr  GarageYrBlt  LotFrontage  \\\n",
       "Id                                                                              \n",
       "1           7             1         2             1            7         65.0   \n",
       "2          34             0         2             1           34         80.0   \n",
       "3           9             1         2             1            9         68.0   \n",
       "4          95             1         1             1           12         60.0   \n",
       "5          10             1         2             1           10         84.0   \n",
       "\n",
       "    MasVnrArea  1stFlrSF  GrLivArea  GarageArea  ...  SaleCondition_1  \\\n",
       "Id                                               ...                    \n",
       "1        196.0       856       1710       548.0  ...                0   \n",
       "2          0.0      1262       1262       460.0  ...                0   \n",
       "3        162.0       920       1786       608.0  ...                0   \n",
       "4          0.0       961       1717       642.0  ...                1   \n",
       "5        350.0      1145       2198       836.0  ...                0   \n",
       "\n",
       "    SaleCondition_2  SaleCondition_5  SaleType_4  BedroomAbvGr_1  \\\n",
       "Id                                                                 \n",
       "1                 0                0           1               0   \n",
       "2                 0                0           1               0   \n",
       "3                 0                0           1               0   \n",
       "4                 0                0           1               0   \n",
       "5                 0                0           1               0   \n",
       "\n",
       "    BedroomAbvGr_4  BedroomAbvGr_5  HalfBath_1  TotalBath_1.0  TotalBath_2.5  \n",
       "Id                                                                            \n",
       "1                0               0           1              0              0  \n",
       "2                0               0           0              0              1  \n",
       "3                0               0           1              0              0  \n",
       "4                0               0           0              0              0  \n",
       "5                1               0           1              0              0  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.247699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.109016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12.317171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11.849405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12.429220</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    SalePrice\n",
       "Id           \n",
       "1   12.247699\n",
       "2   12.109016\n",
       "3   12.317171\n",
       "4   11.849405\n",
       "5   12.429220"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import train data\n",
    "df = pd.read_pickle('df_train.pickle')\n",
    "\n",
    "response = 'SalePrice'\n",
    "predictors = ['YearBuilt',\n",
    "              'BsmtFullBath',\n",
    "              'FullBath',\n",
    "              'KitchenAbvGr',\n",
    "              'GarageYrBlt',\n",
    "              'LotFrontage',\n",
    "              'MasVnrArea',\n",
    "              '1stFlrSF',\n",
    "              'GrLivArea',\n",
    "              'GarageArea',\n",
    "              'WoodDeckSF',\n",
    "              'PorchSF',\n",
    "              'AvgBltRemod',\n",
    "              'FireBathRatio',\n",
    "              'TotalSF x OverallQual x OverallCond',\n",
    "              'AvgBltRemod x Functional x TotalFinSF',\n",
    "              'Functional x OverallQual',\n",
    "              'KitchenAbvGr x KitchenQual',\n",
    "              'GarageCars x GarageYrBlt',\n",
    "              'GarageQual x GarageCond x GarageCars',\n",
    "              'HeatingQC x Heating',\n",
    "              'monthnum',\n",
    "              'log_YearBuilt',\n",
    "              'log_LotArea',\n",
    "              'log_TotalFinSF',\n",
    "              'log_GarageRatio',\n",
    "              'log_TotalSF x OverallQual x OverallCond',\n",
    "              'log_TotalSF x OverallCond',\n",
    "              'log_AvgBltRemod x TotalFinSF',\n",
    "              'sq_2ndFlrSF',\n",
    "              'sq_BsmtFinSF',\n",
    "              'sq_BsmtFinSF x BsmtQual',\n",
    "              'sq_BsmtFinSF x BsmtBath',\n",
    "              'BldgType_4',\n",
    "              'BsmtExposure_1',\n",
    "              'BsmtExposure_4',\n",
    "              'BsmtFinType1_1',\n",
    "              'BsmtFinType1_2',\n",
    "              'BsmtFinType1_4',\n",
    "              'BsmtFinType1_5',\n",
    "              'BsmtFinType1_6',\n",
    "              'CentralAir_0',\n",
    "              'CentralAir_1',\n",
    "              'Condition1_1',\n",
    "              'Condition1_3',\n",
    "              'ExterCond_2',\n",
    "              'ExterQual_2',\n",
    "              'Exterior1st_4',\n",
    "              'Exterior1st_5',\n",
    "              'Exterior1st_10',\n",
    "              'Fence_0',\n",
    "              'Fence_2',\n",
    "              'Foundation_1',\n",
    "              'Foundation_5',\n",
    "              'GarageCars_1',\n",
    "              'GarageFinish_2',\n",
    "              'GarageFinish_3',\n",
    "              'GarageType_2',\n",
    "              'HouseStyle_2',\n",
    "              'KitchenQual_4',\n",
    "              'LotConfig_0',\n",
    "              'LotConfig_4',\n",
    "              'MSSubClass_30',\n",
    "              'MSSubClass_70',\n",
    "              'MSZoning_0',\n",
    "              'MSZoning_1',\n",
    "              'MSZoning_4',\n",
    "              'MasVnrType_2',\n",
    "              'MasVnrType_3',\n",
    "              'MoSold_1',\n",
    "              'MoSold_5',\n",
    "              'MoSold_6',\n",
    "              'MoSold_11',\n",
    "              'Neighborhood_3',\n",
    "              'Neighborhood_4',\n",
    "              'Neighborhood_5',\n",
    "              'Neighborhood_10',\n",
    "              'Neighborhood_11',\n",
    "              'Neighborhood_16',\n",
    "              'Neighborhood_17',\n",
    "              'Neighborhood_19',\n",
    "              'Neighborhood_22',\n",
    "              'Neighborhood_24',\n",
    "              'OverallCond_7',\n",
    "              'OverallQual_5',\n",
    "              'OverallQual_6',\n",
    "              'OverallQual_7',\n",
    "              'OverallQual_9',\n",
    "              'PavedDrive_0',\n",
    "              'PavedDrive_2',\n",
    "              'SaleCondition_1',\n",
    "              'SaleCondition_2',\n",
    "              'SaleCondition_5',\n",
    "              'SaleType_4',\n",
    "              'BedroomAbvGr_1',\n",
    "              'BedroomAbvGr_4',\n",
    "              'BedroomAbvGr_5',\n",
    "              'HalfBath_1',\n",
    "              'TotalBath_1.0',\n",
    "              'TotalBath_2.5']\n",
    "\n",
    "display(df[predictors].head())\n",
    "display(df[[response]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are training on a response which is the log of 1 + the sale price\n",
    "# transform prediction back to original basis with expm1 and evaluate vs. original\n",
    "\n",
    "MEAN_RESPONSE=df[response].mean()\n",
    "def cv_to_raw(cv_val, mean_response=MEAN_RESPONSE):\n",
    "    \"\"\"convert log1p rmse to underlying SalePrice error\"\"\"\n",
    "    # MEAN_RESPONSE assumes folds have same mean response, which is true in expectation but not in each fold\n",
    "    # we can also pass the actual response for each fold\n",
    "    # but we're usually looking to consistently convert the log value to a more meaningful unit\n",
    "    return np.expm1(mean_response+cv_val) - np.expm1(mean_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# always use same k-folds for reproducibility\n",
    "kfolds = KFold(n_splits=10, shuffle=True, random_state=RANDOMSTATE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline linear regression\n",
    "- Raw CV RMSE 18191.9791"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression\n",
      "100 predictors\n",
      "\n",
      "Log1p CV RMSE 0.1037 (STD 0.0099)\n",
      "Raw CV RMSE 18192 (STD 1839)\n",
      "CPU times: user 76 ms, sys: 76 ms, total: 152 ms\n",
      "Wall time: 1.04 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Tune lr search space for alphas and l1_ratio\n",
    "print(\"LinearRegression\")\n",
    "\n",
    "print(len(predictors), \"predictors\")\n",
    "\n",
    "lr = LinearRegression()\n",
    "\n",
    "# evaluate using kfolds\n",
    "scores = -cross_val_score(lr, df[predictors], df[response],\n",
    "                          scoring=\"neg_root_mean_squared_error\",\n",
    "                          cv=kfolds,\n",
    "                          n_jobs=-1)\n",
    "raw_scores = [cv_to_raw(x) for x in scores]\n",
    "print()\n",
    "print(\"Log1p CV RMSE %.04f (STD %.04f)\" % (np.mean(scores), np.std(scores)))\n",
    "print(\"Raw CV RMSE %.0f (STD %.0f)\" % (np.mean(raw_scores), np.std(raw_scores)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Native Sklearn xxxCV\n",
    "- LogisticRegressionCV, LassoCV, RidgeCV, ElasticNetCV, etc.\n",
    "- Test many hyperparameters in parallel with multithreading\n",
    "- Note improvement vs. LinearRegression due to controlling overfitting\n",
    "- RMSE $18103\n",
    "- Time 5s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElasticnetCV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      ".............................................................................................................................................................................................................................................................................................[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    0.3s\n",
      ".....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................[Parallel(n_jobs=-1)]: Done 130 out of 130 | elapsed:    0.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l1_ratio 0.01\n",
      "alpha 0.0031622776601683794\n",
      "\n",
      "Log1p CV RMSE 0.1030 (STD 0.0109)\n",
      "Raw CV RMSE 18061 (STD 2008)\n",
      "CPU times: user 5.93 s, sys: 3.67 s, total: 9.6 s\n",
      "Wall time: 1.61 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Tune elasticnet search space for alphas and L1_ratio\n",
    "# predictor selection used to create the training set used lasso\n",
    "# so l1 parameter is close to 0\n",
    "# could use ridge (eg elasticnet with 0 L1 regularization)\n",
    "# but then only 1 param, more general and useful to do this with elasticnet\n",
    "print(\"ElasticnetCV\")\n",
    "\n",
    "# make pipeline\n",
    "# with regularization must scale predictors\n",
    "elasticnetcv = make_pipeline(RobustScaler(),\n",
    "                             ElasticNetCV(max_iter=100000, \n",
    "                                          l1_ratio=[0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.99],\n",
    "                                          alphas=np.logspace(-4, -2, 9),\n",
    "                                          cv=kfolds,\n",
    "                                          n_jobs=-1,\n",
    "                                          verbose=1,\n",
    "                                         ))\n",
    "\n",
    "#train and get hyperparams\n",
    "elasticnetcv.fit(df[predictors], df[response])\n",
    "l1_ratio = elasticnetcv._final_estimator.l1_ratio_\n",
    "alpha = elasticnetcv._final_estimator.alpha_\n",
    "print('l1_ratio', l1_ratio)\n",
    "print('alpha', alpha)\n",
    "\n",
    "# evaluate using kfolds on full dataset\n",
    "# I don't see API to get CV error from elasticnetcv, so we use cross_val_score\n",
    "elasticnet = ElasticNet(alpha=alpha,\n",
    "                        l1_ratio=l1_ratio,\n",
    "                        max_iter=10000)\n",
    "\n",
    "scores = -cross_val_score(elasticnet, df[predictors], df[response],\n",
    "                          scoring=\"neg_root_mean_squared_error\",\n",
    "                          cv=kfolds,\n",
    "                          n_jobs=-1)\n",
    "raw_scores = [cv_to_raw(x) for x in scores]\n",
    "print()\n",
    "print(\"Log1p CV RMSE %.04f (STD %.04f)\" % (np.mean(scores), np.std(scores)))\n",
    "print(\"Raw CV RMSE %.0f (STD %.0f)\" % (np.mean(raw_scores), np.std(raw_scores)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearchCV\n",
    "- Useful for algos with no native multithreaded xxxCV\n",
    "- Test many hyperparameter combinations in parallel with multithreading\n",
    "- Similar result vs ElasticNetCV, not exact, need more research as to why\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CV on full dataset\n",
      "Fitting 10 folds for each of 117 candidates, totalling 1170 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  30 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 584 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=-1)]: Done 1170 out of 1170 | elapsed:    4.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params {'alpha': 0.0031622776601683794, 'l1_ratio': 0.01}\n",
      "best score 0.10247177583755482\n",
      "Log1p CV RMSE 0.320112\n",
      "Raw CV RMSE 62767\n",
      "ElasticNet(alpha=0.0031622776601683794, l1_ratio=0.01, max_iter=100000)\n",
      "\n",
      "Log1p CV RMSE 0.103003 (STD 0.0109)\n",
      "Raw CV RMSE 18061 (STD 2008)\n",
      "weighted average 0.103023\n",
      "CPU times: user 2.08 s, sys: 612 ms, total: 2.7 s\n",
      "Wall time: 5.14 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gs = make_pipeline(RobustScaler(),\n",
    "                   GridSearchCV(ElasticNet(max_iter=100000),\n",
    "                                param_grid={'l1_ratio': [0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.99],\n",
    "                                            'alpha': np.logspace(-4, -2, 9),\n",
    "                                           },\n",
    "                                scoring='neg_root_mean_squared_error',\n",
    "                                refit=True,\n",
    "                                cv=kfolds,\n",
    "                                n_jobs=-1,\n",
    "                                verbose=1\n",
    "                               ))\n",
    "\n",
    "# do cv using kfolds on full dataset\n",
    "print(\"\\nCV on full dataset\")\n",
    "gs.fit(df[predictors], df[response])\n",
    "print('best params', gs._final_estimator.best_params_)\n",
    "print('best score', -gs._final_estimator.best_score_)\n",
    "l1_ratio = gs._final_estimator.best_params_['l1_ratio']\n",
    "alpha = gs._final_estimator.best_params_['alpha']\n",
    "print(\"Log1p CV RMSE %.06f\" % (np.sqrt(-gs._final_estimator.best_score_)))\n",
    "print(\"Raw CV RMSE %.0f\" % (cv_to_raw(np.sqrt(-gs._final_estimator.best_score_))))\n",
    "\n",
    "# we shouldn't need to do this\n",
    "elasticnet = ElasticNet(alpha=alpha,\n",
    "                        l1_ratio=l1_ratio,\n",
    "                        max_iter=100000)\n",
    "print(elasticnet)\n",
    "\n",
    "scores = -cross_val_score(elasticnet, df[predictors], df[response],\n",
    "                          scoring=\"neg_root_mean_squared_error\",\n",
    "                          cv=kfolds,\n",
    "                          n_jobs=-1)\n",
    "raw_scores = [cv_to_raw(x) for x in scores]\n",
    "print()\n",
    "print(\"Log1p CV RMSE %.06f (STD %.04f)\" % (np.mean(scores), np.std(scores)))\n",
    "print(\"Raw CV RMSE %.0f (STD %.0f)\" % (np.mean(raw_scores), np.std(raw_scores)))\n",
    "\n",
    "# difference in average CV scores reported by GridSearchCV and cross_val_score\n",
    "# with same alpha, l1_ratio, kfolds\n",
    "# one reason could be that we used simple average, GridSearchCV is weighted by # of samples per fold?\n",
    "nsamples = [len(z[1]) for z in kfolds.split(df)]\n",
    "print(\"weighted average %.06f\" % np.average(scores, weights=nsamples))\n",
    "# still tiny difference, not sure why, also ElasticSearchCV shows fewer fits, takes less time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElasticNet(alpha=0.0031622776601683794, l1_ratio=0.01, max_iter=10000)\n",
      "0.103003\n"
     ]
    }
   ],
   "source": [
    "# roll-our-own CV \n",
    "# matches cross_val_score\n",
    "alpha = 0.0031622776601683794\n",
    "l1_ratio = 0.01\n",
    "regressor = ElasticNet(alpha=alpha,\n",
    "                       l1_ratio=l1_ratio,\n",
    "                       max_iter=10000)\n",
    "print(regressor)\n",
    "cverrors = []\n",
    "for train_fold, cv_fold in kfolds.split(df): \n",
    "    fold_X_train=df[predictors].values[train_fold]\n",
    "    fold_y_train=df[response].values[train_fold]\n",
    "    fold_X_test=df[predictors].values[cv_fold]\n",
    "    fold_y_test=df[response].values[cv_fold]\n",
    "    regressor.fit(fold_X_train, fold_y_train)\n",
    "    y_pred_test=regressor.predict(fold_X_test)\n",
    "    cverrors.append(np.sqrt(mean_squared_error(fold_y_test, y_pred_test)))\n",
    "    \n",
    "print(\"%.06f\" % np.average(cverrors))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 702 candidates, totalling 7020 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 584 tasks      | elapsed:    3.0s\n",
      "[Parallel(n_jobs=-1)]: Done 1584 tasks      | elapsed:    7.8s\n",
      "[Parallel(n_jobs=-1)]: Done 2984 tasks      | elapsed:   14.6s\n",
      "[Parallel(n_jobs=-1)]: Done 4784 tasks      | elapsed:   23.3s\n",
      "[Parallel(n_jobs=-1)]: Done 6975 tasks      | elapsed:   33.7s\n",
      "[Parallel(n_jobs=-1)]: Done 7020 out of 7020 | elapsed:   33.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params {'elasticnet__alpha': 0.0017782794100389228, 'elasticnet__l1_ratio': 0.1, 'pca__n_components': 100}\n",
      "best score 0.10225515641215403\n",
      "\n",
      "Log1p CV RMSE 0.102255 (STD 0.0120)\n",
      "Raw CV RMSE 17925 (STD 2213)\n"
     ]
    }
   ],
   "source": [
    "# try PCA\n",
    "pipe = Pipeline(steps=[ ('scaler', RobustScaler()), ('pca', PCA()), ('elasticnet', ElasticNet(max_iter=10000))])\n",
    "\n",
    "param_grid = {\n",
    "    'pca__n_components': [5, 10, 15, 25, 50, 100],\n",
    "    'elasticnet__l1_ratio': [0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.99],\n",
    "    'elasticnet__alpha': np.logspace(-4, -2, 9),\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe, param_grid,\n",
    "                  scoring='neg_root_mean_squared_error',\n",
    "                  refit=True,\n",
    "                  cv=kfolds,\n",
    "                  n_jobs=-1,\n",
    "                  verbose=1)\n",
    "\n",
    "gs.fit(df[predictors], df[response])\n",
    "print('best params', gs.best_params_)\n",
    "print('best score', -gs.best_score_)\n",
    "\n",
    "pipe = Pipeline(steps=[('scaler', RobustScaler()),\n",
    "                       ('pca', PCA(n_components=gs.best_params_['pca__n_components'])),  \n",
    "                       ('elasticnet', ElasticNet(alpha=gs.best_params_['elasticnet__alpha'], \n",
    "                                                 l1_ratio=gs.best_params_['elasticnet__l1_ratio'], \n",
    "                                                 max_iter=10000))])\n",
    "scores = -cross_val_score(pipe, df[predictors], df[response],\n",
    "                          scoring=\"neg_root_mean_squared_error\",\n",
    "                          cv=kfolds,\n",
    "                          n_jobs=-1)\n",
    "raw_scores = [cv_to_raw(x) for x in scores]\n",
    "print()\n",
    "print(\"Log1p CV RMSE %.06f (STD %.04f)\" % (np.mean(scores), np.std(scores)))\n",
    "print(\"Raw CV RMSE %.0f (STD %.0f)\" % (np.mean(raw_scores), np.std(raw_scores)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Sequential Grid Search \n",
    "- XGBoost has native multithreading, CV\n",
    "- XGBoost has many tuning parameters so a complete grid search has an unreasonable number of combinations\n",
    "- We tune reduced sets sequentially and use early stopping. \n",
    "\n",
    "### Tuning methodology\n",
    "- Set an initial set of starting parameters\n",
    "- Do 10-fold CV\n",
    "- Use early stopping to halt training in each fold if no improvement after eg 100 rounds, pick hyperparameters to minimize average error over kfolds\n",
    "- Tune sequentially on groups of hyperparameters that don't interact too much between groups to reduce combinations\n",
    "- Tune max_depth and min_child_weight \n",
    "- Tune subsample and colsample_bytree\n",
    "- Tune alpha, lambda and gamma (regularization)\n",
    "- Tune learning rate: lower learning rate will need more rounds/n_estimators\n",
    "- Retrain and evaluate on full dataset with best learning rate and best n_estimators (average stopping point over kfolds)\n",
    "\n",
    "### Notes\n",
    "- It doesn't seem possible to get XGBoost early stopping and also use GridSearchCV. GridSearchCV doesn't pass the kfolds in a way that XGboost understands for early stopping\n",
    "- 2 alternative approaches \n",
    "    - use native xgboost .cv which understands early stopping but doesn't use sklearn API (uses DMatrix, not np array or dataframe)\n",
    "    - use sklearn API and roll our own grid search instead of GridSearchCV (used below)\n",
    "- XGboost terminology differs from sklearn\n",
    "    - boost_rounds = n_estimators\n",
    "    - eta = learning_rate\n",
    "- parameter reference: https://xgboost.readthedocs.io/en/latest/parameter.html\n",
    "- training reference: https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.training\n",
    "- times are wall times on an amazon t2.2xlarge instance  \n",
    "- to set up environment:\n",
    "    - `conda create --name hyperparam python=3.8`\n",
    "    - `conda activate hyperparam`\n",
    "    - `conda install jupyter`\n",
    "    - `pip install -r requirements.txt`\n",
    "- round 1 Wall time: 6min 23s\n",
    "- round 2 Wall time: 19min 22s\n",
    "- round 3 Wall time: 5min 30s\n",
    "- round 4 Wall time: 4min 54s\n",
    "- total time 36:09\n",
    "- RMSE 18783.117031"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOOST_ROUNDS=50000   # we use early stopping so make this arbitrarily high\n",
    "EARLY_STOPPING_ROUNDS=100 # stop if no improvement after 100 rounds\n",
    "\n",
    "def my_cv(df, predictors, response, kfolds, regressor, verbose=False):\n",
    "    \"\"\"Roll our own CV \n",
    "    train each kfold with early stopping\n",
    "    return average metric, sd over kfolds, average best round\"\"\"\n",
    "    metrics = []\n",
    "    best_iterations = []\n",
    "\n",
    "    for train_fold, cv_fold in kfolds.split(df): \n",
    "        fold_X_train=df[predictors].values[train_fold]\n",
    "        fold_y_train=df[response].values[train_fold]\n",
    "        fold_X_test=df[predictors].values[cv_fold]\n",
    "        fold_y_test=df[response].values[cv_fold]\n",
    "        regressor.fit(fold_X_train, fold_y_train,\n",
    "                      early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n",
    "                      eval_set=[(fold_X_test, fold_y_test)],\n",
    "                      eval_metric='rmse',\n",
    "                      verbose=verbose\n",
    "                     )\n",
    "        y_pred_test=regressor.predict(fold_X_test)\n",
    "        metrics.append(np.sqrt(mean_squared_error(fold_y_test, y_pred_test)))\n",
    "        best_iterations.append(regressor.best_iteration)\n",
    "    return np.average(metrics), np.std(metrics), np.average(best_iterations)\n",
    "\n",
    "def cv_over_param_dict(df, param_dict, predictors, response, kfolds, verbose=False):\n",
    "    \"\"\"given a list of dictionaries of xgb params\n",
    "    run my_cv on params, store result in array\n",
    "    return results\n",
    "    \"\"\"\n",
    "    start_time = datetime.now()\n",
    "    print(\"%-20s %s\" % (\"Start Time\", start_time))\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for i, d in enumerate(param_dict):\n",
    "        xgb = XGBRegressor(\n",
    "            objective='reg:squarederror',\n",
    "            n_estimators=BOOST_ROUNDS,\n",
    "            random_state=RANDOMSTATE,    \n",
    "            verbosity=1,\n",
    "            n_jobs=-1,\n",
    "            booster='gbtree',   \n",
    "            **d\n",
    "        )    \n",
    "\n",
    "        metric_rmse, metric_std, best_iteration = my_cv(df, predictors, response, kfolds, xgb, verbose=False)    \n",
    "        results.append([metric_rmse, metric_std, best_iteration, d])\n",
    "    \n",
    "        print(\"%s %3d result mean: %.6f std: %.6f, iter: %.2f\" % (datetime.strftime(datetime.now(), \"%T\"), i, metric_rmse, metric_std, best_iteration))\n",
    "        \n",
    "    end_time = datetime.now()\n",
    "    print(\"%-20s %s\" % (\"Start Time\", start_time))\n",
    "    print(\"%-20s %s\" % (\"End Time\", end_time))\n",
    "    print(str(timedelta(seconds=(end_time-start_time).seconds)))\n",
    "    \n",
    "    results_df = pd.DataFrame(results, columns=['rmse', 'std', 'best_iter', 'param_dict']).sort_values('rmse')\n",
    "    display(results_df.head())\n",
    "    \n",
    "    best_params = results_df.iloc[0]['param_dict']\n",
    "    return best_params, results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Time           2020-10-18 11:20:58.365236\n",
      "11:21:26   0 result mean: 0.108542 std: 0.011448, iter: 2728.90\n",
      "11:21:54   1 result mean: 0.108123 std: 0.011366, iter: 2225.30\n",
      "11:22:29   2 result mean: 0.107936 std: 0.011714, iter: 2229.00\n",
      "11:23:04   3 result mean: 0.108681 std: 0.011357, iter: 1884.00\n",
      "11:23:41   4 result mean: 0.109373 std: 0.011591, iter: 1729.70\n",
      "11:24:24   5 result mean: 0.109264 std: 0.011672, iter: 1740.10\n",
      "Start Time           2020-10-18 11:20:58.365236\n",
      "End Time             2020-10-18 11:24:24.034127\n",
      "0:03:25\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rmse</th>\n",
       "      <th>std</th>\n",
       "      <th>best_iter</th>\n",
       "      <th>param_dict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.107936</td>\n",
       "      <td>0.011714</td>\n",
       "      <td>2229.0</td>\n",
       "      <td>{'max_depth': 4, 'colsample_bytree': 0.5, 'colsample_bylevel': 0.5, 'subsample': 0.5, 'learning_rate': 0.01}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.108123</td>\n",
       "      <td>0.011366</td>\n",
       "      <td>2225.3</td>\n",
       "      <td>{'max_depth': 3, 'colsample_bytree': 0.5, 'colsample_bylevel': 0.5, 'subsample': 0.5, 'learning_rate': 0.01}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.108542</td>\n",
       "      <td>0.011448</td>\n",
       "      <td>2728.9</td>\n",
       "      <td>{'max_depth': 2, 'colsample_bytree': 0.5, 'colsample_bylevel': 0.5, 'subsample': 0.5, 'learning_rate': 0.01}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.108681</td>\n",
       "      <td>0.011357</td>\n",
       "      <td>1884.0</td>\n",
       "      <td>{'max_depth': 5, 'colsample_bytree': 0.5, 'colsample_bylevel': 0.5, 'subsample': 0.5, 'learning_rate': 0.01}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.109264</td>\n",
       "      <td>0.011672</td>\n",
       "      <td>1740.1</td>\n",
       "      <td>{'max_depth': 7, 'colsample_bytree': 0.5, 'colsample_bylevel': 0.5, 'subsample': 0.5, 'learning_rate': 0.01}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       rmse       std  best_iter  \\\n",
       "2  0.107936  0.011714     2229.0   \n",
       "1  0.108123  0.011366     2225.3   \n",
       "0  0.108542  0.011448     2728.9   \n",
       "3  0.108681  0.011357     1884.0   \n",
       "5  0.109264  0.011672     1740.1   \n",
       "\n",
       "                                                                                                     param_dict  \n",
       "2  {'max_depth': 4, 'colsample_bytree': 0.5, 'colsample_bylevel': 0.5, 'subsample': 0.5, 'learning_rate': 0.01}  \n",
       "1  {'max_depth': 3, 'colsample_bytree': 0.5, 'colsample_bylevel': 0.5, 'subsample': 0.5, 'learning_rate': 0.01}  \n",
       "0  {'max_depth': 2, 'colsample_bytree': 0.5, 'colsample_bylevel': 0.5, 'subsample': 0.5, 'learning_rate': 0.01}  \n",
       "3  {'max_depth': 5, 'colsample_bytree': 0.5, 'colsample_bylevel': 0.5, 'subsample': 0.5, 'learning_rate': 0.01}  \n",
       "5  {'max_depth': 7, 'colsample_bytree': 0.5, 'colsample_bylevel': 0.5, 'subsample': 0.5, 'learning_rate': 0.01}  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 24s, sys: 808 ms, total: 3min 25s\n",
      "Wall time: 3min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# set initial XGboost parameters\n",
    "# run round 1 and get max_depth, min_child_weight based on best values \n",
    "# update parameters and run round 2 for subsample and colsample_bytree \n",
    "# update parameters and run round 3 for learning_rate and best n_iterations\n",
    "# this is not an exhaustive list but a representative list of most important parameters to tune\n",
    "# see https://xgboost.readthedocs.io/en/latest/parameter.html for all parameters\n",
    "# https://sites.google.com/view/lauraepp/parameters\n",
    "\n",
    "# early_stopping_rounds gives a warning but seems to be used\n",
    "# seems like XGB doesn't like it when you use sklearn API beyond the basics\n",
    "# possibly the native .cv might be the way to go\n",
    "\n",
    "# initial hyperparams\n",
    "current_params = {\n",
    "    'max_depth': 5,\n",
    "    'colsample_bytree': 0.5,\n",
    "    'colsample_bylevel': 0.5,\n",
    "    'subsample': 0.5,\n",
    "    'learning_rate': 0.01,\n",
    "}\n",
    "\n",
    "##################################################\n",
    "# round 1: tune depth , max_leaves\n",
    "##################################################\n",
    "max_depths = list(range(2,8))\n",
    "# max_leavess = [1, 3, 10, 30, 100] #  doesn't matter\n",
    "# grid_search_dicts = [dict(zip(['max_depth', 'max_leaves'], [a, b])) \n",
    "#                      for a,b in product(max_depths, max_leavess)]\n",
    "grid_search_dicts = [{'max_depth': md} for md in max_depths]\n",
    "# merge into full param dicts\n",
    "full_search_dicts = [{**current_params, **d} for d in grid_search_dicts]\n",
    "\n",
    "# cv and get best params\n",
    "current_params, results_df = cv_over_param_dict(df, full_search_dicts, predictors, response, kfolds, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Time           2020-10-18 11:24:24.051740\n",
      "11:24:42   0 result mean: 0.107415 std: 0.013652, iter: 3195.50\n",
      "11:25:00   1 result mean: 0.107415 std: 0.013652, iter: 3195.50\n",
      "11:25:15   2 result mean: 0.106196 std: 0.014139, iter: 2341.50\n",
      "11:25:30   3 result mean: 0.106196 std: 0.014139, iter: 2341.50\n",
      "11:25:43   4 result mean: 0.106210 std: 0.013387, iter: 1991.40\n",
      "11:26:02   5 result mean: 0.107478 std: 0.014917, iter: 3299.40\n",
      "11:26:17   6 result mean: 0.106020 std: 0.014010, iter: 2343.20\n",
      "11:26:31   7 result mean: 0.106835 std: 0.013553, iter: 1999.70\n",
      "11:26:44   8 result mean: 0.106835 std: 0.013553, iter: 1999.70\n",
      "11:26:56   9 result mean: 0.107007 std: 0.014161, iter: 1728.80\n",
      "11:27:10  10 result mean: 0.105374 std: 0.014054, iter: 2195.20\n",
      "11:27:23  11 result mean: 0.105629 std: 0.013575, iter: 1997.30\n",
      "11:27:36  12 result mean: 0.106201 std: 0.013386, iter: 1833.70\n",
      "11:27:48  13 result mean: 0.107130 std: 0.013648, iter: 1669.20\n",
      "11:28:01  14 result mean: 0.107105 std: 0.013030, iter: 1660.00\n",
      "11:28:15  15 result mean: 0.106526 std: 0.013503, iter: 2217.80\n",
      "11:28:28  16 result mean: 0.106112 std: 0.012683, iter: 2024.10\n",
      "11:28:43  17 result mean: 0.106293 std: 0.012290, iter: 1949.50\n",
      "11:28:56  18 result mean: 0.106870 std: 0.012249, iter: 1784.00\n",
      "11:29:09  19 result mean: 0.107610 std: 0.012640, iter: 1682.40\n",
      "11:29:24  20 result mean: 0.106392 std: 0.013446, iter: 2182.60\n",
      "11:29:37  21 result mean: 0.106857 std: 0.013721, iter: 1915.50\n",
      "11:29:50  22 result mean: 0.106842 std: 0.013588, iter: 1657.20\n",
      "11:30:03  23 result mean: 0.107149 std: 0.012384, iter: 1679.60\n",
      "11:30:19  24 result mean: 0.106911 std: 0.012358, iter: 1962.80\n",
      "11:30:39  25 result mean: 0.106850 std: 0.013192, iter: 3413.90\n",
      "11:30:59  26 result mean: 0.106850 std: 0.013192, iter: 3413.90\n",
      "11:31:13  27 result mean: 0.106337 std: 0.014180, iter: 2250.80\n",
      "11:31:28  28 result mean: 0.106337 std: 0.014180, iter: 2250.80\n",
      "11:31:43  29 result mean: 0.105594 std: 0.013928, iter: 2210.50\n",
      "11:32:03  30 result mean: 0.106975 std: 0.014379, iter: 3410.60\n",
      "11:32:19  31 result mean: 0.105354 std: 0.013076, iter: 2429.30\n",
      "11:32:33  32 result mean: 0.106439 std: 0.012476, iter: 2079.10\n",
      "11:32:47  33 result mean: 0.106439 std: 0.012476, iter: 2079.10\n",
      "11:33:00  34 result mean: 0.106849 std: 0.013662, iter: 1801.80\n",
      "11:33:15  35 result mean: 0.105110 std: 0.013784, iter: 2338.60\n",
      "11:33:30  36 result mean: 0.104301 std: 0.012940, iter: 2275.10\n",
      "11:33:43  37 result mean: 0.106370 std: 0.013291, iter: 1785.30\n",
      "11:33:57  38 result mean: 0.105981 std: 0.013224, iter: 1825.40\n",
      "11:34:12  39 result mean: 0.106119 std: 0.013355, iter: 1914.80\n",
      "11:34:26  40 result mean: 0.106697 std: 0.012660, iter: 2211.20\n",
      "11:34:39  41 result mean: 0.105945 std: 0.012834, iter: 1973.70\n",
      "11:34:53  42 result mean: 0.106515 std: 0.012862, iter: 1796.60\n",
      "11:35:06  43 result mean: 0.106873 std: 0.012448, iter: 1741.80\n",
      "11:35:20  44 result mean: 0.107227 std: 0.012334, iter: 1662.80\n",
      "11:35:34  45 result mean: 0.106122 std: 0.013604, iter: 2103.30\n",
      "11:35:49  46 result mean: 0.106263 std: 0.013806, iter: 2073.80\n",
      "11:36:04  47 result mean: 0.106232 std: 0.012688, iter: 1899.30\n",
      "11:36:18  48 result mean: 0.107545 std: 0.012324, iter: 1712.50\n",
      "11:36:34  49 result mean: 0.106832 std: 0.011386, iter: 1921.20\n",
      "11:36:53  50 result mean: 0.106952 std: 0.012852, iter: 3202.60\n",
      "11:37:12  51 result mean: 0.106952 std: 0.012852, iter: 3202.60\n",
      "11:37:29  52 result mean: 0.105867 std: 0.014212, iter: 2611.90\n",
      "11:37:46  53 result mean: 0.105867 std: 0.014212, iter: 2611.90\n",
      "11:38:00  54 result mean: 0.106148 std: 0.013447, iter: 2050.80\n",
      "11:38:20  55 result mean: 0.107531 std: 0.014236, iter: 3306.90\n",
      "11:38:36  56 result mean: 0.105351 std: 0.013455, iter: 2378.70\n",
      "11:38:51  57 result mean: 0.105837 std: 0.012671, iter: 2222.70\n",
      "11:39:07  58 result mean: 0.105837 std: 0.012671, iter: 2222.70\n",
      "11:39:21  59 result mean: 0.106256 std: 0.013937, iter: 2009.00\n",
      "11:39:37  60 result mean: 0.104974 std: 0.013032, iter: 2318.30\n",
      "11:39:52  61 result mean: 0.104742 std: 0.012613, iter: 2223.60\n",
      "11:40:07  62 result mean: 0.105486 std: 0.013373, iter: 2055.10\n",
      "11:40:24  63 result mean: 0.105094 std: 0.013028, iter: 2170.80\n",
      "11:40:39  64 result mean: 0.106038 std: 0.012369, iter: 1972.20\n",
      "11:40:56  65 result mean: 0.106381 std: 0.013045, iter: 2551.90\n",
      "11:41:12  66 result mean: 0.105859 std: 0.012944, iter: 2240.60\n",
      "11:41:27  67 result mean: 0.106785 std: 0.012470, iter: 1947.50\n",
      "11:41:41  68 result mean: 0.106878 std: 0.012888, iter: 1844.70\n",
      "11:41:57  69 result mean: 0.106887 std: 0.011984, iter: 1852.20\n",
      "11:42:12  70 result mean: 0.106080 std: 0.013035, iter: 2144.60\n",
      "11:42:27  71 result mean: 0.106951 std: 0.013025, iter: 2019.60\n",
      "11:42:42  72 result mean: 0.106388 std: 0.013586, iter: 1872.70\n",
      "11:42:56  73 result mean: 0.106676 std: 0.012310, iter: 1777.30\n",
      "11:43:14  74 result mean: 0.106828 std: 0.011780, iter: 1937.20\n",
      "11:43:33  75 result mean: 0.106918 std: 0.013219, iter: 3219.30\n",
      "11:43:52  76 result mean: 0.106918 std: 0.013219, iter: 3219.30\n",
      "11:44:08  77 result mean: 0.105568 std: 0.013323, iter: 2376.70\n",
      "11:44:24  78 result mean: 0.105568 std: 0.013323, iter: 2376.70\n",
      "11:44:38  79 result mean: 0.106221 std: 0.013086, iter: 2059.40\n",
      "11:44:59  80 result mean: 0.107052 std: 0.014081, iter: 3397.70\n",
      "11:45:15  81 result mean: 0.105331 std: 0.013607, iter: 2450.50\n",
      "11:45:31  82 result mean: 0.106272 std: 0.012577, iter: 2183.80\n",
      "11:45:46  83 result mean: 0.106272 std: 0.012577, iter: 2183.80\n",
      "11:46:01  84 result mean: 0.106459 std: 0.013860, iter: 2029.70\n",
      "11:46:18  85 result mean: 0.105069 std: 0.013434, iter: 2488.20\n",
      "11:46:33  86 result mean: 0.104706 std: 0.012578, iter: 2203.30\n",
      "11:46:50  87 result mean: 0.105257 std: 0.012437, iter: 2217.50\n",
      "11:47:05  88 result mean: 0.105613 std: 0.013006, iter: 1930.00\n",
      "11:47:20  89 result mean: 0.106517 std: 0.012309, iter: 1875.50\n",
      "11:47:37  90 result mean: 0.106010 std: 0.012895, iter: 2508.50\n",
      "11:47:53  91 result mean: 0.105858 std: 0.013521, iter: 2248.20\n",
      "11:48:09  92 result mean: 0.106806 std: 0.012191, iter: 1976.60\n",
      "11:48:24  93 result mean: 0.106745 std: 0.013238, iter: 1904.40\n",
      "11:48:40  94 result mean: 0.107008 std: 0.012878, iter: 1908.20\n",
      "11:48:56  95 result mean: 0.106395 std: 0.013785, iter: 2166.00\n",
      "11:49:12  96 result mean: 0.106789 std: 0.013293, iter: 2164.00\n",
      "11:49:27  97 result mean: 0.107353 std: 0.013408, iter: 1776.20\n",
      "11:49:42  98 result mean: 0.107609 std: 0.013368, iter: 1730.50\n",
      "11:50:00  99 result mean: 0.107275 std: 0.012379, iter: 2039.80\n",
      "11:50:22 100 result mean: 0.106010 std: 0.013158, iter: 3596.40\n",
      "11:50:44 101 result mean: 0.106010 std: 0.013158, iter: 3596.40\n",
      "11:51:00 102 result mean: 0.105499 std: 0.013110, iter: 2457.30\n",
      "11:51:17 103 result mean: 0.105499 std: 0.013110, iter: 2457.30\n",
      "11:51:31 104 result mean: 0.105811 std: 0.013348, iter: 2069.20\n",
      "11:51:52 105 result mean: 0.106878 std: 0.013987, iter: 3402.00\n",
      "11:52:08 106 result mean: 0.105284 std: 0.012912, iter: 2361.90\n",
      "11:52:25 107 result mean: 0.105681 std: 0.012370, iter: 2336.30\n",
      "11:52:42 108 result mean: 0.105681 std: 0.012370, iter: 2336.30\n",
      "11:52:58 109 result mean: 0.105827 std: 0.013044, iter: 2146.90\n",
      "11:53:16 110 result mean: 0.104553 std: 0.013535, iter: 2666.70\n",
      "11:53:31 111 result mean: 0.105634 std: 0.013152, iter: 2074.30\n",
      "11:53:47 112 result mean: 0.105506 std: 0.012662, iter: 2157.10\n",
      "11:54:03 113 result mean: 0.105129 std: 0.012805, iter: 2035.80\n",
      "11:54:21 114 result mean: 0.105950 std: 0.012625, iter: 2097.30\n",
      "11:54:38 115 result mean: 0.106047 std: 0.013335, iter: 2538.50\n",
      "11:54:54 116 result mean: 0.106105 std: 0.013346, iter: 2214.90\n",
      "11:55:10 117 result mean: 0.105667 std: 0.013082, iter: 2085.20\n",
      "11:55:28 118 result mean: 0.105822 std: 0.013601, iter: 2067.40\n",
      "11:55:44 119 result mean: 0.106299 std: 0.013137, iter: 1921.10\n",
      "11:56:00 120 result mean: 0.105695 std: 0.013038, iter: 2228.60\n",
      "11:56:18 121 result mean: 0.106478 std: 0.012518, iter: 2262.90\n",
      "11:56:34 122 result mean: 0.107800 std: 0.012941, iter: 1904.90\n",
      "11:56:52 123 result mean: 0.107212 std: 0.012829, iter: 2102.40\n",
      "11:57:11 124 result mean: 0.107366 std: 0.011577, iter: 1975.70\n",
      "11:57:32 125 result mean: 0.106136 std: 0.013461, iter: 3517.20\n",
      "11:57:53 126 result mean: 0.106136 std: 0.013461, iter: 3517.20\n",
      "11:58:10 127 result mean: 0.105778 std: 0.013389, iter: 2405.50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11:58:26 128 result mean: 0.105778 std: 0.013389, iter: 2405.50\n",
      "11:58:42 129 result mean: 0.105735 std: 0.012839, iter: 2284.30\n",
      "11:59:04 130 result mean: 0.106816 std: 0.014234, iter: 3550.30\n",
      "11:59:22 131 result mean: 0.104776 std: 0.013353, iter: 2696.40\n",
      "11:59:37 132 result mean: 0.106699 std: 0.013401, iter: 2004.50\n",
      "11:59:51 133 result mean: 0.106699 std: 0.013401, iter: 2004.50\n",
      "12:00:08 134 result mean: 0.106125 std: 0.013070, iter: 2192.50\n",
      "12:00:26 135 result mean: 0.104839 std: 0.014065, iter: 2581.50\n",
      "12:00:43 136 result mean: 0.105091 std: 0.012543, iter: 2430.80\n",
      "12:01:00 137 result mean: 0.105268 std: 0.013035, iter: 2261.30\n",
      "12:01:17 138 result mean: 0.105437 std: 0.012336, iter: 2021.20\n",
      "12:01:34 139 result mean: 0.106073 std: 0.012405, iter: 2001.10\n",
      "12:01:51 140 result mean: 0.106061 std: 0.013342, iter: 2562.40\n",
      "12:02:07 141 result mean: 0.106388 std: 0.013287, iter: 2161.30\n",
      "12:02:22 142 result mean: 0.106554 std: 0.012658, iter: 1939.70\n",
      "12:02:40 143 result mean: 0.106346 std: 0.013230, iter: 2026.90\n",
      "12:02:58 144 result mean: 0.106439 std: 0.012867, iter: 2023.70\n",
      "12:03:13 145 result mean: 0.106099 std: 0.013005, iter: 2104.10\n",
      "12:03:29 146 result mean: 0.106401 std: 0.012792, iter: 2070.70\n",
      "12:03:45 147 result mean: 0.106870 std: 0.012125, iter: 1933.40\n",
      "12:04:03 148 result mean: 0.107226 std: 0.012193, iter: 2048.40\n",
      "12:04:21 149 result mean: 0.107876 std: 0.011682, iter: 1862.30\n",
      "12:04:43 150 result mean: 0.106334 std: 0.013754, iter: 3587.20\n",
      "12:05:05 151 result mean: 0.106334 std: 0.013754, iter: 3587.20\n",
      "12:05:23 152 result mean: 0.105429 std: 0.012882, iter: 2645.50\n",
      "12:05:41 153 result mean: 0.105429 std: 0.012882, iter: 2645.50\n",
      "12:05:57 154 result mean: 0.106164 std: 0.013489, iter: 2282.50\n",
      "12:06:21 155 result mean: 0.106621 std: 0.013939, iter: 3841.20\n",
      "12:06:38 156 result mean: 0.104883 std: 0.013774, iter: 2589.40\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "##################################################\n",
    "# round 2: tune subsample and colsample_bytree\n",
    "##################################################\n",
    "# subsamples = np.linspace(0.01, 1.0, 10)\n",
    "# colsample_bytrees = np.linspace(0.1, 1.0, 10)\n",
    "# colsample_bylevel = np.linspace(0.1, 1.0, 10)\n",
    "# narrower search\n",
    "subsamples = np.linspace(0.25, 0.75, 11)\n",
    "colsample_bytrees = np.linspace(0.1, 0.3, 5)\n",
    "colsample_bylevel = np.linspace(0.1, 0.3, 5)\n",
    "# subsamples = np.linspace(0.4, 0.9, 11)\n",
    "# colsample_bytrees = np.linspace(0.05, 0.25, 5)\n",
    "\n",
    "grid_search_dicts = [dict(zip(['subsample', 'colsample_bytree', 'colsample_bylevel'], [a, b, c])) \n",
    "                     for a,b,c in product(subsamples, colsample_bytrees, colsample_bylevel)]\n",
    "# merge into full param dicts\n",
    "full_search_dicts = [{**current_params, **d} for d in grid_search_dicts]\n",
    "# cv and get best params\n",
    "current_params, results_df = cv_over_param_dict(df, full_search_dicts, predictors, response, kfolds, verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### # round 3: tune alpha, lambda\n",
    "##################################################\n",
    "#lauraepp says don't touch these unless you know what you're doing ¯\\_(ツ)_/¯\n",
    "#\n",
    "# reg_alphas = np.logspace(-3, -2, 3)\n",
    "# reg_lambdas = np.logspace(-2, 1, 4)\n",
    "# grid_search_dicts = [dict(zip(['reg_alpha', 'reg_lambda'], [a, b])) \n",
    "#                      for a,b in product(reg_alphas, reg_lambdas)]\n",
    "# # merge into full param dicts\n",
    "# full_search_dicts = [{**current_params, **d} for d in grid_search_dicts]\n",
    "\n",
    "# # cv and get best params\n",
    "# current_params, results_df = cv_over_param_dict(df, full_search_dicts, predictors, response, kfolds, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# round 4: learning rate\n",
    "learning_rates = np.logspace(-3, -1, 5)\n",
    "grid_search_dicts = [{'learning_rate': lr} for lr in learning_rates]\n",
    "# merge into full param dicts\n",
    "full_search_dicts = [{**current_params, **d} for d in grid_search_dicts]\n",
    "\n",
    "# cv and get best params\n",
    "current_params, results_df = cv_over_param_dict(df, full_search_dicts, predictors, response, kfolds, verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "xgb = XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    n_estimators=3438,\n",
    "    random_state=RANDOMSTATE,    \n",
    "    verbosity=1,\n",
    "    n_jobs=-1,\n",
    "    booster='gbtree',   \n",
    "    **current_params\n",
    ")    \n",
    "\n",
    "print(xgb)\n",
    "\n",
    "scores = -cross_val_score(xgb, df[predictors], df[response],\n",
    "                          scoring=\"neg_root_mean_squared_error\",\n",
    "                          cv=kfolds,\n",
    "                          n_jobs=-1)\n",
    "\n",
    "raw_scores = [cv_to_raw(x) for x in scores]\n",
    "print()\n",
    "print(\"Log1p CV RMSE %.06f (STD %.04f)\" % (np.mean(scores), np.std(scores)))\n",
    "print(\"Raw CV RMSE %.0f (STD %.0f)\" % (np.mean(raw_scores), np.std(raw_scores)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HyperOpt\n",
    "\n",
    "HyperOpt is a Bayesian optimization algorithm by [James Bergstra et al.](https://conference.scipy.org/proceedings/scipy2013/pdfs/bergstra_hyperopt.pdf)\n",
    "  - [Home page](http://hyperopt.github.io/hyperopt/)\n",
    "  - [GitHub](https://github.com/hyperopt/hyperopt)\n",
    "  - [HyperOpt: Bayesian Hyperparameter Optimization](https://blog.dominodatalab.com/hyperopt-bayesian-hyperparameter-optimization/), Subir Mansukhani (2019)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_tune_kwargs = {\n",
    "    \"n_estimators\": tune.loguniform(100, 10000),\n",
    "    \"max_depth\": tune.randint(0, 5),\n",
    "    'max_leaves': tune.loguniform(1, 1000),    \n",
    "    \"subsample\": tune.quniform(0.25, 0.75, 0.01),\n",
    "    \"colsample_bytree\": tune.quniform(0.05, 0.5, 0.01),\n",
    "    \"colsample_bylevel\": tune.quniform(0.05, 0.5, 0.01),    \n",
    "    \"learning_rate\": tune.loguniform(0.001, 0.1),\n",
    "#     \"wandb\": {\n",
    "#         \"project\": \"iowa2\",\n",
    "#        \"api_key_file\": \"secrets/wandb.txt\",\n",
    "#    }    \n",
    "}\n",
    "\n",
    "xgb_tune_params = [k for k in xgb_tune_kwargs.keys() if k != 'wandb']\n",
    "xgb_tune_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refactor to give ray.tune a single function of hyperparameters to optimize\n",
    "\n",
    "# @wandb_mixin\n",
    "def my_xgb(config):\n",
    "    \n",
    "    # fix these configs to match calling convention\n",
    "    config['max_depth'] = int(config['max_depth']) + 2   # hyperopt needs left to start at 0 but we want to start at 2\n",
    "    config['max_leaves'] = int(config['max_leaves'])\n",
    "    config['n_estimators'] = int(config['n_estimators'])   # pass float eg loguniform distribution, use int\n",
    "    \n",
    "    xgb = XGBRegressor(\n",
    "        objective='reg:squarederror',\n",
    "        n_jobs=1,\n",
    "        random_state=RANDOMSTATE,\n",
    "        booster='gbtree',   \n",
    "        scale_pos_weight=1, \n",
    "        **config,\n",
    "    )\n",
    "    scores = -cross_val_score(xgb, df[predictors], df[response],\n",
    "                                      scoring=\"neg_root_mean_squared_error\",\n",
    "                                      cv=kfolds)\n",
    "    rmse = np.mean(scores)\n",
    "    tune.report(rmse=rmse)\n",
    "    # wandb.log({\"rmse\": rmse})\n",
    "    \n",
    "    return {\"rmse\": rmse}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLES=128\n",
    "\n",
    "start_time = datetime.now()\n",
    "print(\"%-20s %s\" % (\"Start Time\", start_time))\n",
    "\n",
    "algo = HyperOptSearch(random_state_seed=RANDOMSTATE)\n",
    "# to limit number of cores, uncomment and set max_concurrent \n",
    "# algo = ConcurrencyLimiter(algo, max_concurrent=10)\n",
    "scheduler = AsyncHyperBandScheduler()\n",
    "\n",
    "analysis = tune.run(my_xgb,\n",
    "                    num_samples=NUM_SAMPLES,\n",
    "                    config=xgb_tune_kwargs,                    \n",
    "                    name=\"hyperopt_xgb\",\n",
    "                    metric=\"rmse\",\n",
    "                    mode=\"min\",\n",
    "                    search_alg=algo,\n",
    "                    scheduler=scheduler,\n",
    "                    verbose=1,\n",
    "#                    loggers=DEFAULT_LOGGERS + (WandbLogger, ),\n",
    "                   )\n",
    "\n",
    "end_time = datetime.now()\n",
    "print(\"%-20s %s\" % (\"Start Time\", start_time))\n",
    "print(\"%-20s %s\" % (\"End Time\", end_time))\n",
    "print(str(timedelta(seconds=(end_time-start_time).seconds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_cols = ['config.' + k for k in xgb_tune_params]\n",
    "analysis_results_df = analysis.results_df[['rmse', 'date', 'time_this_iter_s'] + param_cols].sort_values('rmse')\n",
    "analysis_results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_config = {z: analysis_results_df.iloc[0]['config.' + z] for z in xgb_tune_params}\n",
    "\n",
    "xgb = XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    random_state=RANDOMSTATE,    \n",
    "    verbosity=1,\n",
    "    n_jobs=-1,\n",
    "    **best_config\n",
    ")\n",
    "print(xgb)\n",
    "\n",
    "scores = -cross_val_score(xgb, df[predictors], df[response],\n",
    "                          scoring=\"neg_root_mean_squared_error\",\n",
    "                          cv=kfolds)\n",
    "\n",
    "raw_scores = [cv_to_raw(x) for x in scores]\n",
    "print()\n",
    "print(\"Log1p CV RMSE %.06f (STD %.04f)\" % (np.mean(scores), np.std(scores)))\n",
    "print(\"Raw CV RMSE %.0f (STD %.0f)\" % (np.mean(raw_scores), np.std(raw_scores)))\n",
    "#18338 0:07:25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4096 iterations\n",
    "config_4096 = {'n_estimators': 2534,\n",
    " 'max_depth': 3,\n",
    " 'max_leaves': 22,\n",
    " 'subsample': 0.42,\n",
    " 'colsample_bytree': 0.14,\n",
    " 'colsample_bylevel': 0.21,\n",
    " 'learning_rate': 0.025422975168222197}\n",
    "\n",
    "xgb = XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    random_state=RANDOMSTATE,    \n",
    "    verbosity=1,\n",
    "    n_jobs=-1,\n",
    "    **config_4096\n",
    ")\n",
    "print(xgb)\n",
    "\n",
    "scores = -cross_val_score(xgb, df[predictors], df[response],\n",
    "                          scoring=\"neg_root_mean_squared_error\",\n",
    "                          cv=kfolds)\n",
    "\n",
    "raw_scores = [cv_to_raw(x) for x in scores]\n",
    "print()\n",
    "print(\"Log1p CV RMSE %.06f (STD %.04f)\" % (np.mean(scores), np.std(scores)))\n",
    "print(\"Raw CV RMSE %.0f (STD %.0f)\" % (np.mean(raw_scores), np.std(raw_scores)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optuna\n",
    "\n",
    "Optuna is a Bayesian optimization algorithm by [Takuya Akiba, et al.](https://arxiv.org/abs/1907.10902)\n",
    "\n",
    " - [Home](https://optuna.org/)\n",
    " - [Using Optuna to Optimize XGBoost Hyperparameters](https://medium.com/optuna/using-optuna-to-optimize-xgboost-hyperparameters-63bfcdfd3407), Crissman Loomis, 2020\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optuna\n",
    "NUM_SAMPLES=256\n",
    "\n",
    "start_time = datetime.now()\n",
    "print(\"%-20s %s\" % (\"Start Time\", start_time))\n",
    "\n",
    "algo = OptunaSearch()\n",
    "# uncomment and set max_concurrent to limit number of cores\n",
    "# algo = ConcurrencyLimiter(algo, max_concurrent=10)\n",
    "scheduler = AsyncHyperBandScheduler()\n",
    "\n",
    "# identical tune args\n",
    "analysis = tune.run(my_xgb,\n",
    "                    num_samples=NUM_SAMPLES,\n",
    "                    config=xgb_tune_kwargs,                    \n",
    "                    name=\"optuna_xgb\",\n",
    "                    metric=\"rmse\",\n",
    "                    mode=\"min\",\n",
    "                    search_alg=algo,\n",
    "                    scheduler=scheduler,\n",
    "                    verbose=1,\n",
    "#                     loggers=DEFAULT_LOGGERS + (WandbLogger, ),\n",
    "                   )\n",
    "\n",
    "end_time = datetime.now()\n",
    "print(\"%-20s %s\" % (\"Start Time\", start_time))\n",
    "print(\"%-20s %s\" % (\"End Time\", end_time))\n",
    "print(str(timedelta(seconds=(end_time-start_time).seconds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_cols = ['config.' + k for k in xgb_tune_params]\n",
    "analysis_results_df = analysis.results_df[['rmse', 'date', 'time_this_iter_s'] + param_cols].sort_values('rmse')\n",
    "analysis_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_config = {z: analysis_results_df.iloc[0]['config.' + z] for z in xgb_tune_params}\n",
    "\n",
    "xgb = XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    random_state=RANDOMSTATE,    \n",
    "    verbosity=1,\n",
    "    n_jobs=-1,\n",
    "    **best_config\n",
    ")\n",
    "print(xgb)\n",
    "\n",
    "scores = -cross_val_score(xgb, df[predictors], df[response],\n",
    "                          scoring=\"neg_root_mean_squared_error\",\n",
    "                          cv=kfolds)\n",
    "\n",
    "raw_scores = [cv_to_raw(x) for x in scores]\n",
    "print()\n",
    "print(\"Log1p CV RMSE %.06f (STD %.04f)\" % (np.mean(scores), np.std(scores)))\n",
    "print(\"Raw CV RMSE %.0f (STD %.0f)\" % (np.mean(raw_scores), np.std(raw_scores)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM with HyperOpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_tune_kwargs = {\n",
    "    \"n_estimators\": tune.loguniform(100, 10000),\n",
    "    \"max_depth\": tune.randint(0, 5),\n",
    "    'num_leaves': tune.loguniform(2, 1000),               # max_leaves\n",
    "    \"bagging_fraction\": tune.quniform(0.5, 0.8, 0.01),    # subsample\n",
    "    \"feature_fraction\": tune.quniform(0.05, 0.5, 0.01),   # colsample_bytree\n",
    "    \"learning_rate\": tune.loguniform(0.001, 0.1),\n",
    "#     \"wandb\": {\n",
    "#         \"project\": \"iowa\",\n",
    "#     }        \n",
    "}\n",
    "\n",
    "#print(\"wandb name:\", lgbm_tune_kwargs['wandb']['name'])\n",
    "lgbm_tune_params = [k for k in lgbm_tune_kwargs.keys() if k != 'wandb']\n",
    "print(lgbm_tune_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @wandb_mixin\n",
    "def my_lgbm(config):\n",
    "    \n",
    "    # fix these configs \n",
    "    config['n_estimators'] = int(config['n_estimators'])   # pass float eg loguniform distribution, use int\n",
    "    config['num_leaves'] = int(config['num_leaves'])\n",
    "    \n",
    "    lgbm = LGBMRegressor(objective='regression',\n",
    "                         max_bin=200,\n",
    "                         feature_fraction_seed=7,\n",
    "                         min_data_in_leaf=2,\n",
    "                         verbose=-1,\n",
    "                         n_jobs=1,\n",
    "                         # these are specified to suppress warnings\n",
    "                         colsample_bytree=None,\n",
    "                         min_child_samples=None,\n",
    "                         subsample=None,\n",
    "                         **config,\n",
    "                         # early stopping params, maybe in fit\n",
    "                         #early_stopping_rounds=early_stopping_rounds,\n",
    "                         #valid_sets=[xgtrain, xgvalid], valid_names=['train','valid'], evals_result=evals_results\n",
    "                         #num_boost_round=num_boost_round,\n",
    "                         )\n",
    "    \n",
    "    scores = -cross_val_score(lgbm, df[predictors], df[response],\n",
    "                              scoring=\"neg_root_mean_squared_error\",\n",
    "                              cv=kfolds)\n",
    "    rmse=np.mean(scores)  \n",
    "    tune.report(rmse=rmse)\n",
    "    # wandb.log({\"rmse\": rmse})\n",
    "    \n",
    "    return {'rmse': np.mean(scores)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tune LightGBM\n",
    "print(\"LightGBM\")\n",
    "#!conda install -y -c conda-forge lightgbm\n",
    "\n",
    "NUM_SAMPLES=256\n",
    "\n",
    "start_time = datetime.now()\n",
    "print(\"%-20s %s\" % (\"Start Time\", start_time))\n",
    "\n",
    "algo = HyperOptSearch(random_state_seed=RANDOMSTATE)\n",
    "# uncomment and set max_concurrent to limit number of cores\n",
    "# algo = ConcurrencyLimiter(algo, max_concurrent=10)\n",
    "scheduler = AsyncHyperBandScheduler()\n",
    "\n",
    "# lgbm_tune_kwargs['wandb']['name'] = 'hyperopt_' + xgb_tune_kwargs['wandb']['name']\n",
    "\n",
    "analysis = tune.run(my_lgbm,\n",
    "                    num_samples=NUM_SAMPLES,\n",
    "                    config = lgbm_tune_kwargs,\n",
    "                    name=\"hyperopt_lgbm\",\n",
    "                    metric=\"rmse\",\n",
    "                    mode=\"min\",\n",
    "                    search_alg=algo,\n",
    "                    scheduler=scheduler,\n",
    "                    verbose=1,\n",
    "#                     loggers=DEFAULT_LOGGERS + (WandbLogger, ),\n",
    "                   )\n",
    "\n",
    "end_time = datetime.now()\n",
    "print(\"%-20s %s\" % (\"Start Time\", start_time))\n",
    "print(\"%-20s %s\" % (\"End Time\", end_time))\n",
    "print(str(timedelta(seconds=(end_time-start_time).seconds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_cols = ['config.' + k for k in lgbm_tune_params]\n",
    "analysis_results_df = analysis.results_df[['rmse', 'date', 'time_this_iter_s'] + param_cols].sort_values('rmse')\n",
    "analysis_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_config = {z: analysis_results_df.iloc[0]['config.' + z] for z in lgbm_tune_params}\n",
    "\n",
    "lgbm = LGBMRegressor(objective='regression',\n",
    "                     max_bin=200,\n",
    "                     feature_fraction_seed=7,\n",
    "                     min_data_in_leaf=2,\n",
    "                     verbose=-1,\n",
    "                     **best_config,\n",
    "                     # early stopping params, maybe in fit\n",
    "                     #early_stopping_rounds=early_stopping_rounds,\n",
    "                     #valid_sets=[xgtrain, xgvalid], valid_names=['train','valid'], evals_result=evals_results\n",
    "                     #num_boost_round=num_boost_round,\n",
    "                     )\n",
    " \n",
    "print(lgbm)\n",
    "\n",
    "scores = -cross_val_score(lgbm, df[predictors], df[response],\n",
    "                          scoring=\"neg_root_mean_squared_error\",\n",
    "                          cv=kfolds)\n",
    "\n",
    "raw_scores = [cv_to_raw(x) for x in scores]\n",
    "print()\n",
    "print(\"Log1p CV RMSE %.06f (STD %.04f)\" % (np.mean(scores), np.std(scores)))\n",
    "print(\"Raw CV RMSE %.0f (STD %.0f)\" % (np.mean(raw_scores), np.std(raw_scores)))\n",
    "raw_scores = [cv_to_raw(x) for x in scores]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tune LightGBM\n",
    "print(\"LightGBM\")\n",
    "#!conda install -y -c conda-forge lightgbm\n",
    "\n",
    "NUM_SAMPLES=256\n",
    "\n",
    "start_time = datetime.now()\n",
    "print(\"%-20s %s\" % (\"Start Time\", start_time))\n",
    "\n",
    "algo = OptunaSearch()\n",
    "# uncomment and set max_concurrent to limit number of cores\n",
    "# algo = ConcurrencyLimiter(algo, max_concurrent=10)\n",
    "scheduler = AsyncHyperBandScheduler()\n",
    "\n",
    "# lgbm_tune_kwargs['wandb']['name'] = 'optuna_' + xgb_tune_kwargs['wandb']['name']\n",
    "\n",
    "analysis = tune.run(my_lgbm,\n",
    "                    num_samples=NUM_SAMPLES,\n",
    "                    config = lgbm_tune_kwargs,\n",
    "                    name=\"optuna_lgbm\",\n",
    "                    metric=\"rmse\",\n",
    "                    mode=\"min\",\n",
    "                    search_alg=algo,\n",
    "                    scheduler=scheduler,\n",
    "                    verbose=1,\n",
    "#                     loggers=DEFAULT_LOGGERS + (WandbLogger, ),\n",
    "                   )\n",
    "\n",
    "end_time = datetime.now()\n",
    "print(\"%-20s %s\" % (\"Start Time\", start_time))\n",
    "print(\"%-20s %s\" % (\"End Time\", end_time))\n",
    "print(str(timedelta(seconds=(end_time-start_time).seconds)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_cols = ['config.' + k for k in lgbm_tune_params]\n",
    "analysis_results_df = analysis.results_df[['rmse', 'date', 'time_this_iter_s'] + param_cols].sort_values('rmse')\n",
    "analysis_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_config = {z: analysis_results_df.iloc[0]['config.' + z] for z in lgbm_tune_params}\n",
    "\n",
    "lgbm = LGBMRegressor(objective='regression',\n",
    "                     max_bin=200,\n",
    "                     feature_fraction_seed=7,\n",
    "                     min_data_in_leaf=2,\n",
    "                     verbosfe=-1,\n",
    "                     **best_config,\n",
    "                     # early stopping params, maybe in fit\n",
    "                     #early_stopping_rounds=early_stopping_rounds,\n",
    "                     #valid_sets=[xgtrain, xgvalid], valid_names=['train','valid'], evals_result=evals_results\n",
    "                     #num_boost_round=num_boost_round,\n",
    "                     )\n",
    " \n",
    "print(lgbm)\n",
    "\n",
    "scores = -cross_val_score(lgbm, df[predictors], df[response],\n",
    "                          scoring=\"neg_root_mean_squared_error\",\n",
    "                          cv=kfolds)\n",
    "\n",
    "raw_scores = [cv_to_raw(x) for x in scores]\n",
    "print()\n",
    "print(\"Log1p CV RMSE %.06f (STD %.04f)\" % (np.mean(scores), np.std(scores)))\n",
    "print(\"Raw CV RMSE %.0f (STD %.0f)\" % (np.mean(raw_scores), np.std(raw_scores)))\n",
    "raw_scores = [cv_to_raw(x) for x in scores]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ray Cluster on AWS\n",
    "(Google Cloud Platform and Azure should be supported as well, see [Ray docs and examples](https://github.com/ray-project/ray/tree/master/python/ray/autoscaler))\n",
    "\n",
    "- Clusters are defined in `ray1.1.yaml`\n",
    "- boto3 and AWS CLI configured credentials are used, so [install and configure AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html)\n",
    "- Edit `ray1.1.yaml` file with your region, availability zone, subnet, imageid information\n",
    "    - to get those variables launch the latest Deep Learning AMI (Ubuntu 18.04) Version 35.0 into a small instance in your favorite region/zone\n",
    "    - test that it works\n",
    "    - note those 4 variables: region, availability zone, subnet, AMI imageid\n",
    "    - terminate the instance and edit `ray1.1.yaml` accordingly\n",
    "    - in future you can create your own image with everything pre-installed and specify its AMI imageid, instead of using the generic image and installing everything at launch.\n",
    "- To run the cluster: \n",
    "`ray up ray1.1.yaml`\n",
    "    - Creates head instance using image specified.\n",
    "    - Installs ray and related requirements\n",
    "    - Clones this Iowa repo\n",
    "    - Launches worker nodes per auto-scaling parameters (currently we fix the number of nodes because we're not benching the time the cluster will take to auto-scale)\n",
    "- After cluster starts you can check AWS console and note that several instances launched.\n",
    "- Check `ray monitor ray1.1.yaml` for any error messages\n",
    "- Run Jupyter on the cluster with port forwarding\n",
    " `ray exec ray1.1.yaml --port-forward=8899 'source ~/anaconda3/bin/activate tensorflow_p36 && jupyter notebook --port=8899'`\n",
    "- Open the notebook on the generated URL e.g. http://localhost:8899/?token=5f46d4355ae7174524ba71f30ef3f0633a20b19a204b93b4\n",
    "- Make sure to hoose the default kernel to make sure it runs in the conda environment with all installs\n",
    "- Make sure to use the ray.init() command given in the startup messages.\n",
    "- You can also run a terminal on the head node of the cluster with\n",
    " `ray attach /Users/drucev/projects/iowa/ray1.1.yaml`\n",
    "- You can also ssh explicitly with the IP address and the generated private key\n",
    " `ssh -o IdentitiesOnly=yes -i ~/.ssh/ray-autoscaler_1_us-east-1.pem ubuntu@54.161.200.54`\n",
    "- run port forwarding to the Ray dashboard with   \n",
    "`ray dashboard ray1.1.yaml`\n",
    "and then open\n",
    " http://localhost:8265/\n",
    "- the cluster will incur AWS charges so `ray down ray1.1.yaml` when complete\n",
    "- Other than connecting to Ray cluster, runs identically\n",
    "- see hyperparameter_optimization.ipynb, separated out so each notebook can be run end-to-end with/without cluster setup\n",
    "\n",
    "see https://docs.ray.io/en/latest/cluster/launcher.html for additional info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concluding remarks\n",
    "\n",
    "We observe a modest but non-negligeable improvement in the target metric with a less manual process vs. sequential tuning.\n",
    "hyoperopt.optuna yes\n",
    "ray maybe, depends on further work compare w/optuna and hyperopt online. depends on integration between ray, early stopping and search algo\n",
    "cluster , in general no need and costs add up. MacBook Pro w/16 threads and desktop with GPU are plenty.\n",
    "\n",
    "I intend to use HyperOpt and Optuna for xgboost going forward, no more grid search for me! In every case I've applied them, I've gotten at least a small improvement in the best metrics I found using grid search methods. Additionally, it's fire and forget (although with a little elbow grease the 4-pass sequential grid search could be made fire and forget.)\n",
    "\n",
    "These two algorithms seem to be the most popular but I may try the other algos systematically. \n",
    "\n",
    "I am surprised that Elasticnet, i.e. regularized linear regression outperforms boosting. \n",
    "This dataset has been heavily engineered so that linear methods work well. Predictors were chosen using lasso/elasticnet and we used log and Box-Cox transforms to force predictors to follow assumptions of least-squares.  \n",
    "\n",
    "This tends to validate one of the [critiques of machine learning](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3624052), that the most powerful ML methods don't necessarily converge all the way to the best solution. If you have a ground truth that is linear plus noise, a complex XGBoost or neural network algorithm should get arbitrarily close to the closed-form optimal solution but will never match it exactly. XGBoost is piecewise constant and the complex neural network is subject to the vagaries of stochastic gradient descent. \n",
    "\n",
    "But Elasticnet with L1 + L2 regularization plus gradient descent and hyperparameter optimization is still machine learning. It's simply the form best matched to the problem. In the real world where things don't match assumptions of least-squares, boosting performs extremely well. And even on this dataset, engineered for the linear models, SVR and KernelRidge performed better than Elasticnet (not shown) and ensembling Elasticnet with XGBoost, LightGBM, SVR, neural networks worked best of all. \n",
    "\n",
    "To paraphrase Casey Stengel, clever feature engineering will always outperform clever model algorithms and vice-versa<sup>*</sup>. But improving your hyperparameters with these best practices will always improve your results.\n",
    "\n",
    "<sup>*</sup>This is not intended to make sense.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
