{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning with XGBoost, Ray Tune, Hyperopt and Optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "\n",
    "In this post we are going to demonstrate how we can speed up hyperparameter tuning with:\n",
    "\n",
    "1) Bayesian optimization tuning algos like HyperOpt and Optuna, running on…\n",
    "\n",
    "2) the [Ray](https://ray.io/) distributed ML framework, with a [unified API to many hyperparameter search algos](https://medium.com/riselab/cutting-edge-hyperparameter-tuning-with-ray-tune-be6c0447afdf) with early stopping and…\n",
    "\n",
    "3) a distributed cluster of cloud instances for even more speedup.\n",
    "\n",
    "### Outline:\n",
    "- Overview of hyperparameter tuning\n",
    "- Baseline linear regression with no hyperparameters\n",
    "- ElasticNet with L1 and L2 regularization using ElasticNetCV hyperparameter optimization\n",
    "- ElasticNet with GridSearchCV hyperparameter optimization\n",
    "- XGBoost: sequential grid search over hyperparameter subsets with early stopping \n",
    "- XGBoost: with HyperOpt and Optuna search algorithms\n",
    "- LightGBM: with HyperOpt and Optuna search algorithms\n",
    "- XGBoost: HyperOpt on a Ray cluster\n",
    "- LightGBM: HyperOpt on a Ray cluster\n",
    "- Concluding remarks\n",
    "18735\n",
    "\n",
    "But first, here are results on the Ames housing data set, predicting Iowa home prices:\n",
    "\n",
    "| ML Algo           | Hyperparameter search algo   | CV Error (RMSE in $)  | Time     |\n",
    "|-------------------|------------------------------|-----------------------|----------|\n",
    "| XGB               | Sequential Grid Search       | 18783                |   36:09  |\n",
    "| XGB               | HyperOpt (128 samples)       | 18770                | 13:36 |\n",
    "| XGB               | Optuna (256 samples)         | 18722                | 43:21 |\n",
    "| LightGBM          | HyperOpt (256 samples)       | 18612              |   45:08  |\n",
    "| LightGBM          | Optuna                       |  18534               |  | 34:54\n",
    "| XGB               | Optuna - 16-instance cluster | 18770                |   14:23  |\n",
    "| LightGBM          | Optuna - 16-instance cluster | 18612                |    4:22  |\n",
    "| Baseline:         |                              |\n",
    "| Linear Regression | --                           | 18192                |   0:01s  |\n",
    "| ElasticNet        | ElasticNetCV (Grid Search)   | 18122                |   0:02s  |          \n",
    "| ElasticNet        | GridSearchCV                 | 18061                |   0:05s  |          \n",
    "\n",
    "We see both speedup and RMSE improvement when using HyperOpt and Optuna, and the cluster. But our feature engineering was quite good and our simple linear model still outperforms boosting. (Not shown, SVR and KR are high-performing and an ensemble improves over all individual algos)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning Overview\n",
    "\n",
    "Here are [the principal approaches to hyperparameter tuning](https://en.wikipedia.org/wiki/Hyperparameter_optimization)\n",
    "\n",
    "- Grid search: given a finite set of discrete values for each hyperparameter, exhaustively cross-validate all combinations\n",
    "\n",
    "- Random search: given a discrete or continuous distribution for each hyperparameter, randomly sample from the joint distribution. Generally [more efficient than exhaustive grid search.](https://dl.acm.org/doi/10.5555/2188385.2188395 ) \n",
    "\n",
    "- Bayesian optimization: update the search space as you go based on outcomes of prior searches.\n",
    "\n",
    "- Gradient-based optimization: attempt to estimate the gradient of the CV metric with respect to the hyperparameter and ascend/descend the gradient.\n",
    "\n",
    "- Evolutionary optimization: sample the search space, discard combinations with poor metrics, and genetically evolve new combinations to try based on the successful combinations.\n",
    "\n",
    "- Population-based: A method of performing hyperparameter optimization at the same time as training.\n",
    "\n",
    "In this post we focus on Bayesian optimization with HyperOpt and Optuna. What is Bayesian optimization? When we perform a grid search, the search space can be considered a prior belief that the best hyperparameter vector is in the search space, and the combinations have equal probability of being the best combination. So we try them all and pick the best one.\n",
    "\n",
    "Perhaps we might do two passes of grid search. After an initial search on a broad, coarsely spaced grid, we might do a deeper dive in a smaller area around the best metric from the first pass, with a more finely-spaced grid. In Bayesian terminology we updated our prior belief.\n",
    "\n",
    "Bayesian optimization first samples randomly, e.g. 30 combinations, and computes the cross-validation metric for each combination. Then the algorithm updates the distribution it samples from, so it is more likely to sample combinations near the good metrics, and less likely to sample combinations near the poor metrics. As it continues to sample, it continues to update the search distribution based on the metrics it finds.\n",
    "\n",
    "Early stopping may also highly beneficial: often we can discard a combination without fully training it. In this post we use [ASHA](https://arxiv.org/abs/1810.05934). \n",
    "\n",
    "We use 4 regression algorithms:\n",
    "- LinearRegression: baseline with no hyperparameters\n",
    "- ElasticNet: Linear regression with L1 and L2 regularization (2 hyperparameters).\n",
    "- XGBoost\n",
    "- LightGBM\n",
    "\n",
    "We use 5 approaches :\n",
    "- *Native CV*: In sklearn if an algo has hyperparameters it will often have an xxxCV version which performs automated hyperparameter tuning over a search space with specified kfolds.\n",
    "- *GridSearchCV*: Abstracts CV for any sklearn algo, running multithreaded trials over specified folds. \n",
    "- *Manual sequential grid search*: What we typically do with XGBoost, which doesn't play well with GridSearchCV and has too many hyperparameters to tune in one pass.\n",
    "- *Ray on local machine*: HyperOpt and Optuna with early stopping.\n",
    "- *Ray on cluster*: Additionally scale out to run a single hyperparameter optimization task over many instances.\n",
    "\n",
    "We use data from the Ames Housing Dataset https://www.kaggle.com/c/house-prices-advanced-regression-techniques . The original data has 79 raw features. The data we will use has 100 features with a fair amount of feature engineering from [my own attempt at modeling](https://github.com/druce/iowa), which was in the top 5% or so when I submitted it to Kaggle.\n",
    "\n",
    "### Further reading: \n",
    " - [Hyper-Parameter Optimization: A Review of Algorithms and Applications](https://arxiv.org/abs/2003.05689) Tong Yu, Hong Zhu (2020)\n",
    " - [Hyperparameter Search in Machine Learning](https://arxiv.org/abs/1502.02127v2), Marc Claesen, Bart De Moor (2015)\n",
    " - [Hyperparameter Optimization](https://link.springer.com/chapter/10.1007/978-3-030-05318-5_1), Matthias Feurer, Frank Hutter (2019) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-15 03:53:41.753350\n",
      "numpy                1.19.1\n",
      "pandas               1.1.3\n",
      "sklearn              0.23.2\n",
      "xgboost              1.2.0\n",
      "lightgbm             2.3.0\n",
      "ray                  1.0.0\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import random\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression, ElasticNet, ElasticNetCV, Ridge, RidgeCV\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, GridSearchCV, KFold\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "#!conda install -y -c conda-forge  xgboost \n",
    "import xgboost\n",
    "from xgboost import XGBRegressor\n",
    "from xgboost import plot_importance\n",
    "\n",
    "import lightgbm\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.suggest import ConcurrencyLimiter\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "from ray.tune.suggest.hyperopt import HyperOptSearch\n",
    "from ray.tune.suggest.optuna import OptunaSearch\n",
    "from ray.tune.logger import DEFAULT_LOGGERS\n",
    "from ray.tune.integration.wandb import WandbLogger\n",
    "\n",
    "# pip install hyperopt\n",
    "# pip install optuna\n",
    "\n",
    "import wandb\n",
    "os.environ['WANDB_NOTEBOOK_NAME']='hyperparameter_optimization.ipynb'\n",
    "\n",
    "print(datetime.now())\n",
    "\n",
    "print (\"%-20s %s\"% (\"numpy\", np.__version__))\n",
    "print (\"%-20s %s\"% (\"pandas\", pd.__version__))\n",
    "print (\"%-20s %s\"% (\"sklearn\", sklearn.__version__))\n",
    "print (\"%-20s %s\"% (\"xgboost\", xgboost.__version__))\n",
    "print (\"%-20s %s\"% (\"lightgbm\", lightgbm.__version__))\n",
    "print (\"%-20s %s\"% (\"ray\", ray.__version__))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed for reproducibility\n",
    "RANDOMSTATE = 42\n",
    "np.random.seed(RANDOMSTATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'KYDND9VQ'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_random_tag(length):\n",
    "    \"\"\"random tag for experiments\"\"\"\n",
    "    letters_and_digits = string.ascii_letters + string.digits\n",
    "    result_str = ''.join((random.choice(letters_and_digits) for i in range(length)))\n",
    "    return result_str.upper()\n",
    "\n",
    "get_random_tag(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>BsmtFullBath</th>\n",
       "      <th>FullBath</th>\n",
       "      <th>KitchenAbvGr</th>\n",
       "      <th>GarageYrBlt</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>1stFlrSF</th>\n",
       "      <th>GrLivArea</th>\n",
       "      <th>GarageArea</th>\n",
       "      <th>...</th>\n",
       "      <th>SaleCondition_1</th>\n",
       "      <th>SaleCondition_2</th>\n",
       "      <th>SaleCondition_5</th>\n",
       "      <th>SaleType_4</th>\n",
       "      <th>BedroomAbvGr_1</th>\n",
       "      <th>BedroomAbvGr_4</th>\n",
       "      <th>BedroomAbvGr_5</th>\n",
       "      <th>HalfBath_1</th>\n",
       "      <th>TotalBath_1.0</th>\n",
       "      <th>TotalBath_2.5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>65.0</td>\n",
       "      <td>196.0</td>\n",
       "      <td>856</td>\n",
       "      <td>1710</td>\n",
       "      <td>548.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1262</td>\n",
       "      <td>1262</td>\n",
       "      <td>460.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>68.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>920</td>\n",
       "      <td>1786</td>\n",
       "      <td>608.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>95</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>961</td>\n",
       "      <td>1717</td>\n",
       "      <td>642.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>84.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>1145</td>\n",
       "      <td>2198</td>\n",
       "      <td>836.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    YearBuilt  BsmtFullBath  FullBath  KitchenAbvGr  GarageYrBlt  LotFrontage  \\\n",
       "Id                                                                              \n",
       "1           7             1         2             1            7         65.0   \n",
       "2          34             0         2             1           34         80.0   \n",
       "3           9             1         2             1            9         68.0   \n",
       "4          95             1         1             1           12         60.0   \n",
       "5          10             1         2             1           10         84.0   \n",
       "\n",
       "    MasVnrArea  1stFlrSF  GrLivArea  GarageArea  ...  SaleCondition_1  \\\n",
       "Id                                               ...                    \n",
       "1        196.0       856       1710       548.0  ...                0   \n",
       "2          0.0      1262       1262       460.0  ...                0   \n",
       "3        162.0       920       1786       608.0  ...                0   \n",
       "4          0.0       961       1717       642.0  ...                1   \n",
       "5        350.0      1145       2198       836.0  ...                0   \n",
       "\n",
       "    SaleCondition_2  SaleCondition_5  SaleType_4  BedroomAbvGr_1  \\\n",
       "Id                                                                 \n",
       "1                 0                0           1               0   \n",
       "2                 0                0           1               0   \n",
       "3                 0                0           1               0   \n",
       "4                 0                0           1               0   \n",
       "5                 0                0           1               0   \n",
       "\n",
       "    BedroomAbvGr_4  BedroomAbvGr_5  HalfBath_1  TotalBath_1.0  TotalBath_2.5  \n",
       "Id                                                                            \n",
       "1                0               0           1              0              0  \n",
       "2                0               0           0              0              1  \n",
       "3                0               0           1              0              0  \n",
       "4                0               0           0              0              0  \n",
       "5                1               0           1              0              0  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.247699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.109016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12.317171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11.849405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12.429220</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    SalePrice\n",
       "Id           \n",
       "1   12.247699\n",
       "2   12.109016\n",
       "3   12.317171\n",
       "4   11.849405\n",
       "5   12.429220"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import train data\n",
    "df = pd.read_pickle('df_train.pickle')\n",
    "\n",
    "response = 'SalePrice'\n",
    "predictors = ['YearBuilt',\n",
    "              'BsmtFullBath',\n",
    "              'FullBath',\n",
    "              'KitchenAbvGr',\n",
    "              'GarageYrBlt',\n",
    "              'LotFrontage',\n",
    "              'MasVnrArea',\n",
    "              '1stFlrSF',\n",
    "              'GrLivArea',\n",
    "              'GarageArea',\n",
    "              'WoodDeckSF',\n",
    "              'PorchSF',\n",
    "              'AvgBltRemod',\n",
    "              'FireBathRatio',\n",
    "              'TotalSF x OverallQual x OverallCond',\n",
    "              'AvgBltRemod x Functional x TotalFinSF',\n",
    "              'Functional x OverallQual',\n",
    "              'KitchenAbvGr x KitchenQual',\n",
    "              'GarageCars x GarageYrBlt',\n",
    "              'GarageQual x GarageCond x GarageCars',\n",
    "              'HeatingQC x Heating',\n",
    "              'monthnum',\n",
    "              'log_YearBuilt',\n",
    "              'log_LotArea',\n",
    "              'log_TotalFinSF',\n",
    "              'log_GarageRatio',\n",
    "              'log_TotalSF x OverallQual x OverallCond',\n",
    "              'log_TotalSF x OverallCond',\n",
    "              'log_AvgBltRemod x TotalFinSF',\n",
    "              'sq_2ndFlrSF',\n",
    "              'sq_BsmtFinSF',\n",
    "              'sq_BsmtFinSF x BsmtQual',\n",
    "              'sq_BsmtFinSF x BsmtBath',\n",
    "              'BldgType_4',\n",
    "              'BsmtExposure_1',\n",
    "              'BsmtExposure_4',\n",
    "              'BsmtFinType1_1',\n",
    "              'BsmtFinType1_2',\n",
    "              'BsmtFinType1_4',\n",
    "              'BsmtFinType1_5',\n",
    "              'BsmtFinType1_6',\n",
    "              'CentralAir_0',\n",
    "              'CentralAir_1',\n",
    "              'Condition1_1',\n",
    "              'Condition1_3',\n",
    "              'ExterCond_2',\n",
    "              'ExterQual_2',\n",
    "              'Exterior1st_4',\n",
    "              'Exterior1st_5',\n",
    "              'Exterior1st_10',\n",
    "              'Fence_0',\n",
    "              'Fence_2',\n",
    "              'Foundation_1',\n",
    "              'Foundation_5',\n",
    "              'GarageCars_1',\n",
    "              'GarageFinish_2',\n",
    "              'GarageFinish_3',\n",
    "              'GarageType_2',\n",
    "              'HouseStyle_2',\n",
    "              'KitchenQual_4',\n",
    "              'LotConfig_0',\n",
    "              'LotConfig_4',\n",
    "              'MSSubClass_30',\n",
    "              'MSSubClass_70',\n",
    "              'MSZoning_0',\n",
    "              'MSZoning_1',\n",
    "              'MSZoning_4',\n",
    "              'MasVnrType_2',\n",
    "              'MasVnrType_3',\n",
    "              'MoSold_1',\n",
    "              'MoSold_5',\n",
    "              'MoSold_6',\n",
    "              'MoSold_11',\n",
    "              'Neighborhood_3',\n",
    "              'Neighborhood_4',\n",
    "              'Neighborhood_5',\n",
    "              'Neighborhood_10',\n",
    "              'Neighborhood_11',\n",
    "              'Neighborhood_16',\n",
    "              'Neighborhood_17',\n",
    "              'Neighborhood_19',\n",
    "              'Neighborhood_22',\n",
    "              'Neighborhood_24',\n",
    "              'OverallCond_7',\n",
    "              'OverallQual_5',\n",
    "              'OverallQual_6',\n",
    "              'OverallQual_7',\n",
    "              'OverallQual_9',\n",
    "              'PavedDrive_0',\n",
    "              'PavedDrive_2',\n",
    "              'SaleCondition_1',\n",
    "              'SaleCondition_2',\n",
    "              'SaleCondition_5',\n",
    "              'SaleType_4',\n",
    "              'BedroomAbvGr_1',\n",
    "              'BedroomAbvGr_4',\n",
    "              'BedroomAbvGr_5',\n",
    "              'HalfBath_1',\n",
    "              'TotalBath_1.0',\n",
    "              'TotalBath_2.5']\n",
    "\n",
    "display(df[predictors].head())\n",
    "display(df[[response]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are training on a response which is the log of 1 + the sale price\n",
    "# transform prediction back to original basis with expm1 and evaluate vs. original\n",
    "\n",
    "def evaluate(y_train, y_pred_train, y_test, y_pred_test):\n",
    "    \"\"\"evaluate in train_test split\"\"\"\n",
    "    print('Train RMSE', np.sqrt(mean_squared_error(np.expm1(y_train), np.expm1(y_pred_train))))\n",
    "    print('Train R-squared', r2_score(np.expm1(y_train), np.expm1(y_pred_train)))\n",
    "    print('Train MAE', mean_absolute_error(np.expm1(y_train), np.expm1(y_pred_train)))\n",
    "    print()\n",
    "    print('Test RMSE', np.sqrt(mean_squared_error(np.expm1(y_test), np.expm1(y_pred_test))))\n",
    "    print('Test R-squared', r2_score(np.expm1(y_test), np.expm1(y_pred_test)))\n",
    "    print('Test MAE', mean_absolute_error(np.expm1(y_test), np.expm1(y_pred_test)))\n",
    "\n",
    "MEAN_RESPONSE=df[response].mean()\n",
    "def cv_to_raw(cv_val, mean_response=MEAN_RESPONSE):\n",
    "    \"\"\"convert log1p rmse to underlying SalePrice error\"\"\"\n",
    "    # MEAN_RESPONSE assumes folds have same mean response, which is true in expectation but not in each fold\n",
    "    # we can also pass the actual response for each fold\n",
    "    # but we're usually looking to consistently convert the log value to a more meaningful unit\n",
    "    return np.expm1(mean_response+cv_val) - np.expm1(mean_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# always use same k-folds for reproducibility\n",
    "kfolds = KFold(n_splits=10, shuffle=True, random_state=RANDOMSTATE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline linear regression\n",
    "- Raw CV RMSE 18191.9791"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression\n",
      "100 predictors\n",
      "\n",
      "Log1p CV RMSE 0.1037 (STD 0.0099)\n",
      "Raw CV RMSE 18192 (STD 1839)\n",
      "CPU times: user 63 ms, sys: 50.5 ms, total: 113 ms\n",
      "Wall time: 998 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Tune lr search space for alphas and l1_ratio\n",
    "print(\"LinearRegression\")\n",
    "\n",
    "print(len(predictors), \"predictors\")\n",
    "\n",
    "lr = LinearRegression()\n",
    "\n",
    "# evaluate using kfolds\n",
    "scores = -cross_val_score(lr, df[predictors], df[response],\n",
    "                          scoring=\"neg_root_mean_squared_error\",\n",
    "                          cv=kfolds,\n",
    "                          n_jobs=-1)\n",
    "raw_scores = [cv_to_raw(x) for x in scores]\n",
    "print()\n",
    "print(\"Log1p CV RMSE %.04f (STD %.04f)\" % (np.mean(scores), np.std(scores)))\n",
    "print(\"Raw CV RMSE %.0f (STD %.0f)\" % (np.mean(raw_scores), np.std(raw_scores)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Native Sklearn xxxCV\n",
    "- LogisticRegressionCV, LassoCV, RidgeCV, ElasticNetCV, etc.\n",
    "- Test many hyperparameters in parallel with multithreading\n",
    "- Note improvement vs. LinearRegression due to controlling overfitting\n",
    "- RMSE $18103\n",
    "- Time 5s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElasticnetCV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "..................................................................................................................................................................................................................................................................................................................................................[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.5s\n",
      "................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................[Parallel(n_jobs=-1)]: Done 130 out of 130 | elapsed:    1.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l1_ratio 0.01\n",
      "alpha 0.0031622776601683794\n",
      "\n",
      "Log1p CV RMSE 0.1030 (STD 0.0109)\n",
      "Raw CV RMSE 18061 (STD 2008)\n",
      "CPU times: user 6.65 s, sys: 5.32 s, total: 12 s\n",
      "Wall time: 1.87 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Tune elasticnet search space for alphas and L1_ratio\n",
    "# predictor selection used to create the training set used lasso\n",
    "# so l1 parameter is close to 0\n",
    "# could use ridge (eg elasticnet with 0 L1 regularization)\n",
    "# but then only 1 param, more general and useful to do this with elasticnet\n",
    "print(\"ElasticnetCV\")\n",
    "\n",
    "# make pipeline\n",
    "# with regularization must scale predictors\n",
    "elasticnetcv = make_pipeline(RobustScaler(),\n",
    "                             ElasticNetCV(max_iter=100000, \n",
    "                                          l1_ratio=[0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.99],\n",
    "                                          alphas=np.logspace(-4, -2, 9),\n",
    "                                          cv=kfolds,\n",
    "                                          n_jobs=-1,\n",
    "                                          verbose=1,\n",
    "                                         ))\n",
    "\n",
    "#train and get hyperparams\n",
    "elasticnetcv.fit(df[predictors], df[response])\n",
    "l1_ratio = elasticnetcv._final_estimator.l1_ratio_\n",
    "alpha = elasticnetcv._final_estimator.alpha_\n",
    "print('l1_ratio', l1_ratio)\n",
    "print('alpha', alpha)\n",
    "\n",
    "# evaluate using kfolds on full dataset\n",
    "# I don't see API to get CV error from elasticnetcv, so we use cross_val_score\n",
    "elasticnet = ElasticNet(alpha=alpha,\n",
    "                        l1_ratio=l1_ratio,\n",
    "                        max_iter=10000)\n",
    "\n",
    "scores = -cross_val_score(elasticnet, df[predictors], df[response],\n",
    "                          scoring=\"neg_root_mean_squared_error\",\n",
    "                          cv=kfolds,\n",
    "                          n_jobs=-1)\n",
    "raw_scores = [cv_to_raw(x) for x in scores]\n",
    "print()\n",
    "print(\"Log1p CV RMSE %.04f (STD %.04f)\" % (np.mean(scores), np.std(scores)))\n",
    "print(\"Raw CV RMSE %.0f (STD %.0f)\" % (np.mean(raw_scores), np.std(raw_scores)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearchCV\n",
    "- Useful for algos with no native multithreaded xxxCV\n",
    "- Test many hyperparameter combinations in parallel with multithreading\n",
    "- Similar result vs ElasticNetCV, not exact, need more research as to why\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CV on full dataset\n",
      "Fitting 10 folds for each of 117 candidates, totalling 1170 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 656 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=-1)]: Done 1170 out of 1170 | elapsed:    4.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params {'alpha': 0.0031622776601683794, 'l1_ratio': 0.01}\n",
      "best score 0.010637685614240404\n",
      "Log1p CV RMSE 0.103139\n",
      "Raw CV RMSE 18075\n",
      "ElasticNet(alpha=0.0031622776601683794, l1_ratio=0.01, max_iter=100000)\n",
      "\n",
      "Log1p CV RMSE 0.103003 (STD 0.0109)\n",
      "Raw CV RMSE 18061 (STD 2008)\n",
      "weighted average 0.103023\n",
      "CPU times: user 1.37 s, sys: 393 ms, total: 1.76 s\n",
      "Wall time: 4.38 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gs = make_pipeline(RobustScaler(),\n",
    "                   GridSearchCV(ElasticNet(max_iter=100000),\n",
    "                                param_grid={'l1_ratio': [0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.99],\n",
    "                                            'alpha': np.logspace(-4, -2, 9),\n",
    "                                           },\n",
    "                                scoring='neg_mean_squared_error',\n",
    "                                refit=True,\n",
    "                                cv=kfolds,\n",
    "                                n_jobs=-1,\n",
    "                                verbose=1\n",
    "                               ))\n",
    "\n",
    "# do cv using kfolds on full dataset\n",
    "print(\"\\nCV on full dataset\")\n",
    "gs.fit(df[predictors], df[response])\n",
    "print('best params', gs._final_estimator.best_params_)\n",
    "print('best score', -gs._final_estimator.best_score_)\n",
    "l1_ratio = gs._final_estimator.best_params_['l1_ratio']\n",
    "alpha = gs._final_estimator.best_params_['alpha']\n",
    "print(\"Log1p CV RMSE %.06f\" % (np.sqrt(-gs._final_estimator.best_score_)))\n",
    "print(\"Raw CV RMSE %.0f\" % (cv_to_raw(np.sqrt(-gs._final_estimator.best_score_))))\n",
    "\n",
    "# we shouldn't need to do this\n",
    "elasticnet = ElasticNet(alpha=alpha,\n",
    "                        l1_ratio=l1_ratio,\n",
    "                        max_iter=100000)\n",
    "print(elasticnet)\n",
    "\n",
    "scores = -cross_val_score(elasticnet, df[predictors], df[response],\n",
    "                          scoring=\"neg_root_mean_squared_error\",\n",
    "                          cv=kfolds,\n",
    "                          n_jobs=-1)\n",
    "raw_scores = [cv_to_raw(x) for x in scores]\n",
    "print()\n",
    "print(\"Log1p CV RMSE %.06f (STD %.04f)\" % (np.mean(scores), np.std(scores)))\n",
    "print(\"Raw CV RMSE %.0f (STD %.0f)\" % (np.mean(raw_scores), np.std(raw_scores)))\n",
    "\n",
    "# difference in average CV scores reported by GridSearchCV and cross_val_score\n",
    "# with same alpha, l1_ratio, kfolds\n",
    "# one reason could be that we used simple average, GridSearchCV is weighted by # of samples per fold?\n",
    "nsamples = [len(z[1]) for z in kfolds.split(df)]\n",
    "print(\"weighted average %.06f\" % np.average(scores, weights=nsamples))\n",
    "# still tiny difference, not sure why, also ElasticSearchCV shows fewer fits, takes less time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElasticNet(alpha=0.0031622776601683794, l1_ratio=0.01, max_iter=10000)\n",
      "0.103003\n"
     ]
    }
   ],
   "source": [
    "# roll-our-own CV \n",
    "# matches cross_val_score\n",
    "alpha = 0.0031622776601683794\n",
    "l1_ratio = 0.01\n",
    "regressor = ElasticNet(alpha=alpha,\n",
    "                       l1_ratio=l1_ratio,\n",
    "                       max_iter=10000)\n",
    "print(regressor)\n",
    "cverrors = []\n",
    "for train_fold, cv_fold in kfolds.split(df): \n",
    "    fold_X_train=df[predictors].values[train_fold]\n",
    "    fold_y_train=df[response].values[train_fold]\n",
    "    fold_X_test=df[predictors].values[cv_fold]\n",
    "    fold_y_test=df[response].values[cv_fold]\n",
    "    regressor.fit(fold_X_train, fold_y_train)\n",
    "    y_pred_test=regressor.predict(fold_X_test)\n",
    "    cverrors.append(np.sqrt(mean_squared_error(fold_y_test, y_pred_test)))\n",
    "    \n",
    "print(\"%.06f\" % np.average(cverrors))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Sequential Grid Search \n",
    "- XGBoost has native multithreading, CV\n",
    "- XGBoost has many tuning parameters so a complete grid search has an unreasonable number of combinations\n",
    "- We tune reduced sets sequentially and use early stopping. \n",
    "\n",
    "### Tuning methodology\n",
    "- Set an initial set of starting parameters\n",
    "- Do 10-fold CV\n",
    "- Use early stopping to halt training in each fold if no improvement after eg 100 rounds, pick hyperparameters to minimize average error over kfolds\n",
    "- Tune sequentially on groups of hyperparameters that don't interact too much between groups to reduce combinations\n",
    "- Tune max_depth and min_child_weight \n",
    "- Tune subsample and colsample_bytree\n",
    "- Tune alpha, lambda and gamma (regularization)\n",
    "- Tune learning rate: lower learning rate will need more rounds/n_estimators\n",
    "- Retrain and evaluate on full dataset with best learning rate and best n_estimators (average stopping point over kfolds)\n",
    "\n",
    "### Notes\n",
    "- It doesn't seem possible to get XGBoost early stopping and also use GridSearchCV. GridSearchCV doesn't pass the kfolds in a way that XGboost understands for early stopping\n",
    "- 2 alternative approaches \n",
    "    - use native xgboost .cv which understands early stopping but doesn't use sklearn API (uses DMatrix, not np array or dataframe)\n",
    "    - use sklearn API and roll our own grid search instead of GridSearchCV (used below)\n",
    "- XGboost terminology differs from sklearn\n",
    "    - boost_rounds = n_estimators\n",
    "    - eta = learning_rate\n",
    "- parameter reference: https://xgboost.readthedocs.io/en/latest/parameter.html\n",
    "- training reference: https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.training\n",
    "- times are wall times on an amazon t2.2xlarge instance  \n",
    "- to set up environment:\n",
    "    - `conda create --name hyperparam python=3.8`\n",
    "    - `conda activate hyperparam`\n",
    "    - `conda install jupyter`\n",
    "    - `pip install -r requirements.txt`\n",
    "- round 1 Wall time: 6min 23s\n",
    "- round 2 Wall time: 19min 22s\n",
    "- round 3 Wall time: 5min 30s\n",
    "- round 4 Wall time: 4min 54s\n",
    "- total time 36:09\n",
    "- RMSE 18783.117031"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03:53:50 params    0: {'max_depth': 2, 'min_child_weight': 2, 'subsample': 0.6, 'colsample_bytree': 0.05, 'reg_alpha': 0.003162, 'reg_lambda': 0.1, 'gamma': 0, 'learning_rate': 0.1}\n",
      "[03:53:50] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { early_stopping_rounds } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[03:53:51] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { early_stopping_rounds } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[03:53:51] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { early_stopping_rounds } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[03:53:52] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { early_stopping_rounds } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[03:53:52] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { early_stopping_rounds } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[03:53:53] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { early_stopping_rounds } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[03:53:54] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { early_stopping_rounds } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[03:53:54] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { early_stopping_rounds } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[03:53:54] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { early_stopping_rounds } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[03:53:55] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { early_stopping_rounds } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "03:53:55   0 result mean: 0.109786 std: 0.010449, iter: 476.00\n",
      "03:53:55 params    1: {'max_depth': 2, 'min_child_weight': 2, 'subsample': 0.6, 'colsample_bytree': 0.05, 'reg_alpha': 0.003162, 'reg_lambda': 0.1, 'gamma': 0, 'learning_rate': 0.03162277660168379}\n",
      "[03:53:55] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { early_stopping_rounds } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[03:53:56] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { early_stopping_rounds } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[03:53:57] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { early_stopping_rounds } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[03:53:58] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { early_stopping_rounds } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[03:53:58] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { early_stopping_rounds } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[03:54:00] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { early_stopping_rounds } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[03:54:01] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { early_stopping_rounds } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[03:54:02] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { early_stopping_rounds } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[03:54:03] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { early_stopping_rounds } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[03:54:04] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { early_stopping_rounds } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "03:54:05   1 result mean: 0.105996 std: 0.012395, iter: 1134.80\n",
      "03:54:05 params    2: {'max_depth': 2, 'min_child_weight': 2, 'subsample': 0.6, 'colsample_bytree': 0.05, 'reg_alpha': 0.003162, 'reg_lambda': 0.1, 'gamma': 0, 'learning_rate': 0.01}\n",
      "[03:54:05] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { early_stopping_rounds } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[03:54:07] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { early_stopping_rounds } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[03:54:09] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { early_stopping_rounds } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:54:13] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { early_stopping_rounds } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[03:54:14] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { early_stopping_rounds } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[03:54:16] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { early_stopping_rounds } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[03:54:19] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { early_stopping_rounds } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[03:54:22] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { early_stopping_rounds } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[03:54:25] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { early_stopping_rounds } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[03:54:27] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { early_stopping_rounds } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "03:54:29   2 result mean: 0.106046 std: 0.013608, iter: 2888.10\n",
      "03:54:29 params    3: {'max_depth': 2, 'min_child_weight': 2, 'subsample': 0.6, 'colsample_bytree': 0.05, 'reg_alpha': 0.003162, 'reg_lambda': 0.1, 'gamma': 0, 'learning_rate': 0.0031622776601683794}\n",
      "[03:54:29] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { early_stopping_rounds } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[03:54:38] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { early_stopping_rounds } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[03:54:43] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { early_stopping_rounds } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[03:54:48] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { early_stopping_rounds } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[03:54:52] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { early_stopping_rounds } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[03:54:59] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { early_stopping_rounds } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[03:55:06] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { early_stopping_rounds } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[03:55:14] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { early_stopping_rounds } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[03:55:21] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { early_stopping_rounds } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[03:55:26] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { early_stopping_rounds } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "03:55:33   3 result mean: 0.106838 std: 0.013188, iter: 7783.70\n",
      "03:55:33 params    4: {'max_depth': 2, 'min_child_weight': 2, 'subsample': 0.6, 'colsample_bytree': 0.05, 'reg_alpha': 0.003162, 'reg_lambda': 0.1, 'gamma': 0, 'learning_rate': 0.001}\n",
      "[03:55:33] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { early_stopping_rounds } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[03:55:52] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { early_stopping_rounds } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[03:56:08] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { early_stopping_rounds } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[03:56:26] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { early_stopping_rounds } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[03:56:37] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { early_stopping_rounds } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[03:56:56] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { early_stopping_rounds } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:57:20] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { early_stopping_rounds } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[03:57:40] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { early_stopping_rounds } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[03:58:01] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { early_stopping_rounds } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[03:58:16] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { early_stopping_rounds } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "03:58:36   4 result mean: 0.107344 std: 0.013132, iter: 22413.30\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_child_weight</th>\n",
       "      <th>subsample</th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>reg_alpha</th>\n",
       "      <th>reg_lambda</th>\n",
       "      <th>reg_gamma</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>rmse</th>\n",
       "      <th>std</th>\n",
       "      <th>best_iter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.003162</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.031623</td>\n",
       "      <td>0.105996</td>\n",
       "      <td>0.012395</td>\n",
       "      <td>1134.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.003162</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.106046</td>\n",
       "      <td>0.013608</td>\n",
       "      <td>2888.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.003162</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.003162</td>\n",
       "      <td>0.106838</td>\n",
       "      <td>0.013188</td>\n",
       "      <td>7783.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.003162</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.107344</td>\n",
       "      <td>0.013132</td>\n",
       "      <td>22413.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.003162</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.109786</td>\n",
       "      <td>0.010449</td>\n",
       "      <td>476.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   max_depth  min_child_weight  subsample  colsample_bytree  reg_alpha  \\\n",
       "1          2                 2        0.6              0.05   0.003162   \n",
       "2          2                 2        0.6              0.05   0.003162   \n",
       "3          2                 2        0.6              0.05   0.003162   \n",
       "4          2                 2        0.6              0.05   0.003162   \n",
       "0          2                 2        0.6              0.05   0.003162   \n",
       "\n",
       "   reg_lambda  reg_gamma  learning_rate      rmse       std  best_iter  \n",
       "1         0.1          0       0.031623  0.105996  0.012395     1134.8  \n",
       "2         0.1          0       0.010000  0.106046  0.013608     2888.1  \n",
       "3         0.1          0       0.003162  0.106838  0.013188     7783.7  \n",
       "4         0.1          0       0.001000  0.107344  0.013132    22413.3  \n",
       "0         0.1          0       0.100000  0.109786  0.010449      476.0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%%time\n",
    "# this cell runs a single round\n",
    "# the full process is \n",
    "# set initial XGboost parameters\n",
    "# remove overrides (search for TODO: in this cell)\n",
    "# run round 1 and override initial max_depth, min_child_weight based on best values (search for TODO:)\n",
    "# run round 2 and override subsample and colsample_bytree based on best values\n",
    "# run round 3 and override reg_alpha, reg_lambda, reg_gamma based on best values\n",
    "# run round 4 and obtain learning_rate and best n_iterations\n",
    "# this is not an exhaustive list but a representative list of most important parameters to tune\n",
    "# see https://xgboost.readthedocs.io/en/latest/parameter.html for all parameters\n",
    "\n",
    "# in XGBoost > 1.0.2 this seems to give a warning\n",
    "# Parameters: { early_stopping_rounds } might not be used.\n",
    "# but early stopping seems to be used correctly\n",
    "\n",
    "# TODO: refactor as a function that takes a config with search spaces\n",
    "# returns a fully specified config with the best values\n",
    "# run 4 times for 4 passes to get a fully cross-validated parameter set.\n",
    "\n",
    "# initial hyperparams\n",
    "max_depth = 5\n",
    "min_child_weight=5\n",
    "colsample_bytree = 0.5\n",
    "subsample = 0.5\n",
    "reg_alpha = 1e-05\n",
    "reg_lambda = 1\n",
    "reg_gamma = 0\n",
    "learning_rate = 0.01\n",
    "\n",
    "BOOST_ROUNDS=50000   # we use early stopping so make this arbitrarily high\n",
    "EARLY_STOPPING_ROUNDS=100 # stop if no improvement after 100 rounds\n",
    "\n",
    "# round 1: tune depth and min_child_weight\n",
    "max_depths = list(range(1,5))\n",
    "min_child_weights = list(range(1,5))\n",
    "gridsearch_params_1 = product(max_depths, min_child_weights)\n",
    "\n",
    "# round 2: tune subsample and colsample_bytree\n",
    "subsamples = np.linspace(0.1, 1.0, 10)\n",
    "colsample_bytrees = np.linspace(0.1, 1.0, 10)\n",
    "gridsearch_params_2 = product(subsamples, colsample_bytrees)\n",
    "\n",
    "# round 2 (refined): tune subsample and colsample_bytree\n",
    "subsamples = np.linspace(0.4, 0.8, 9)\n",
    "colsample_bytrees = np.linspace(0.05, 0.25, 5)\n",
    "gridsearch_params_2 = product(subsamples, colsample_bytrees)\n",
    "\n",
    "# round 3: tune alpha, lambda, gamma\n",
    "reg_alphas = np.logspace(-3, -2, 3)\n",
    "reg_lambdas = np.logspace(-2, 1, 4)\n",
    "reg_gammas = [0]\n",
    "#reg_gammas = np.linspace(0, 5, 6)\n",
    "gridsearch_params_3 = product(reg_alphas, reg_lambdas, reg_gammas)\n",
    "\n",
    "# round 4: learning rate\n",
    "learning_rates = reversed(np.logspace(-3, -1, 5).tolist())\n",
    "gridsearch_params_4 = learning_rates\n",
    "\n",
    "# TODO: remove these overrides to reset the search\n",
    "# override initial parameters after search\n",
    "# round 1:\n",
    "max_depth=2\n",
    "min_child_weight=2\n",
    "# # round 2:\n",
    "subsample=0.60\n",
    "colsample_bytree=0.05\n",
    "# # round 3:  \n",
    "reg_alpha = 0.003162\n",
    "reg_lambda = 0.1\n",
    "reg_gamma = 0\n",
    "\n",
    "def my_cv(df, predictors, response, kfolds, regressor, verbose=False):\n",
    "    \"\"\"Roll our own CV over kfolds with early stopping\"\"\"\n",
    "    metrics = []\n",
    "    best_iterations = []\n",
    "\n",
    "    for train_fold, cv_fold in kfolds.split(df): \n",
    "        fold_X_train=df[predictors].values[train_fold]\n",
    "        fold_y_train=df[response].values[train_fold]\n",
    "        fold_X_test=df[predictors].values[cv_fold]\n",
    "        fold_y_test=df[response].values[cv_fold]\n",
    "        regressor.fit(fold_X_train, fold_y_train,\n",
    "                      early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n",
    "                      eval_set=[(fold_X_test, fold_y_test)],\n",
    "                      eval_metric='rmse',\n",
    "                      verbose=verbose\n",
    "                     )\n",
    "        y_pred_test=regressor.predict(fold_X_test)\n",
    "        metrics.append(np.sqrt(mean_squared_error(fold_y_test, y_pred_test)))\n",
    "        best_iterations.append(xgb.best_iteration)\n",
    "    return np.average(metrics), np.std(metrics), np.average(best_iterations)\n",
    "\n",
    "results = []\n",
    "best_iterations = []\n",
    "\n",
    "# TODO: iteratively uncomment 1 of the following 4 lines\n",
    "# for i, (max_depth, min_child_weight) in enumerate(gridsearch_params_1): # round 1\n",
    "# for i, (subsample, colsample_bytree) in enumerate(gridsearch_params_2): # round 2\n",
    "# for i, (reg_alpha, reg_lambda, reg_gamma) in enumerate(gridsearch_params_3): # round 3\n",
    "for i, learning_rate in enumerate(gridsearch_params_4): # round 4\n",
    "\n",
    "    params = {\n",
    "        'max_depth': max_depth,\n",
    "        'min_child_weight': min_child_weight,\n",
    "        'subsample': subsample,\n",
    "        'colsample_bytree': colsample_bytree,\n",
    "        'reg_alpha': reg_alpha,\n",
    "        'reg_lambda': reg_lambda,\n",
    "        'gamma': reg_gamma,\n",
    "        'learning_rate': learning_rate,\n",
    "    }\n",
    "    print(\"%s params  %3d: %s\" % (datetime.strftime(datetime.now(), \"%T\"), i, params))\n",
    "    xgb = XGBRegressor(\n",
    "        objective='reg:squarederror',\n",
    "        n_estimators=BOOST_ROUNDS,\n",
    "        early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n",
    "        random_state=RANDOMSTATE,    \n",
    "        verbosity=1,\n",
    "        n_jobs=-1,\n",
    "        booster='gbtree',   \n",
    "        scale_pos_weight=1,      \n",
    "        **params\n",
    "    )\n",
    "    \n",
    "    metric_rmse, metric_std, best_iteration = my_cv(df, predictors, response, kfolds, xgb, verbose=False)    \n",
    "    results.append([max_depth, min_child_weight, subsample, colsample_bytree, reg_alpha, reg_lambda, reg_gamma, \n",
    "                   learning_rate, metric_rmse, metric_std, best_iteration])\n",
    "    \n",
    "    print(\"%s %3d result mean: %.6f std: %.6f, iter: %.2f\" % (datetime.strftime(datetime.now(), \"%T\"), i, metric_rmse, metric_std, best_iteration))\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame(results, columns=['max_depth', 'min_child_weight', 'subsample', 'colsample_bytree', \n",
    "                               'reg_alpha', 'reg_lambda', 'reg_gamma', 'learning_rate', 'rmse', 'std', 'best_iter']).sort_values('rmse')\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 2, 'min_child_weight': 2.0, 'subsample': 0.6, 'colsample_bytree': 0.05, 'reg_alpha': 0.003162, 'reg_lambda': 0.1, 'gamma': 0.0, 'learning_rate': 0.03162277660168379, 'n_estimators': 1134}\n"
     ]
    }
   ],
   "source": [
    "# get best hyperparameters from results dataframe\n",
    "max_depth = int(results_df.iloc[0]['max_depth'])\n",
    "min_child_weight = results_df.iloc[0]['min_child_weight']\n",
    "subsample = results_df.iloc[0]['subsample']\n",
    "colsample_bytree = results_df.iloc[0]['colsample_bytree']\n",
    "reg_alpha = results_df.iloc[0]['reg_alpha']\n",
    "reg_lambda = results_df.iloc[0]['reg_lambda']\n",
    "reg_gamma = results_df.iloc[0]['reg_gamma']\n",
    "learning_rate = results_df.iloc[0]['learning_rate']\n",
    "N_ESTIMATORS = int(results_df.iloc[0]['best_iter'])\n",
    "\n",
    "params = {\n",
    "    'max_depth': int(max_depth),\n",
    "    'min_child_weight': min_child_weight,\n",
    "    'subsample': subsample,\n",
    "    'colsample_bytree': colsample_bytree,\n",
    "    'reg_alpha': reg_alpha,\n",
    "    'reg_lambda': reg_lambda,\n",
    "    'gamma': reg_gamma,\n",
    "    'learning_rate': learning_rate,\n",
    "    'n_estimators': N_ESTIMATORS,    \n",
    "}\n",
    "\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBRegressor(base_score=None, booster='gbtree', colsample_bylevel=None,\n",
      "             colsample_bynode=None, colsample_bytree=0.05, gamma=0.0,\n",
      "             gpu_id=None, importance_type='gain', interaction_constraints=None,\n",
      "             learning_rate=0.03162277660168379, max_delta_step=None,\n",
      "             max_depth=2, min_child_weight=2.0, missing=nan,\n",
      "             monotone_constraints=None, n_estimators=1134, n_jobs=-1,\n",
      "             num_parallel_tree=None, random_state=42, reg_alpha=0.003162,\n",
      "             reg_lambda=0.1, scale_pos_weight=1, subsample=0.6,\n",
      "             tree_method=None, validate_parameters=None, verbosity=1)\n",
      "\n",
      "Log1p CV RMSE 0.106893 (STD 0.0125)\n",
      "Raw CV RMSE 18783 (STD 2307)\n",
      "CPU times: user 42.8 ms, sys: 7.94 ms, total: 50.7 ms\n",
      "Wall time: 2.35 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# CV and evaluate with best params and without early stopping\n",
    "\n",
    "xgb = XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    random_state=RANDOMSTATE,    \n",
    "    verbosity=1,\n",
    "    n_jobs=-1,\n",
    "    booster='gbtree',   \n",
    "    scale_pos_weight=1,        \n",
    "    **params\n",
    ")\n",
    "print(xgb)\n",
    "\n",
    "scores = -cross_val_score(xgb, df[predictors], df[response],\n",
    "                          scoring=\"neg_root_mean_squared_error\",\n",
    "                          cv=kfolds,\n",
    "                          n_jobs=-1)\n",
    "raw_scores = [cv_to_raw(x) for x in scores]\n",
    "print()\n",
    "print(\"Log1p CV RMSE %.06f (STD %.04f)\" % (np.mean(scores), np.std(scores)))\n",
    "print(\"Raw CV RMSE %.0f (STD %.0f)\" % (np.mean(raw_scores), np.std(raw_scores)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HyperOpt\n",
    "\n",
    "HyperOpt is a Bayesian optimization algorithm by [James Bergstra et al.](https://conference.scipy.org/proceedings/scipy2013/pdfs/bergstra_hyperopt.pdf)\n",
    "  - [Home page](http://hyperopt.github.io/hyperopt/)\n",
    "  - [GitHub](https://github.com/hyperopt/hyperopt)\n",
    "  - [HyperOpt: Bayesian Hyperparameter Optimization](https://blog.dominodatalab.com/hyperopt-bayesian-hyperparameter-optimization/), Subir Mansukhani (2019)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refactor to give ray.tune a single function of hyperparameters to optimize\n",
    "def my_xgb(config):\n",
    "    \n",
    "    # fix these configs to match calling convention\n",
    "    config['max_depth'] += 2   # hyperopt needs left to start at 0 but we want to start at 2\n",
    "    config['max_depth'] = int(config['max_depth'])\n",
    "    config['n_estimators'] = int(config['n_estimators'])   # pass float eg loguniform distribution, use int\n",
    "    \n",
    "    xgb = XGBRegressor(\n",
    "        objective='reg:squarederror',\n",
    "        n_jobs=1,\n",
    "        random_state=RANDOMSTATE,\n",
    "        booster='gbtree',   \n",
    "        scale_pos_weight=1, \n",
    "        **config,\n",
    "    )\n",
    "    scores = np.sqrt(-cross_val_score(xgb, df[predictors], df[response],\n",
    "                                      scoring=\"neg_mean_squared_error\",\n",
    "                                      cv=kfolds))\n",
    "    tune.report(mse=np.mean(scores))\n",
    "    return {'mse': np.mean(scores)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Session not detected. You should not be calling this function outside `tune.run` or while using the class API. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mse': 0.10689314042904505}\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    'max_depth': max_depth-2,\n",
    "    'min_child_weight': min_child_weight,\n",
    "    'subsample': subsample,\n",
    "    'colsample_bytree': colsample_bytree,\n",
    "    'reg_alpha': reg_alpha,\n",
    "    'reg_lambda': reg_lambda,\n",
    "    'gamma': reg_gamma,\n",
    "    'learning_rate': learning_rate,\n",
    "    'n_estimators': N_ESTIMATORS,\n",
    "}\n",
    "\n",
    "xgb = my_xgb(config)\n",
    "\n",
    "print(xgb)\n",
    "# same mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.2/31.4 GiB<br>Using AsyncHyperBand: num_stopped=0\n",
       "Bracket: Iter 64.000: None | Iter 16.000: None | Iter 4.000: None | Iter 1.000: None<br>Resources requested: 0/8 CPUs, 0/0 GPUs, 0.0/18.02 GiB heap, 0.0/6.2 GiB objects<br>Result logdir: /home/ubuntu/ray_results/xgb_hyperopt<br>Number of trials: 128 (128 ERROR)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name     </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">  colsample_bytree</th><th style=\"text-align: right;\">  gamma</th><th style=\"text-align: right;\">  learning_rate</th><th style=\"text-align: right;\">  max_depth</th><th style=\"text-align: right;\">  min_child_weight</th><th style=\"text-align: right;\">  n_estimators</th><th style=\"text-align: right;\">  reg_alpha</th><th style=\"text-align: right;\">  reg_lambda</th><th style=\"text-align: right;\">  subsample</th><th>wandb/api_key_file  </th><th>wandb/log_config  </th><th>wandb/project  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>my_xgb_ba57a242</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.7 </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.0105391 </td><td style=\"text-align: right;\">          0</td><td style=\"text-align: right;\">                 4</td><td style=\"text-align: right;\">       435.406</td><td style=\"text-align: right;\">0.000188888</td><td style=\"text-align: right;\"> 9.90358    </td><td style=\"text-align: right;\">       0.65</td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_ba5c05c6</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.3 </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.00673344</td><td style=\"text-align: right;\">          3</td><td style=\"text-align: right;\">                 2</td><td style=\"text-align: right;\">       428.153</td><td style=\"text-align: right;\">0.538855   </td><td style=\"text-align: right;\"> 2.32463    </td><td style=\"text-align: right;\">       0.6 </td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_ba5fd782</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.15</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.00133237</td><td style=\"text-align: right;\">          5</td><td style=\"text-align: right;\">                 4</td><td style=\"text-align: right;\">      7988.18 </td><td style=\"text-align: right;\">0.00163546 </td><td style=\"text-align: right;\">19.9476     </td><td style=\"text-align: right;\">       0.8 </td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_ba6372f2</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.2 </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.0187659 </td><td style=\"text-align: right;\">          2</td><td style=\"text-align: right;\">                 3</td><td style=\"text-align: right;\">       735.64 </td><td style=\"text-align: right;\">0.000262686</td><td style=\"text-align: right;\">32.5338     </td><td style=\"text-align: right;\">       0.6 </td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_ba67813a</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.6 </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.00155207</td><td style=\"text-align: right;\">          0</td><td style=\"text-align: right;\">                 3</td><td style=\"text-align: right;\">      4032.08 </td><td style=\"text-align: right;\">0.0246711  </td><td style=\"text-align: right;\"> 0.00107456 </td><td style=\"text-align: right;\">       0.8 </td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_ba6b9298</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.15</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.00106609</td><td style=\"text-align: right;\">          1</td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">      2700.39 </td><td style=\"text-align: right;\">0.0127785  </td><td style=\"text-align: right;\"> 0.0424791  </td><td style=\"text-align: right;\">       0.8 </td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_ba6fb062</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.4 </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.00556109</td><td style=\"text-align: right;\">          3</td><td style=\"text-align: right;\">                 2</td><td style=\"text-align: right;\">       238.15 </td><td style=\"text-align: right;\">0.000451262</td><td style=\"text-align: right;\">43.2731     </td><td style=\"text-align: right;\">       0.75</td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_ba717956</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.55</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.0154611 </td><td style=\"text-align: right;\">          0</td><td style=\"text-align: right;\">                 1</td><td style=\"text-align: right;\">      5605.97 </td><td style=\"text-align: right;\">0.00272955 </td><td style=\"text-align: right;\"> 4.48938    </td><td style=\"text-align: right;\">       0.7 </td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_ba75293e</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.5 </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.00126552</td><td style=\"text-align: right;\">          5</td><td style=\"text-align: right;\">                 3</td><td style=\"text-align: right;\">      4734.92 </td><td style=\"text-align: right;\">0.00967981 </td><td style=\"text-align: right;\">33.1173     </td><td style=\"text-align: right;\">       0.65</td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_ba783f0c</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.75</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.00153686</td><td style=\"text-align: right;\">          2</td><td style=\"text-align: right;\">                 2</td><td style=\"text-align: right;\">       678.644</td><td style=\"text-align: right;\">0.306625   </td><td style=\"text-align: right;\"> 1.59856    </td><td style=\"text-align: right;\">       0.9 </td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_ba7baa3e</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.3 </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.00354348</td><td style=\"text-align: right;\">          4</td><td style=\"text-align: right;\">                 4</td><td style=\"text-align: right;\">       322.93 </td><td style=\"text-align: right;\">0.092285   </td><td style=\"text-align: right;\"> 8.07198    </td><td style=\"text-align: right;\">       0.4 </td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_ba7f7542</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.55</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.0274633 </td><td style=\"text-align: right;\">          1</td><td style=\"text-align: right;\">                 4</td><td style=\"text-align: right;\">      1059.54 </td><td style=\"text-align: right;\">0.00883192 </td><td style=\"text-align: right;\"> 0.0141543  </td><td style=\"text-align: right;\">       0.85</td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_ba815312</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.35</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.00591754</td><td style=\"text-align: right;\">          2</td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">       413.973</td><td style=\"text-align: right;\">0.693163   </td><td style=\"text-align: right;\"> 0.00697508 </td><td style=\"text-align: right;\">       0.85</td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_ba8324e4</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.1 </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.00222835</td><td style=\"text-align: right;\">          5</td><td style=\"text-align: right;\">                 0</td><td style=\"text-align: right;\">       562.024</td><td style=\"text-align: right;\">0.251502   </td><td style=\"text-align: right;\"> 0.000119977</td><td style=\"text-align: right;\">       0.65</td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_ba84e306</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.75</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.00156672</td><td style=\"text-align: right;\">          5</td><td style=\"text-align: right;\">                 4</td><td style=\"text-align: right;\">       968.572</td><td style=\"text-align: right;\">0.000895231</td><td style=\"text-align: right;\">33.5653     </td><td style=\"text-align: right;\">       0.7 </td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_ba86beb0</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.1 </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.00458952</td><td style=\"text-align: right;\">          5</td><td style=\"text-align: right;\">                 0</td><td style=\"text-align: right;\">      2942.74 </td><td style=\"text-align: right;\">0.00300133 </td><td style=\"text-align: right;\"> 0.691927   </td><td style=\"text-align: right;\">       0.7 </td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_ba888100</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.6 </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.00433395</td><td style=\"text-align: right;\">          3</td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">      6339.39 </td><td style=\"text-align: right;\">0.00145562 </td><td style=\"text-align: right;\"> 0.000560021</td><td style=\"text-align: right;\">       0.65</td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_ba8a3176</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.1 </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.0230778 </td><td style=\"text-align: right;\">          2</td><td style=\"text-align: right;\">                 3</td><td style=\"text-align: right;\">      4374.43 </td><td style=\"text-align: right;\">0.00957733 </td><td style=\"text-align: right;\"> 1.21965    </td><td style=\"text-align: right;\">       0.75</td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_ba8bd56c</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.65</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.0815055 </td><td style=\"text-align: right;\">          4</td><td style=\"text-align: right;\">                 0</td><td style=\"text-align: right;\">      2561.93 </td><td style=\"text-align: right;\">0.175053   </td><td style=\"text-align: right;\"> 1.47278    </td><td style=\"text-align: right;\">       0.55</td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_ba8d753e</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.35</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.00821375</td><td style=\"text-align: right;\">          2</td><td style=\"text-align: right;\">                 2</td><td style=\"text-align: right;\">       110.345</td><td style=\"text-align: right;\">0.22884    </td><td style=\"text-align: right;\">17.8103     </td><td style=\"text-align: right;\">       0.6 </td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_ba8f1c86</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.8 </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.0416909 </td><td style=\"text-align: right;\">          0</td><td style=\"text-align: right;\">                 1</td><td style=\"text-align: right;\">       165.494</td><td style=\"text-align: right;\">0.000106841</td><td style=\"text-align: right;\"> 0.283102   </td><td style=\"text-align: right;\">       0.5 </td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_ba93a436</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.7 </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.0804891 </td><td style=\"text-align: right;\">          4</td><td style=\"text-align: right;\">                 0</td><td style=\"text-align: right;\">      1742.54 </td><td style=\"text-align: right;\">0.0910498  </td><td style=\"text-align: right;\"> 0.200186   </td><td style=\"text-align: right;\">       0.5 </td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_ba97d54c</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.45</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.0112987 </td><td style=\"text-align: right;\">          0</td><td style=\"text-align: right;\">                 2</td><td style=\"text-align: right;\">       112.108</td><td style=\"text-align: right;\">0.0294126  </td><td style=\"text-align: right;\">78.6198     </td><td style=\"text-align: right;\">       0.45</td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_ba9c3650</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.8 </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.0402148 </td><td style=\"text-align: right;\">          0</td><td style=\"text-align: right;\">                 1</td><td style=\"text-align: right;\">       181.694</td><td style=\"text-align: right;\">0.000106834</td><td style=\"text-align: right;\"> 0.244242   </td><td style=\"text-align: right;\">       0.5 </td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_baa0c3d2</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.7 </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.0692628 </td><td style=\"text-align: right;\">          4</td><td style=\"text-align: right;\">                 0</td><td style=\"text-align: right;\">      1594.42 </td><td style=\"text-align: right;\">0.0642315  </td><td style=\"text-align: right;\"> 0.0354821  </td><td style=\"text-align: right;\">       0.55</td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_baa52e40</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.5 </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.0130061 </td><td style=\"text-align: right;\">          0</td><td style=\"text-align: right;\">                 4</td><td style=\"text-align: right;\">       106.503</td><td style=\"text-align: right;\">0.030306   </td><td style=\"text-align: right;\">78.0165     </td><td style=\"text-align: right;\">       0.4 </td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_baa9afb0</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.8 </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.0430367 </td><td style=\"text-align: right;\">          0</td><td style=\"text-align: right;\">                 1</td><td style=\"text-align: right;\">       197.393</td><td style=\"text-align: right;\">0.000127316</td><td style=\"text-align: right;\"> 0.38486    </td><td style=\"text-align: right;\">       0.45</td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_baae3e0e</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.7 </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.0593905 </td><td style=\"text-align: right;\">          4</td><td style=\"text-align: right;\">                 0</td><td style=\"text-align: right;\">      1390.13 </td><td style=\"text-align: right;\">0.0621831  </td><td style=\"text-align: right;\"> 0.0723834  </td><td style=\"text-align: right;\">       0.55</td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bab2a688</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.45</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.010992  </td><td style=\"text-align: right;\">          0</td><td style=\"text-align: right;\">                 4</td><td style=\"text-align: right;\">       100.302</td><td style=\"text-align: right;\">0.0294485  </td><td style=\"text-align: right;\"> 5.83659    </td><td style=\"text-align: right;\">       0.6 </td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bab76cd6</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.8 </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.0355751 </td><td style=\"text-align: right;\">          0</td><td style=\"text-align: right;\">                 1</td><td style=\"text-align: right;\">       301.714</td><td style=\"text-align: right;\">0.000215304</td><td style=\"text-align: right;\"> 0.677919   </td><td style=\"text-align: right;\">       0.45</td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_babbf8aa</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.65</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.00306236</td><td style=\"text-align: right;\">          4</td><td style=\"text-align: right;\">                 0</td><td style=\"text-align: right;\">      1396.14 </td><td style=\"text-align: right;\">0.950564   </td><td style=\"text-align: right;\"> 0.00353952 </td><td style=\"text-align: right;\">       0.55</td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bac08ca8</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.25</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.00766333</td><td style=\"text-align: right;\">          0</td><td style=\"text-align: right;\">                 4</td><td style=\"text-align: right;\">       140.009</td><td style=\"text-align: right;\">0.00406967 </td><td style=\"text-align: right;\"> 5.99834    </td><td style=\"text-align: right;\">       0.6 </td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bac4e820</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.75</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.0307371 </td><td style=\"text-align: right;\">          1</td><td style=\"text-align: right;\">                 1</td><td style=\"text-align: right;\">       298.096</td><td style=\"text-align: right;\">0.000297429</td><td style=\"text-align: right;\"> 0.113994   </td><td style=\"text-align: right;\">       0.45</td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bac96cd8</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.65</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.00267181</td><td style=\"text-align: right;\">          4</td><td style=\"text-align: right;\">                 0</td><td style=\"text-align: right;\">       486.392</td><td style=\"text-align: right;\">0.866338   </td><td style=\"text-align: right;\"> 0.00293319 </td><td style=\"text-align: right;\">       0.75</td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bacdfdfc</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.25</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.00819142</td><td style=\"text-align: right;\">          3</td><td style=\"text-align: right;\">                 4</td><td style=\"text-align: right;\">       129.296</td><td style=\"text-align: right;\">0.000733121</td><td style=\"text-align: right;\">14.6433     </td><td style=\"text-align: right;\">       0.6 </td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bad25bae</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.35</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.0186949 </td><td style=\"text-align: right;\">          2</td><td style=\"text-align: right;\">                 2</td><td style=\"text-align: right;\">       239.762</td><td style=\"text-align: right;\">0.00561217 </td><td style=\"text-align: right;\">11.2702     </td><td style=\"text-align: right;\">       0.7 </td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bad6f3c6</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.6 </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.050706  </td><td style=\"text-align: right;\">          0</td><td style=\"text-align: right;\">                 1</td><td style=\"text-align: right;\">       387.869</td><td style=\"text-align: right;\">0.000154608</td><td style=\"text-align: right;\"> 3.00124    </td><td style=\"text-align: right;\">       0.5 </td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_badb8fee</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.55</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.0981784 </td><td style=\"text-align: right;\">          0</td><td style=\"text-align: right;\">                 1</td><td style=\"text-align: right;\">       592.744</td><td style=\"text-align: right;\">0.000421097</td><td style=\"text-align: right;\"> 0.0300833  </td><td style=\"text-align: right;\">       0.65</td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bae0772a</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.7 </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.0923905 </td><td style=\"text-align: right;\">          1</td><td style=\"text-align: right;\">                 3</td><td style=\"text-align: right;\">      8696.81 </td><td style=\"text-align: right;\">0.0159487  </td><td style=\"text-align: right;\"> 0.10013    </td><td style=\"text-align: right;\">       0.75</td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bae589ea</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.5 </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.0170539 </td><td style=\"text-align: right;\">          4</td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">       807.598</td><td style=\"text-align: right;\">0.377887   </td><td style=\"text-align: right;\"> 0.000497298</td><td style=\"text-align: right;\">       0.5 </td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_baea7734</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.45</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.0113824 </td><td style=\"text-align: right;\">          3</td><td style=\"text-align: right;\">                 2</td><td style=\"text-align: right;\">       249.1  </td><td style=\"text-align: right;\">0.0183581  </td><td style=\"text-align: right;\">95.0758     </td><td style=\"text-align: right;\">       0.8 </td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_baef8b20</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.4 </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.0228032 </td><td style=\"text-align: right;\">          0</td><td style=\"text-align: right;\">                 2</td><td style=\"text-align: right;\">       373.72 </td><td style=\"text-align: right;\">0.00137162 </td><td style=\"text-align: right;\">67.5462     </td><td style=\"text-align: right;\">       0.9 </td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_baf49372</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.75</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.0155082 </td><td style=\"text-align: right;\">          0</td><td style=\"text-align: right;\">                 4</td><td style=\"text-align: right;\">       189.701</td><td style=\"text-align: right;\">0.000633025</td><td style=\"text-align: right;\"> 0.0108827  </td><td style=\"text-align: right;\">       0.85</td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_baf99e6c</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.6 </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.0271343 </td><td style=\"text-align: right;\">          1</td><td style=\"text-align: right;\">                 1</td><td style=\"text-align: right;\">       509.944</td><td style=\"text-align: right;\">0.000193971</td><td style=\"text-align: right;\"> 3.14616    </td><td style=\"text-align: right;\">       0.4 </td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bafed760</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.7 </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.00205838</td><td style=\"text-align: right;\">          4</td><td style=\"text-align: right;\">                 3</td><td style=\"text-align: right;\">      2156.63 </td><td style=\"text-align: right;\">0.0498853  </td><td style=\"text-align: right;\"> 0.0355878  </td><td style=\"text-align: right;\">       0.55</td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bb0376bc</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.55</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.00103425</td><td style=\"text-align: right;\">          5</td><td style=\"text-align: right;\">                 0</td><td style=\"text-align: right;\">      1067.63 </td><td style=\"text-align: right;\">0.0064779  </td><td style=\"text-align: right;\"> 0.00106439 </td><td style=\"text-align: right;\">       0.7 </td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bb08a484</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.2 </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.00504001</td><td style=\"text-align: right;\">          3</td><td style=\"text-align: right;\">                 4</td><td style=\"text-align: right;\">       785.047</td><td style=\"text-align: right;\">0.00204532 </td><td style=\"text-align: right;\">42.2658     </td><td style=\"text-align: right;\">       0.4 </td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bb0d9a8e</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.5 </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.00376611</td><td style=\"text-align: right;\">          0</td><td style=\"text-align: right;\">                 4</td><td style=\"text-align: right;\">      3546.25 </td><td style=\"text-align: right;\">0.116935   </td><td style=\"text-align: right;\"> 0.000130113</td><td style=\"text-align: right;\">       0.75</td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bb12cc0c</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.65</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.0221297 </td><td style=\"text-align: right;\">          5</td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">       198.946</td><td style=\"text-align: right;\">0.000361749</td><td style=\"text-align: right;\"> 0.558593   </td><td style=\"text-align: right;\">       0.65</td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bb17fa24</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.8 </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.00642388</td><td style=\"text-align: right;\">          0</td><td style=\"text-align: right;\">                 1</td><td style=\"text-align: right;\">       148.264</td><td style=\"text-align: right;\">0.000141496</td><td style=\"text-align: right;\"> 0.420565   </td><td style=\"text-align: right;\">       0.7 </td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bb1cb1ae</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.75</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.0600033 </td><td style=\"text-align: right;\">          4</td><td style=\"text-align: right;\">                 0</td><td style=\"text-align: right;\">      1193.5  </td><td style=\"text-align: right;\">0.0473921  </td><td style=\"text-align: right;\"> 0.0627479  </td><td style=\"text-align: right;\">       0.6 </td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bb21a97a</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.55</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.0596483 </td><td style=\"text-align: right;\">          2</td><td style=\"text-align: right;\">                 3</td><td style=\"text-align: right;\">      2072.44 </td><td style=\"text-align: right;\">0.145667   </td><td style=\"text-align: right;\"> 1.02135    </td><td style=\"text-align: right;\">       0.55</td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bb26ab64</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.05</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.00986594</td><td style=\"text-align: right;\">          1</td><td style=\"text-align: right;\">                 4</td><td style=\"text-align: right;\">       668.603</td><td style=\"text-align: right;\">0.0136262  </td><td style=\"text-align: right;\"> 6.39466    </td><td style=\"text-align: right;\">       0.65</td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bb2bca54</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.3 </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.014299  </td><td style=\"text-align: right;\">          0</td><td style=\"text-align: right;\">                 4</td><td style=\"text-align: right;\">      5551.71 </td><td style=\"text-align: right;\">0.484234   </td><td style=\"text-align: right;\"> 1.99934    </td><td style=\"text-align: right;\">       0.8 </td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bb30c9aa</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.75</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.0280841 </td><td style=\"text-align: right;\">          3</td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">       296.721</td><td style=\"text-align: right;\">0.00027662 </td><td style=\"text-align: right;\">22.5591     </td><td style=\"text-align: right;\">       0.45</td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bb36190a</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.8 </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.0335324 </td><td style=\"text-align: right;\">          5</td><td style=\"text-align: right;\">                 1</td><td style=\"text-align: right;\">       459.256</td><td style=\"text-align: right;\">0.00105738 </td><td style=\"text-align: right;\"> 0.896258   </td><td style=\"text-align: right;\">       0.7 </td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bb3b4e8e</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.65</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.00301412</td><td style=\"text-align: right;\">          4</td><td style=\"text-align: right;\">                 0</td><td style=\"text-align: right;\">       902.92 </td><td style=\"text-align: right;\">0.00457859 </td><td style=\"text-align: right;\"> 0.00397137 </td><td style=\"text-align: right;\">       0.55</td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bb406194</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.4 </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.00130317</td><td style=\"text-align: right;\">          4</td><td style=\"text-align: right;\">                 0</td><td style=\"text-align: right;\">      3483.79 </td><td style=\"text-align: right;\">0.00283949 </td><td style=\"text-align: right;\"> 0.000334701</td><td style=\"text-align: right;\">       0.65</td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bb45a0f0</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.15</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.00806527</td><td style=\"text-align: right;\">          0</td><td style=\"text-align: right;\">                 4</td><td style=\"text-align: right;\">       143.907</td><td style=\"text-align: right;\">0.00331113 </td><td style=\"text-align: right;\"> 9.66671    </td><td style=\"text-align: right;\">       0.6 </td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bb4acc9c</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.25</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.00535749</td><td style=\"text-align: right;\">          2</td><td style=\"text-align: right;\">                 4</td><td style=\"text-align: right;\">       355.002</td><td style=\"text-align: right;\">0.000585565</td><td style=\"text-align: right;\">24.7877     </td><td style=\"text-align: right;\">       0.6 </td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bb4ff6fe</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.6 </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.0201873 </td><td style=\"text-align: right;\">          1</td><td style=\"text-align: right;\">                 1</td><td style=\"text-align: right;\">       294.841</td><td style=\"text-align: right;\">0.00189038 </td><td style=\"text-align: right;\"> 0.119983   </td><td style=\"text-align: right;\">       0.85</td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bb55e78a</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.75</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.0272728 </td><td style=\"text-align: right;\">          1</td><td style=\"text-align: right;\">                 3</td><td style=\"text-align: right;\">       263.28 </td><td style=\"text-align: right;\">0.000302492</td><td style=\"text-align: right;\"> 0.0232296  </td><td style=\"text-align: right;\">       0.45</td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bb5ba62a</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.6 </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.00185362</td><td style=\"text-align: right;\">          4</td><td style=\"text-align: right;\">                 0</td><td style=\"text-align: right;\">       487.953</td><td style=\"text-align: right;\">0.825219   </td><td style=\"text-align: right;\"> 0.00249954 </td><td style=\"text-align: right;\">       0.75</td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bb610160</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.65</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.00244965</td><td style=\"text-align: right;\">          4</td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">       609.359</td><td style=\"text-align: right;\">0.228839   </td><td style=\"text-align: right;\"> 0.000243096</td><td style=\"text-align: right;\">       0.8 </td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bb664738</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.3 </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.00907531</td><td style=\"text-align: right;\">          3</td><td style=\"text-align: right;\">                 4</td><td style=\"text-align: right;\">       118.841</td><td style=\"text-align: right;\">0.000825632</td><td style=\"text-align: right;\">11.9185     </td><td style=\"text-align: right;\">       0.5 </td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bb6bc460</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.35</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.0130503 </td><td style=\"text-align: right;\">          2</td><td style=\"text-align: right;\">                 2</td><td style=\"text-align: right;\">       220.641</td><td style=\"text-align: right;\">0.00648234 </td><td style=\"text-align: right;\">12.1335     </td><td style=\"text-align: right;\">       0.7 </td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bb714ac0</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.35</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.0534217 </td><td style=\"text-align: right;\">          2</td><td style=\"text-align: right;\">                 2</td><td style=\"text-align: right;\">       391.233</td><td style=\"text-align: right;\">0.000162423</td><td style=\"text-align: right;\"> 2.92006    </td><td style=\"text-align: right;\">       0.7 </td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bb769124</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.45</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.0439363 </td><td style=\"text-align: right;\">          2</td><td style=\"text-align: right;\">                 2</td><td style=\"text-align: right;\">       342.198</td><td style=\"text-align: right;\">0.00011005 </td><td style=\"text-align: right;\"> 3.77572    </td><td style=\"text-align: right;\">       0.65</td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bb7be7a0</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.55</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.0940199 </td><td style=\"text-align: right;\">          0</td><td style=\"text-align: right;\">                 1</td><td style=\"text-align: right;\">       589.161</td><td style=\"text-align: right;\">0.000440528</td><td style=\"text-align: right;\"> 0.0215912  </td><td style=\"text-align: right;\">       0.5 </td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bb81314c</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.5 </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.0974159 </td><td style=\"text-align: right;\">          0</td><td style=\"text-align: right;\">                 1</td><td style=\"text-align: right;\">       427.393</td><td style=\"text-align: right;\">0.000468674</td><td style=\"text-align: right;\"> 0.00815797 </td><td style=\"text-align: right;\">       0.65</td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bb8686ba</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.7 </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.0735381 </td><td style=\"text-align: right;\">          1</td><td style=\"text-align: right;\">                 3</td><td style=\"text-align: right;\">      9156.5  </td><td style=\"text-align: right;\">0.0205803  </td><td style=\"text-align: right;\"> 0.0013701  </td><td style=\"text-align: right;\">       0.75</td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bb8ba406</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.8 </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.00606424</td><td style=\"text-align: right;\">          0</td><td style=\"text-align: right;\">                 0</td><td style=\"text-align: right;\">      1407.67 </td><td style=\"text-align: right;\">0.0475207  </td><td style=\"text-align: right;\"> 0.0582149  </td><td style=\"text-align: right;\">       0.6 </td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bb91299e</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.55</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.0702093 </td><td style=\"text-align: right;\">          2</td><td style=\"text-align: right;\">                 3</td><td style=\"text-align: right;\">      1925.02 </td><td style=\"text-align: right;\">0.12652    </td><td style=\"text-align: right;\"> 0.204715   </td><td style=\"text-align: right;\">       0.55</td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bb965aa4</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.75</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.059416  </td><td style=\"text-align: right;\">          2</td><td style=\"text-align: right;\">                 3</td><td style=\"text-align: right;\">      1143.77 </td><td style=\"text-align: right;\">0.172612   </td><td style=\"text-align: right;\"> 0.878922   </td><td style=\"text-align: right;\">       0.55</td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bb9b7322</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.15</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.0094878 </td><td style=\"text-align: right;\">          1</td><td style=\"text-align: right;\">                 3</td><td style=\"text-align: right;\">      7221.4  </td><td style=\"text-align: right;\">0.0130072  </td><td style=\"text-align: right;\"> 1.56131    </td><td style=\"text-align: right;\">       0.65</td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bba09096</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.05</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.00706042</td><td style=\"text-align: right;\">          1</td><td style=\"text-align: right;\">                 4</td><td style=\"text-align: right;\">      5468.9  </td><td style=\"text-align: right;\">0.0727165  </td><td style=\"text-align: right;\"> 2.17253    </td><td style=\"text-align: right;\">       0.8 </td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bba5b0e4</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.2 </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.0127514 </td><td style=\"text-align: right;\">          3</td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">      2689.08 </td><td style=\"text-align: right;\">0.620043   </td><td style=\"text-align: right;\">21.5729     </td><td style=\"text-align: right;\">       0.85</td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bbab12fa</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.4 </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.0158162 </td><td style=\"text-align: right;\">          3</td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">       906.767</td><td style=\"text-align: right;\">0.436345   </td><td style=\"text-align: right;\">60.5041     </td><td style=\"text-align: right;\">       0.9 </td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bbb0a2ec</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.8 </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.0354128 </td><td style=\"text-align: right;\">          5</td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">       459.383</td><td style=\"text-align: right;\">0.0011889  </td><td style=\"text-align: right;\"> 4.76826    </td><td style=\"text-align: right;\">       0.7 </td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bbb5ea5e</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.7 </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.00391599</td><td style=\"text-align: right;\">          5</td><td style=\"text-align: right;\">                 0</td><td style=\"text-align: right;\">       890.571</td><td style=\"text-align: right;\">0.00110193 </td><td style=\"text-align: right;\"> 0.151169   </td><td style=\"text-align: right;\">       0.6 </td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bbbb0e08</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.4 </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.00123429</td><td style=\"text-align: right;\">          4</td><td style=\"text-align: right;\">                 0</td><td style=\"text-align: right;\">      3735.77 </td><td style=\"text-align: right;\">0.00268057 </td><td style=\"text-align: right;\"> 0.00447743 </td><td style=\"text-align: right;\">       0.65</td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bbc080c2</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.45</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.00300393</td><td style=\"text-align: right;\">          4</td><td style=\"text-align: right;\">                 0</td><td style=\"text-align: right;\">      3368.83 </td><td style=\"text-align: right;\">0.0108682  </td><td style=\"text-align: right;\"> 0.000205613</td><td style=\"text-align: right;\">       0.55</td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bbc6001a</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.15</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.00165199</td><td style=\"text-align: right;\">          0</td><td style=\"text-align: right;\">                 4</td><td style=\"text-align: right;\">      3002.91 </td><td style=\"text-align: right;\">0.00304467 </td><td style=\"text-align: right;\"> 8.28528    </td><td style=\"text-align: right;\">       0.6 </td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bbcbca86</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.25</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.00489767</td><td style=\"text-align: right;\">          2</td><td style=\"text-align: right;\">                 4</td><td style=\"text-align: right;\">       345.19 </td><td style=\"text-align: right;\">0.000631698</td><td style=\"text-align: right;\">32.0856     </td><td style=\"text-align: right;\">       0.6 </td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bbd1880e</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.6 </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.0200743 </td><td style=\"text-align: right;\">          1</td><td style=\"text-align: right;\">                 4</td><td style=\"text-align: right;\">       268.313</td><td style=\"text-align: right;\">0.00173855 </td><td style=\"text-align: right;\"> 0.408758   </td><td style=\"text-align: right;\">       0.85</td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bbd7428a</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.1 </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.00432785</td><td style=\"text-align: right;\">          1</td><td style=\"text-align: right;\">                 1</td><td style=\"text-align: right;\">       535.782</td><td style=\"text-align: right;\">0.0022526  </td><td style=\"text-align: right;\">49.9409     </td><td style=\"text-align: right;\">       0.9 </td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bbdca1da</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.6 </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.024929  </td><td style=\"text-align: right;\">          1</td><td style=\"text-align: right;\">                 3</td><td style=\"text-align: right;\">       171.885</td><td style=\"text-align: right;\">0.000249744</td><td style=\"text-align: right;\"> 0.0171642  </td><td style=\"text-align: right;\">       0.45</td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bbe1ddd0</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.65</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.00211124</td><td style=\"text-align: right;\">          4</td><td style=\"text-align: right;\">                 3</td><td style=\"text-align: right;\">       708.022</td><td style=\"text-align: right;\">0.000339325</td><td style=\"text-align: right;\"> 0.00179439 </td><td style=\"text-align: right;\">       0.75</td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bbe72416</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.65</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.0025398 </td><td style=\"text-align: right;\">          4</td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">       213.471</td><td style=\"text-align: right;\">0.230677   </td><td style=\"text-align: right;\"> 0.00049454 </td><td style=\"text-align: right;\">       0.8 </td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bbecba16</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.7 </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.0016958 </td><td style=\"text-align: right;\">          4</td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">       649.901</td><td style=\"text-align: right;\">0.996682   </td><td style=\"text-align: right;\"> 0.000182507</td><td style=\"text-align: right;\">       0.75</td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bbf24846</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.3 </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.00114128</td><td style=\"text-align: right;\">          3</td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">      1569.54 </td><td style=\"text-align: right;\">0.0366514  </td><td style=\"text-align: right;\">13.7068     </td><td style=\"text-align: right;\">       0.9 </td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bbf7b812</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.3 </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.00881747</td><td style=\"text-align: right;\">          3</td><td style=\"text-align: right;\">                 2</td><td style=\"text-align: right;\">       125.019</td><td style=\"text-align: right;\">0.00722274 </td><td style=\"text-align: right;\">87.6808     </td><td style=\"text-align: right;\">       0.7 </td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bbfd06fa</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.35</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.0106787 </td><td style=\"text-align: right;\">          2</td><td style=\"text-align: right;\">                 2</td><td style=\"text-align: right;\">       221.006</td><td style=\"text-align: right;\">0.000183634</td><td style=\"text-align: right;\"> 2.56926    </td><td style=\"text-align: right;\">       0.7 </td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bc02707c</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.35</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.0483441 </td><td style=\"text-align: right;\">          2</td><td style=\"text-align: right;\">                 2</td><td style=\"text-align: right;\">       159.889</td><td style=\"text-align: right;\">0.000152991</td><td style=\"text-align: right;\"> 0.610962   </td><td style=\"text-align: right;\">       0.7 </td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bc07da1c</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.45</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.050314  </td><td style=\"text-align: right;\">          2</td><td style=\"text-align: right;\">                 2</td><td style=\"text-align: right;\">       769.023</td><td style=\"text-align: right;\">0.000113207</td><td style=\"text-align: right;\"> 1.45414    </td><td style=\"text-align: right;\">       0.65</td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bc0d87e6</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.5 </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.0839331 </td><td style=\"text-align: right;\">          0</td><td style=\"text-align: right;\">                 1</td><td style=\"text-align: right;\">       590.72 </td><td style=\"text-align: right;\">0.000480907</td><td style=\"text-align: right;\"> 3.89541    </td><td style=\"text-align: right;\">       0.65</td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bc134870</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.5 </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.0946598 </td><td style=\"text-align: right;\">          0</td><td style=\"text-align: right;\">                 1</td><td style=\"text-align: right;\">       416.27 </td><td style=\"text-align: right;\">0.00136911 </td><td style=\"text-align: right;\"> 0.0060082  </td><td style=\"text-align: right;\">       0.5 </td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bc18dab0</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.55</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.087441  </td><td style=\"text-align: right;\">          0</td><td style=\"text-align: right;\">                 1</td><td style=\"text-align: right;\">      1220.16 </td><td style=\"text-align: right;\">0.000226523</td><td style=\"text-align: right;\"> 0.0132637  </td><td style=\"text-align: right;\">       0.5 </td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bc1e982e</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.7 </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.0729463 </td><td style=\"text-align: right;\">          0</td><td style=\"text-align: right;\">                 3</td><td style=\"text-align: right;\">      9445.46 </td><td style=\"text-align: right;\">0.0043499  </td><td style=\"text-align: right;\"> 0.0010187  </td><td style=\"text-align: right;\">       0.75</td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bc246d80</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.75</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.0174898 </td><td style=\"text-align: right;\">          1</td><td style=\"text-align: right;\">                 3</td><td style=\"text-align: right;\">      7412.16 </td><td style=\"text-align: right;\">0.0200572  </td><td style=\"text-align: right;\"> 0.0576149  </td><td style=\"text-align: right;\">       0.8 </td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bc29ded2</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.8 </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.00679646</td><td style=\"text-align: right;\">          0</td><td style=\"text-align: right;\">                 0</td><td style=\"text-align: right;\">      2253.07 </td><td style=\"text-align: right;\">0.127732   </td><td style=\"text-align: right;\"> 0.229276   </td><td style=\"text-align: right;\">       0.55</td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bc2fa876</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.55</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.00591803</td><td style=\"text-align: right;\">          0</td><td style=\"text-align: right;\">                 3</td><td style=\"text-align: right;\">      1840.04 </td><td style=\"text-align: right;\">0.0800967  </td><td style=\"text-align: right;\"> 0.298097   </td><td style=\"text-align: right;\">       0.6 </td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bc35523a</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.65</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.0623471 </td><td style=\"text-align: right;\">          2</td><td style=\"text-align: right;\">                 3</td><td style=\"text-align: right;\">      4558    </td><td style=\"text-align: right;\">0.166134   </td><td style=\"text-align: right;\"> 0.841652   </td><td style=\"text-align: right;\">       0.55</td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bc3afc44</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.1 </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.0393942 </td><td style=\"text-align: right;\">          1</td><td style=\"text-align: right;\">                 3</td><td style=\"text-align: right;\">      7680.57 </td><td style=\"text-align: right;\">0.10018    </td><td style=\"text-align: right;\"> 1.66934    </td><td style=\"text-align: right;\">       0.65</td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bc40a874</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.05</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.0125142 </td><td style=\"text-align: right;\">          1</td><td style=\"text-align: right;\">                 4</td><td style=\"text-align: right;\">      6130.4  </td><td style=\"text-align: right;\">0.0241013  </td><td style=\"text-align: right;\"> 5.28142    </td><td style=\"text-align: right;\">       0.8 </td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bc4668d6</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.05</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.00717294</td><td style=\"text-align: right;\">          1</td><td style=\"text-align: right;\">                 4</td><td style=\"text-align: right;\">      5835.12 </td><td style=\"text-align: right;\">0.0351727  </td><td style=\"text-align: right;\"> 8.03348    </td><td style=\"text-align: right;\">       0.85</td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bc4c27f8</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.2 </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.0119715 </td><td style=\"text-align: right;\">          3</td><td style=\"text-align: right;\">                 4</td><td style=\"text-align: right;\">      5074.84 </td><td style=\"text-align: right;\">0.69478    </td><td style=\"text-align: right;\">37.5014     </td><td style=\"text-align: right;\">       0.85</td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bc5241d8</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.2 </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.01549   </td><td style=\"text-align: right;\">          3</td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">      1010.44 </td><td style=\"text-align: right;\">0.440607   </td><td style=\"text-align: right;\">57.4248     </td><td style=\"text-align: right;\">       0.9 </td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bc586d88</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.45</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.0317293 </td><td style=\"text-align: right;\">          5</td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">       915.627</td><td style=\"text-align: right;\">0.00107909 </td><td style=\"text-align: right;\">17.3927     </td><td style=\"text-align: right;\">       0.75</td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bc5e3682</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.4 </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.0358628 </td><td style=\"text-align: right;\">          5</td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">       464.887</td><td style=\"text-align: right;\">0.000808654</td><td style=\"text-align: right;\">31.5533     </td><td style=\"text-align: right;\">       0.9 </td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bc648faa</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.75</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.00352813</td><td style=\"text-align: right;\">          5</td><td style=\"text-align: right;\">                 0</td><td style=\"text-align: right;\">       837.651</td><td style=\"text-align: right;\">0.0012512  </td><td style=\"text-align: right;\"> 0.135928   </td><td style=\"text-align: right;\">       0.6 </td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bc6b4322</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.4 </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.00145111</td><td style=\"text-align: right;\">          5</td><td style=\"text-align: right;\">                 0</td><td style=\"text-align: right;\">      1586.25 </td><td style=\"text-align: right;\">0.00233292 </td><td style=\"text-align: right;\"> 0.151082   </td><td style=\"text-align: right;\">       0.6 </td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bc71646e</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.25</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.00305042</td><td style=\"text-align: right;\">          4</td><td style=\"text-align: right;\">                 0</td><td style=\"text-align: right;\">      3534.17 </td><td style=\"text-align: right;\">0.00909769 </td><td style=\"text-align: right;\"> 0.000116203</td><td style=\"text-align: right;\">       0.4 </td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bc7733e4</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.45</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.0018914 </td><td style=\"text-align: right;\">          4</td><td style=\"text-align: right;\">                 0</td><td style=\"text-align: right;\">      3921.97 </td><td style=\"text-align: right;\">0.0111937  </td><td style=\"text-align: right;\"> 0.000846597</td><td style=\"text-align: right;\">       0.5 </td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bc7d2420</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.15</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.00229984</td><td style=\"text-align: right;\">          0</td><td style=\"text-align: right;\">                 4</td><td style=\"text-align: right;\">      3241.66 </td><td style=\"text-align: right;\">0.00342105 </td><td style=\"text-align: right;\"> 9.27311    </td><td style=\"text-align: right;\">       0.55</td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bc838540</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.1 </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.00512811</td><td style=\"text-align: right;\">          2</td><td style=\"text-align: right;\">                 4</td><td style=\"text-align: right;\">       325.834</td><td style=\"text-align: right;\">0.000621322</td><td style=\"text-align: right;\"> 7.14416    </td><td style=\"text-align: right;\">       0.6 </td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bc893116</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.6 </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.0212219 </td><td style=\"text-align: right;\">          2</td><td style=\"text-align: right;\">                 4</td><td style=\"text-align: right;\">       286.188</td><td style=\"text-align: right;\">0.00155348 </td><td style=\"text-align: right;\"> 0.086478   </td><td style=\"text-align: right;\">       0.45</td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bc8f2a94</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.25</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.0138863 </td><td style=\"text-align: right;\">          1</td><td style=\"text-align: right;\">                 4</td><td style=\"text-align: right;\">       104.837</td><td style=\"text-align: right;\">0.00188033 </td><td style=\"text-align: right;\">26.5879     </td><td style=\"text-align: right;\">       0.85</td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bc9525de</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.6 </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.00428342</td><td style=\"text-align: right;\">          1</td><td style=\"text-align: right;\">                 1</td><td style=\"text-align: right;\">       531.867</td><td style=\"text-align: right;\">0.00754193 </td><td style=\"text-align: right;\"> 0.446463   </td><td style=\"text-align: right;\">       0.9 </td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bc9b6660</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.65</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.0233802 </td><td style=\"text-align: right;\">          1</td><td style=\"text-align: right;\">                 1</td><td style=\"text-align: right;\">       179.348</td><td style=\"text-align: right;\">0.000229353</td><td style=\"text-align: right;\"> 0.0252992  </td><td style=\"text-align: right;\">       0.45</td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bca1cabe</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.6 </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.0258273 </td><td style=\"text-align: right;\">          4</td><td style=\"text-align: right;\">                 3</td><td style=\"text-align: right;\">       680.819</td><td style=\"text-align: right;\">0.000343165</td><td style=\"text-align: right;\"> 0.00143765 </td><td style=\"text-align: right;\">       0.75</td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bca8060e</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.65</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.0175148 </td><td style=\"text-align: right;\">          4</td><td style=\"text-align: right;\">                 3</td><td style=\"text-align: right;\">       712.204</td><td style=\"text-align: right;\">0.000100716</td><td style=\"text-align: right;\"> 0.00284025 </td><td style=\"text-align: right;\">       0.7 </td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bcae77aa</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.7 </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.00206959</td><td style=\"text-align: right;\">          4</td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">       203.008</td><td style=\"text-align: right;\">0.331362   </td><td style=\"text-align: right;\"> 0.000546509</td><td style=\"text-align: right;\">       0.8 </td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bcb4bb56</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.7 </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.00100926</td><td style=\"text-align: right;\">          4</td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">       639.089</td><td style=\"text-align: right;\">0.274691   </td><td style=\"text-align: right;\"> 0.000180678</td><td style=\"text-align: right;\">       0.8 </td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bcbad1a8</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.3 </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.00117837</td><td style=\"text-align: right;\">          3</td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">      2511.86 </td><td style=\"text-align: right;\">0.0399989  </td><td style=\"text-align: right;\">14.3945     </td><td style=\"text-align: right;\">       0.75</td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bcc134e4</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.35</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.00110078</td><td style=\"text-align: right;\">          3</td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">      1602.26 </td><td style=\"text-align: right;\">0.0166464  </td><td style=\"text-align: right;\"> 1.25233    </td><td style=\"text-align: right;\">       0.85</td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bcc77f20</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.3 </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.00351136</td><td style=\"text-align: right;\">          3</td><td style=\"text-align: right;\">                 2</td><td style=\"text-align: right;\">      1325.86 </td><td style=\"text-align: right;\">0.00538391 </td><td style=\"text-align: right;\">46.2637     </td><td style=\"text-align: right;\">       0.7 </td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "<tr><td>my_xgb_bccd8622</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">              0.35</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.00891576</td><td style=\"text-align: right;\">          3</td><td style=\"text-align: right;\">                 2</td><td style=\"text-align: right;\">       239.828</td><td style=\"text-align: right;\">0.000173346</td><td style=\"text-align: right;\">99.6655     </td><td style=\"text-align: right;\">       0.65</td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 128<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name     </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                                                                                                                            </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>my_xgb_ba57a242</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_ba57a242_1_colsample_bytree=0.7,gamma=0,learning_rate=0.010539,max_depth=0,min_child_weight=4,n_estimators=435.41,reg_alpha_2020-10-15_03-58-52/error.txt</td></tr>\n",
       "<tr><td>my_xgb_ba5c05c6</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_ba5c05c6_2_colsample_bytree=0.3,gamma=0,learning_rate=0.0067334,max_depth=3,min_child_weight=2,n_estimators=428.15,reg_alph_2020-10-15_03-58-52/error.txt</td></tr>\n",
       "<tr><td>my_xgb_ba5fd782</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_ba5fd782_3_colsample_bytree=0.15,gamma=0,learning_rate=0.0013324,max_depth=5,min_child_weight=4,n_estimators=7988.2,reg_alp_2020-10-15_03-58-52/error.txt</td></tr>\n",
       "<tr><td>my_xgb_ba6372f2</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_ba6372f2_4_colsample_bytree=0.2,gamma=0,learning_rate=0.018766,max_depth=2,min_child_weight=3,n_estimators=735.64,reg_alpha_2020-10-15_03-58-52/error.txt</td></tr>\n",
       "<tr><td>my_xgb_ba67813a</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_ba67813a_5_colsample_bytree=0.6,gamma=0,learning_rate=0.0015521,max_depth=0,min_child_weight=3,n_estimators=4032.1,reg_alph_2020-10-15_03-58-52/error.txt</td></tr>\n",
       "<tr><td>my_xgb_ba6b9298</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_ba6b9298_6_colsample_bytree=0.15,gamma=0,learning_rate=0.0010661,max_depth=1,min_child_weight=5,n_estimators=2700.4,reg_alp_2020-10-15_03-58-52/error.txt</td></tr>\n",
       "<tr><td>my_xgb_ba6fb062</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_ba6fb062_7_colsample_bytree=0.4,gamma=0,learning_rate=0.0055611,max_depth=3,min_child_weight=2,n_estimators=238.15,reg_alph_2020-10-15_03-58-52/error.txt</td></tr>\n",
       "<tr><td>my_xgb_ba717956</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_ba717956_8_colsample_bytree=0.55,gamma=0,learning_rate=0.015461,max_depth=0,min_child_weight=1,n_estimators=5606.0,reg_alph_2020-10-15_03-58-52/error.txt</td></tr>\n",
       "<tr><td>my_xgb_ba75293e</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_ba75293e_9_colsample_bytree=0.5,gamma=0,learning_rate=0.0012655,max_depth=5,min_child_weight=3,n_estimators=4734.9,reg_alph_2020-10-15_03-58-52/error.txt</td></tr>\n",
       "<tr><td>my_xgb_ba783f0c</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_ba783f0c_10_colsample_bytree=0.75,gamma=0,learning_rate=0.0015369,max_depth=2,min_child_weight=2,n_estimators=678.64,reg_al_2020-10-15_03-58-52/error.txt</td></tr>\n",
       "<tr><td>my_xgb_ba7baa3e</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_ba7baa3e_11_colsample_bytree=0.3,gamma=0,learning_rate=0.0035435,max_depth=4,min_child_weight=4,n_estimators=322.93,reg_alp_2020-10-15_03-58-52/error.txt</td></tr>\n",
       "<tr><td>my_xgb_ba7f7542</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_ba7f7542_12_colsample_bytree=0.55,gamma=0,learning_rate=0.027463,max_depth=1,min_child_weight=4,n_estimators=1059.5,reg_alp_2020-10-15_03-58-52/error.txt</td></tr>\n",
       "<tr><td>my_xgb_ba815312</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_ba815312_13_colsample_bytree=0.35,gamma=0,learning_rate=0.0059175,max_depth=2,min_child_weight=5,n_estimators=413.97,reg_al_2020-10-15_03-58-52/error.txt</td></tr>\n",
       "<tr><td>my_xgb_ba8324e4</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_ba8324e4_14_colsample_bytree=0.1,gamma=0,learning_rate=0.0022284,max_depth=5,min_child_weight=0,n_estimators=562.02,reg_alp_2020-10-15_03-58-52/error.txt</td></tr>\n",
       "<tr><td>my_xgb_ba84e306</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_ba84e306_15_colsample_bytree=0.75,gamma=0,learning_rate=0.0015667,max_depth=5,min_child_weight=4,n_estimators=968.57,reg_al_2020-10-15_03-58-52/error.txt</td></tr>\n",
       "<tr><td>my_xgb_ba86beb0</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_ba86beb0_16_colsample_bytree=0.1,gamma=0,learning_rate=0.0045895,max_depth=5,min_child_weight=0,n_estimators=2942.7,reg_alp_2020-10-15_03-58-52/error.txt</td></tr>\n",
       "<tr><td>my_xgb_ba888100</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_ba888100_17_colsample_bytree=0.6,gamma=0,learning_rate=0.0043339,max_depth=3,min_child_weight=5,n_estimators=6339.4,reg_alp_2020-10-15_03-58-53/error.txt</td></tr>\n",
       "<tr><td>my_xgb_ba8a3176</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_ba8a3176_18_colsample_bytree=0.1,gamma=0,learning_rate=0.023078,max_depth=2,min_child_weight=3,n_estimators=4374.4,reg_alph_2020-10-15_03-58-53/error.txt</td></tr>\n",
       "<tr><td>my_xgb_ba8bd56c</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_ba8bd56c_19_colsample_bytree=0.65,gamma=0,learning_rate=0.081506,max_depth=4,min_child_weight=0,n_estimators=2561.9,reg_alp_2020-10-15_03-58-53/error.txt</td></tr>\n",
       "<tr><td>my_xgb_ba8d753e</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_ba8d753e_20_colsample_bytree=0.35,gamma=0,learning_rate=0.0082137,max_depth=2,min_child_weight=2,n_estimators=110.35,reg_al_2020-10-15_03-58-53/error.txt</td></tr>\n",
       "<tr><td>my_xgb_ba8f1c86</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_ba8f1c86_21_colsample_bytree=0.8,gamma=0,learning_rate=0.041691,max_depth=0,min_child_weight=1,n_estimators=165.49,reg_alph_2020-10-15_03-58-53/error.txt</td></tr>\n",
       "<tr><td>my_xgb_ba93a436</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_ba93a436_22_colsample_bytree=0.7,gamma=0,learning_rate=0.080489,max_depth=4,min_child_weight=0,n_estimators=1742.5,reg_alph_2020-10-15_03-58-53/error.txt</td></tr>\n",
       "<tr><td>my_xgb_ba97d54c</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_ba97d54c_23_colsample_bytree=0.45,gamma=0,learning_rate=0.011299,max_depth=0,min_child_weight=2,n_estimators=112.11,reg_alp_2020-10-15_03-58-53/error.txt</td></tr>\n",
       "<tr><td>my_xgb_ba9c3650</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_ba9c3650_24_colsample_bytree=0.8,gamma=0,learning_rate=0.040215,max_depth=0,min_child_weight=1,n_estimators=181.69,reg_alph_2020-10-15_03-58-53/error.txt</td></tr>\n",
       "<tr><td>my_xgb_baa0c3d2</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_baa0c3d2_25_colsample_bytree=0.7,gamma=0,learning_rate=0.069263,max_depth=4,min_child_weight=0,n_estimators=1594.4,reg_alph_2020-10-15_03-58-53/error.txt</td></tr>\n",
       "<tr><td>my_xgb_baa52e40</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_baa52e40_26_colsample_bytree=0.5,gamma=0,learning_rate=0.013006,max_depth=0,min_child_weight=4,n_estimators=106.5,reg_alpha_2020-10-15_03-58-54/error.txt</td></tr>\n",
       "<tr><td>my_xgb_baa9afb0</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_baa9afb0_27_colsample_bytree=0.8,gamma=0,learning_rate=0.043037,max_depth=0,min_child_weight=1,n_estimators=197.39,reg_alph_2020-10-15_03-58-54/error.txt</td></tr>\n",
       "<tr><td>my_xgb_baae3e0e</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_baae3e0e_28_colsample_bytree=0.7,gamma=0,learning_rate=0.059391,max_depth=4,min_child_weight=0,n_estimators=1390.1,reg_alph_2020-10-15_03-58-54/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bab2a688</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bab2a688_29_colsample_bytree=0.45,gamma=0,learning_rate=0.010992,max_depth=0,min_child_weight=4,n_estimators=100.3,reg_alph_2020-10-15_03-58-54/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bab76cd6</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bab76cd6_30_colsample_bytree=0.8,gamma=0,learning_rate=0.035575,max_depth=0,min_child_weight=1,n_estimators=301.71,reg_alph_2020-10-15_03-58-54/error.txt</td></tr>\n",
       "<tr><td>my_xgb_babbf8aa</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_babbf8aa_31_colsample_bytree=0.65,gamma=0,learning_rate=0.0030624,max_depth=4,min_child_weight=0,n_estimators=1396.1,reg_al_2020-10-15_03-58-54/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bac08ca8</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bac08ca8_32_colsample_bytree=0.25,gamma=0,learning_rate=0.0076633,max_depth=0,min_child_weight=4,n_estimators=140.01,reg_al_2020-10-15_03-58-54/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bac4e820</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bac4e820_33_colsample_bytree=0.75,gamma=0,learning_rate=0.030737,max_depth=1,min_child_weight=1,n_estimators=298.1,reg_alph_2020-10-15_03-58-54/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bac96cd8</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bac96cd8_34_colsample_bytree=0.65,gamma=0,learning_rate=0.0026718,max_depth=4,min_child_weight=0,n_estimators=486.39,reg_al_2020-10-15_03-58-54/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bacdfdfc</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bacdfdfc_35_colsample_bytree=0.25,gamma=0,learning_rate=0.0081914,max_depth=3,min_child_weight=4,n_estimators=129.3,reg_alp_2020-10-15_03-58-55/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bad25bae</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bad25bae_36_colsample_bytree=0.35,gamma=0,learning_rate=0.018695,max_depth=2,min_child_weight=2,n_estimators=239.76,reg_alp_2020-10-15_03-58-55/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bad6f3c6</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bad6f3c6_37_colsample_bytree=0.6,gamma=0,learning_rate=0.050706,max_depth=0,min_child_weight=1,n_estimators=387.87,reg_alph_2020-10-15_03-58-55/error.txt</td></tr>\n",
       "<tr><td>my_xgb_badb8fee</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_badb8fee_38_colsample_bytree=0.55,gamma=0,learning_rate=0.098178,max_depth=0,min_child_weight=1,n_estimators=592.74,reg_alp_2020-10-15_03-58-55/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bae0772a</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bae0772a_39_colsample_bytree=0.7,gamma=0,learning_rate=0.092391,max_depth=1,min_child_weight=3,n_estimators=8696.8,reg_alph_2020-10-15_03-58-55/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bae589ea</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bae589ea_40_colsample_bytree=0.5,gamma=0,learning_rate=0.017054,max_depth=4,min_child_weight=5,n_estimators=807.6,reg_alpha_2020-10-15_03-58-55/error.txt</td></tr>\n",
       "<tr><td>my_xgb_baea7734</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_baea7734_41_colsample_bytree=0.45,gamma=0,learning_rate=0.011382,max_depth=3,min_child_weight=2,n_estimators=249.1,reg_alph_2020-10-15_03-58-55/error.txt</td></tr>\n",
       "<tr><td>my_xgb_baef8b20</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_baef8b20_42_colsample_bytree=0.4,gamma=0,learning_rate=0.022803,max_depth=0,min_child_weight=2,n_estimators=373.72,reg_alph_2020-10-15_03-58-55/error.txt</td></tr>\n",
       "<tr><td>my_xgb_baf49372</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_baf49372_43_colsample_bytree=0.75,gamma=0,learning_rate=0.015508,max_depth=0,min_child_weight=4,n_estimators=189.7,reg_alph_2020-10-15_03-58-55/error.txt</td></tr>\n",
       "<tr><td>my_xgb_baf99e6c</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_baf99e6c_44_colsample_bytree=0.6,gamma=0,learning_rate=0.027134,max_depth=1,min_child_weight=1,n_estimators=509.94,reg_alph_2020-10-15_03-58-55/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bafed760</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bafed760_45_colsample_bytree=0.7,gamma=0,learning_rate=0.0020584,max_depth=4,min_child_weight=3,n_estimators=2156.6,reg_alp_2020-10-15_03-58-56/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bb0376bc</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bb0376bc_46_colsample_bytree=0.55,gamma=0,learning_rate=0.0010343,max_depth=5,min_child_weight=0,n_estimators=1067.6,reg_al_2020-10-15_03-58-56/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bb08a484</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bb08a484_47_colsample_bytree=0.2,gamma=0,learning_rate=0.00504,max_depth=3,min_child_weight=4,n_estimators=785.05,reg_alpha_2020-10-15_03-58-56/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bb0d9a8e</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bb0d9a8e_48_colsample_bytree=0.5,gamma=0,learning_rate=0.0037661,max_depth=0,min_child_weight=4,n_estimators=3546.3,reg_alp_2020-10-15_03-58-56/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bb12cc0c</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bb12cc0c_49_colsample_bytree=0.65,gamma=0,learning_rate=0.02213,max_depth=5,min_child_weight=5,n_estimators=198.95,reg_alph_2020-10-15_03-58-56/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bb17fa24</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bb17fa24_50_colsample_bytree=0.8,gamma=0,learning_rate=0.0064239,max_depth=0,min_child_weight=1,n_estimators=148.26,reg_alp_2020-10-15_03-58-56/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bb1cb1ae</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bb1cb1ae_51_colsample_bytree=0.75,gamma=0,learning_rate=0.060003,max_depth=4,min_child_weight=0,n_estimators=1193.5,reg_alp_2020-10-15_03-58-56/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bb21a97a</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bb21a97a_52_colsample_bytree=0.55,gamma=0,learning_rate=0.059648,max_depth=2,min_child_weight=3,n_estimators=2072.4,reg_alp_2020-10-15_03-58-56/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bb26ab64</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bb26ab64_53_colsample_bytree=0.05,gamma=0,learning_rate=0.0098659,max_depth=1,min_child_weight=4,n_estimators=668.6,reg_alp_2020-10-15_03-58-56/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bb2bca54</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bb2bca54_54_colsample_bytree=0.3,gamma=0,learning_rate=0.014299,max_depth=0,min_child_weight=4,n_estimators=5551.7,reg_alph_2020-10-15_03-58-56/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bb30c9aa</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bb30c9aa_55_colsample_bytree=0.75,gamma=0,learning_rate=0.028084,max_depth=3,min_child_weight=5,n_estimators=296.72,reg_alp_2020-10-15_03-58-57/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bb36190a</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bb36190a_56_colsample_bytree=0.8,gamma=0,learning_rate=0.033532,max_depth=5,min_child_weight=1,n_estimators=459.26,reg_alph_2020-10-15_03-58-57/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bb3b4e8e</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bb3b4e8e_57_colsample_bytree=0.65,gamma=0,learning_rate=0.0030141,max_depth=4,min_child_weight=0,n_estimators=902.92,reg_al_2020-10-15_03-58-57/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bb406194</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bb406194_58_colsample_bytree=0.4,gamma=0,learning_rate=0.0013032,max_depth=4,min_child_weight=0,n_estimators=3483.8,reg_alp_2020-10-15_03-58-57/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bb45a0f0</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bb45a0f0_59_colsample_bytree=0.15,gamma=0,learning_rate=0.0080653,max_depth=0,min_child_weight=4,n_estimators=143.91,reg_al_2020-10-15_03-58-57/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bb4acc9c</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bb4acc9c_60_colsample_bytree=0.25,gamma=0,learning_rate=0.0053575,max_depth=2,min_child_weight=4,n_estimators=355.0,reg_alp_2020-10-15_03-58-57/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bb4ff6fe</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bb4ff6fe_61_colsample_bytree=0.6,gamma=0,learning_rate=0.020187,max_depth=1,min_child_weight=1,n_estimators=294.84,reg_alph_2020-10-15_03-58-57/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bb55e78a</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bb55e78a_62_colsample_bytree=0.75,gamma=0,learning_rate=0.027273,max_depth=1,min_child_weight=3,n_estimators=263.28,reg_alp_2020-10-15_03-58-57/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bb5ba62a</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bb5ba62a_63_colsample_bytree=0.6,gamma=0,learning_rate=0.0018536,max_depth=4,min_child_weight=0,n_estimators=487.95,reg_alp_2020-10-15_03-58-57/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bb610160</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bb610160_64_colsample_bytree=0.65,gamma=0,learning_rate=0.0024497,max_depth=4,min_child_weight=5,n_estimators=609.36,reg_al_2020-10-15_03-58-57/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bb664738</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bb664738_65_colsample_bytree=0.3,gamma=0,learning_rate=0.0090753,max_depth=3,min_child_weight=4,n_estimators=118.84,reg_alp_2020-10-15_03-58-57/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bb6bc460</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bb6bc460_66_colsample_bytree=0.35,gamma=0,learning_rate=0.01305,max_depth=2,min_child_weight=2,n_estimators=220.64,reg_alph_2020-10-15_03-58-57/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bb714ac0</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bb714ac0_67_colsample_bytree=0.35,gamma=0,learning_rate=0.053422,max_depth=2,min_child_weight=2,n_estimators=391.23,reg_alp_2020-10-15_03-58-58/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bb769124</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bb769124_68_colsample_bytree=0.45,gamma=0,learning_rate=0.043936,max_depth=2,min_child_weight=2,n_estimators=342.2,reg_alph_2020-10-15_03-58-58/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bb7be7a0</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bb7be7a0_69_colsample_bytree=0.55,gamma=0,learning_rate=0.09402,max_depth=0,min_child_weight=1,n_estimators=589.16,reg_alph_2020-10-15_03-58-58/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bb81314c</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bb81314c_70_colsample_bytree=0.5,gamma=0,learning_rate=0.097416,max_depth=0,min_child_weight=1,n_estimators=427.39,reg_alph_2020-10-15_03-58-58/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bb8686ba</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bb8686ba_71_colsample_bytree=0.7,gamma=0,learning_rate=0.073538,max_depth=1,min_child_weight=3,n_estimators=9156.5,reg_alph_2020-10-15_03-58-58/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bb8ba406</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bb8ba406_72_colsample_bytree=0.8,gamma=0,learning_rate=0.0060642,max_depth=0,min_child_weight=0,n_estimators=1407.7,reg_alp_2020-10-15_03-58-58/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bb91299e</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bb91299e_73_colsample_bytree=0.55,gamma=0,learning_rate=0.070209,max_depth=2,min_child_weight=3,n_estimators=1925.0,reg_alp_2020-10-15_03-58-58/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bb965aa4</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bb965aa4_74_colsample_bytree=0.75,gamma=0,learning_rate=0.059416,max_depth=2,min_child_weight=3,n_estimators=1143.8,reg_alp_2020-10-15_03-58-58/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bb9b7322</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bb9b7322_75_colsample_bytree=0.15,gamma=0,learning_rate=0.0094878,max_depth=1,min_child_weight=3,n_estimators=7221.4,reg_al_2020-10-15_03-58-58/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bba09096</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bba09096_76_colsample_bytree=0.05,gamma=0,learning_rate=0.0070604,max_depth=1,min_child_weight=4,n_estimators=5468.9,reg_al_2020-10-15_03-58-59/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bba5b0e4</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bba5b0e4_77_colsample_bytree=0.2,gamma=0,learning_rate=0.012751,max_depth=3,min_child_weight=5,n_estimators=2689.1,reg_alph_2020-10-15_03-58-59/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bbab12fa</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bbab12fa_78_colsample_bytree=0.4,gamma=0,learning_rate=0.015816,max_depth=3,min_child_weight=5,n_estimators=906.77,reg_alph_2020-10-15_03-58-59/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bbb0a2ec</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bbb0a2ec_79_colsample_bytree=0.8,gamma=0,learning_rate=0.035413,max_depth=5,min_child_weight=5,n_estimators=459.38,reg_alph_2020-10-15_03-58-59/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bbb5ea5e</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bbb5ea5e_80_colsample_bytree=0.7,gamma=0,learning_rate=0.003916,max_depth=5,min_child_weight=0,n_estimators=890.57,reg_alph_2020-10-15_03-58-59/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bbbb0e08</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bbbb0e08_81_colsample_bytree=0.4,gamma=0,learning_rate=0.0012343,max_depth=4,min_child_weight=0,n_estimators=3735.8,reg_alp_2020-10-15_03-58-59/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bbc080c2</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bbc080c2_82_colsample_bytree=0.45,gamma=0,learning_rate=0.0030039,max_depth=4,min_child_weight=0,n_estimators=3368.8,reg_al_2020-10-15_03-58-59/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bbc6001a</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bbc6001a_83_colsample_bytree=0.15,gamma=0,learning_rate=0.001652,max_depth=0,min_child_weight=4,n_estimators=3002.9,reg_alp_2020-10-15_03-58-59/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bbcbca86</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bbcbca86_84_colsample_bytree=0.25,gamma=0,learning_rate=0.0048977,max_depth=2,min_child_weight=4,n_estimators=345.19,reg_al_2020-10-15_03-58-59/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bbd1880e</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bbd1880e_85_colsample_bytree=0.6,gamma=0,learning_rate=0.020074,max_depth=1,min_child_weight=4,n_estimators=268.31,reg_alph_2020-10-15_03-58-59/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bbd7428a</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bbd7428a_86_colsample_bytree=0.1,gamma=0,learning_rate=0.0043278,max_depth=1,min_child_weight=1,n_estimators=535.78,reg_alp_2020-10-15_03-58-59/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bbdca1da</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bbdca1da_87_colsample_bytree=0.6,gamma=0,learning_rate=0.024929,max_depth=1,min_child_weight=3,n_estimators=171.89,reg_alph_2020-10-15_03-58-59/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bbe1ddd0</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bbe1ddd0_88_colsample_bytree=0.65,gamma=0,learning_rate=0.0021112,max_depth=4,min_child_weight=3,n_estimators=708.02,reg_al_2020-10-15_03-59-00/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bbe72416</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bbe72416_89_colsample_bytree=0.65,gamma=0,learning_rate=0.0025398,max_depth=4,min_child_weight=5,n_estimators=213.47,reg_al_2020-10-15_03-59-00/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bbecba16</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bbecba16_90_colsample_bytree=0.7,gamma=0,learning_rate=0.0016958,max_depth=4,min_child_weight=5,n_estimators=649.9,reg_alph_2020-10-15_03-59-00/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bbf24846</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bbf24846_91_colsample_bytree=0.3,gamma=0,learning_rate=0.0011413,max_depth=3,min_child_weight=5,n_estimators=1569.5,reg_alp_2020-10-15_03-59-00/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bbf7b812</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bbf7b812_92_colsample_bytree=0.3,gamma=0,learning_rate=0.0088175,max_depth=3,min_child_weight=2,n_estimators=125.02,reg_alp_2020-10-15_03-59-00/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bbfd06fa</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bbfd06fa_93_colsample_bytree=0.35,gamma=0,learning_rate=0.010679,max_depth=2,min_child_weight=2,n_estimators=221.01,reg_alp_2020-10-15_03-59-00/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bc02707c</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bc02707c_94_colsample_bytree=0.35,gamma=0,learning_rate=0.048344,max_depth=2,min_child_weight=2,n_estimators=159.89,reg_alp_2020-10-15_03-59-00/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bc07da1c</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bc07da1c_95_colsample_bytree=0.45,gamma=0,learning_rate=0.050314,max_depth=2,min_child_weight=2,n_estimators=769.02,reg_alp_2020-10-15_03-59-00/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bc0d87e6</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bc0d87e6_96_colsample_bytree=0.5,gamma=0,learning_rate=0.083933,max_depth=0,min_child_weight=1,n_estimators=590.72,reg_alph_2020-10-15_03-59-00/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bc134870</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bc134870_97_colsample_bytree=0.5,gamma=0,learning_rate=0.09466,max_depth=0,min_child_weight=1,n_estimators=416.27,reg_alpha_2020-10-15_03-59-00/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bc18dab0</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bc18dab0_98_colsample_bytree=0.55,gamma=0,learning_rate=0.087441,max_depth=0,min_child_weight=1,n_estimators=1220.2,reg_alp_2020-10-15_03-59-01/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bc1e982e</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bc1e982e_99_colsample_bytree=0.7,gamma=0,learning_rate=0.072946,max_depth=0,min_child_weight=3,n_estimators=9445.5,reg_alph_2020-10-15_03-59-01/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bc246d80</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bc246d80_100_colsample_bytree=0.75,gamma=0,learning_rate=0.01749,max_depth=1,min_child_weight=3,n_estimators=7412.2,reg_alp_2020-10-15_03-59-01/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bc29ded2</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bc29ded2_101_colsample_bytree=0.8,gamma=0,learning_rate=0.0067965,max_depth=0,min_child_weight=0,n_estimators=2253.1,reg_al_2020-10-15_03-59-01/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bc2fa876</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bc2fa876_102_colsample_bytree=0.55,gamma=0,learning_rate=0.005918,max_depth=0,min_child_weight=3,n_estimators=1840.0,reg_al_2020-10-15_03-59-01/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bc35523a</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bc35523a_103_colsample_bytree=0.65,gamma=0,learning_rate=0.062347,max_depth=2,min_child_weight=3,n_estimators=4558.0,reg_al_2020-10-15_03-59-01/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bc3afc44</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bc3afc44_104_colsample_bytree=0.1,gamma=0,learning_rate=0.039394,max_depth=1,min_child_weight=3,n_estimators=7680.6,reg_alp_2020-10-15_03-59-01/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bc40a874</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bc40a874_105_colsample_bytree=0.05,gamma=0,learning_rate=0.012514,max_depth=1,min_child_weight=4,n_estimators=6130.4,reg_al_2020-10-15_03-59-01/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bc4668d6</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bc4668d6_106_colsample_bytree=0.05,gamma=0,learning_rate=0.0071729,max_depth=1,min_child_weight=4,n_estimators=5835.1,reg_a_2020-10-15_03-59-01/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bc4c27f8</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bc4c27f8_107_colsample_bytree=0.2,gamma=0,learning_rate=0.011972,max_depth=3,min_child_weight=4,n_estimators=5074.8,reg_alp_2020-10-15_03-59-02/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bc5241d8</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bc5241d8_108_colsample_bytree=0.2,gamma=0,learning_rate=0.01549,max_depth=3,min_child_weight=5,n_estimators=1010.4,reg_alph_2020-10-15_03-59-02/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bc586d88</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bc586d88_109_colsample_bytree=0.45,gamma=0,learning_rate=0.031729,max_depth=5,min_child_weight=5,n_estimators=915.63,reg_al_2020-10-15_03-59-02/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bc5e3682</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bc5e3682_110_colsample_bytree=0.4,gamma=0,learning_rate=0.035863,max_depth=5,min_child_weight=5,n_estimators=464.89,reg_alp_2020-10-15_03-59-02/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bc648faa</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bc648faa_111_colsample_bytree=0.75,gamma=0,learning_rate=0.0035281,max_depth=5,min_child_weight=0,n_estimators=837.65,reg_a_2020-10-15_03-59-02/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bc6b4322</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bc6b4322_112_colsample_bytree=0.4,gamma=0,learning_rate=0.0014511,max_depth=5,min_child_weight=0,n_estimators=1586.2,reg_al_2020-10-15_03-59-02/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bc71646e</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bc71646e_113_colsample_bytree=0.25,gamma=0,learning_rate=0.0030504,max_depth=4,min_child_weight=0,n_estimators=3534.2,reg_a_2020-10-15_03-59-02/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bc7733e4</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bc7733e4_114_colsample_bytree=0.45,gamma=0,learning_rate=0.0018914,max_depth=4,min_child_weight=0,n_estimators=3922.0,reg_a_2020-10-15_03-59-02/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bc7d2420</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bc7d2420_115_colsample_bytree=0.15,gamma=0,learning_rate=0.0022998,max_depth=0,min_child_weight=4,n_estimators=3241.7,reg_a_2020-10-15_03-59-02/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bc838540</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bc838540_116_colsample_bytree=0.1,gamma=0,learning_rate=0.0051281,max_depth=2,min_child_weight=4,n_estimators=325.83,reg_al_2020-10-15_03-59-02/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bc893116</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bc893116_117_colsample_bytree=0.6,gamma=0,learning_rate=0.021222,max_depth=2,min_child_weight=4,n_estimators=286.19,reg_alp_2020-10-15_03-59-02/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bc8f2a94</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bc8f2a94_118_colsample_bytree=0.25,gamma=0,learning_rate=0.013886,max_depth=1,min_child_weight=4,n_estimators=104.84,reg_al_2020-10-15_03-59-03/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bc9525de</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bc9525de_119_colsample_bytree=0.6,gamma=0,learning_rate=0.0042834,max_depth=1,min_child_weight=1,n_estimators=531.87,reg_al_2020-10-15_03-59-03/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bc9b6660</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bc9b6660_120_colsample_bytree=0.65,gamma=0,learning_rate=0.02338,max_depth=1,min_child_weight=1,n_estimators=179.35,reg_alp_2020-10-15_03-59-03/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bca1cabe</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bca1cabe_121_colsample_bytree=0.6,gamma=0,learning_rate=0.025827,max_depth=4,min_child_weight=3,n_estimators=680.82,reg_alp_2020-10-15_03-59-03/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bca8060e</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bca8060e_122_colsample_bytree=0.65,gamma=0,learning_rate=0.017515,max_depth=4,min_child_weight=3,n_estimators=712.2,reg_alp_2020-10-15_03-59-03/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bcae77aa</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bcae77aa_123_colsample_bytree=0.7,gamma=0,learning_rate=0.0020696,max_depth=4,min_child_weight=5,n_estimators=203.01,reg_al_2020-10-15_03-59-03/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bcb4bb56</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bcb4bb56_124_colsample_bytree=0.7,gamma=0,learning_rate=0.0010093,max_depth=4,min_child_weight=5,n_estimators=639.09,reg_al_2020-10-15_03-59-03/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bcbad1a8</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bcbad1a8_125_colsample_bytree=0.3,gamma=0,learning_rate=0.0011784,max_depth=3,min_child_weight=5,n_estimators=2511.9,reg_al_2020-10-15_03-59-03/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bcc134e4</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bcc134e4_126_colsample_bytree=0.35,gamma=0,learning_rate=0.0011008,max_depth=3,min_child_weight=5,n_estimators=1602.3,reg_a_2020-10-15_03-59-03/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bcc77f20</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bcc77f20_127_colsample_bytree=0.3,gamma=0,learning_rate=0.0035114,max_depth=3,min_child_weight=2,n_estimators=1325.9,reg_al_2020-10-15_03-59-03/error.txt</td></tr>\n",
       "<tr><td>my_xgb_bccd8622</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/xgb_hyperopt/my_xgb_bccd8622_128_colsample_bytree=0.35,gamma=0,learning_rate=0.0089158,max_depth=3,min_child_weight=2,n_estimators=239.83,reg_a_2020-10-15_03-59-04/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TuneError",
     "evalue": "('Trials did not complete', [my_xgb_ba57a242, my_xgb_ba5c05c6, my_xgb_ba5fd782, my_xgb_ba6372f2, my_xgb_ba67813a, my_xgb_ba6b9298, my_xgb_ba6fb062, my_xgb_ba717956, my_xgb_ba75293e, my_xgb_ba783f0c, my_xgb_ba7baa3e, my_xgb_ba7f7542, my_xgb_ba815312, my_xgb_ba8324e4, my_xgb_ba84e306, my_xgb_ba86beb0, my_xgb_ba888100, my_xgb_ba8a3176, my_xgb_ba8bd56c, my_xgb_ba8d753e, my_xgb_ba8f1c86, my_xgb_ba93a436, my_xgb_ba97d54c, my_xgb_ba9c3650, my_xgb_baa0c3d2, my_xgb_baa52e40, my_xgb_baa9afb0, my_xgb_baae3e0e, my_xgb_bab2a688, my_xgb_bab76cd6, my_xgb_babbf8aa, my_xgb_bac08ca8, my_xgb_bac4e820, my_xgb_bac96cd8, my_xgb_bacdfdfc, my_xgb_bad25bae, my_xgb_bad6f3c6, my_xgb_badb8fee, my_xgb_bae0772a, my_xgb_bae589ea, my_xgb_baea7734, my_xgb_baef8b20, my_xgb_baf49372, my_xgb_baf99e6c, my_xgb_bafed760, my_xgb_bb0376bc, my_xgb_bb08a484, my_xgb_bb0d9a8e, my_xgb_bb12cc0c, my_xgb_bb17fa24, my_xgb_bb1cb1ae, my_xgb_bb21a97a, my_xgb_bb26ab64, my_xgb_bb2bca54, my_xgb_bb30c9aa, my_xgb_bb36190a, my_xgb_bb3b4e8e, my_xgb_bb406194, my_xgb_bb45a0f0, my_xgb_bb4acc9c, my_xgb_bb4ff6fe, my_xgb_bb55e78a, my_xgb_bb5ba62a, my_xgb_bb610160, my_xgb_bb664738, my_xgb_bb6bc460, my_xgb_bb714ac0, my_xgb_bb769124, my_xgb_bb7be7a0, my_xgb_bb81314c, my_xgb_bb8686ba, my_xgb_bb8ba406, my_xgb_bb91299e, my_xgb_bb965aa4, my_xgb_bb9b7322, my_xgb_bba09096, my_xgb_bba5b0e4, my_xgb_bbab12fa, my_xgb_bbb0a2ec, my_xgb_bbb5ea5e, my_xgb_bbbb0e08, my_xgb_bbc080c2, my_xgb_bbc6001a, my_xgb_bbcbca86, my_xgb_bbd1880e, my_xgb_bbd7428a, my_xgb_bbdca1da, my_xgb_bbe1ddd0, my_xgb_bbe72416, my_xgb_bbecba16, my_xgb_bbf24846, my_xgb_bbf7b812, my_xgb_bbfd06fa, my_xgb_bc02707c, my_xgb_bc07da1c, my_xgb_bc0d87e6, my_xgb_bc134870, my_xgb_bc18dab0, my_xgb_bc1e982e, my_xgb_bc246d80, my_xgb_bc29ded2, my_xgb_bc2fa876, my_xgb_bc35523a, my_xgb_bc3afc44, my_xgb_bc40a874, my_xgb_bc4668d6, my_xgb_bc4c27f8, my_xgb_bc5241d8, my_xgb_bc586d88, my_xgb_bc5e3682, my_xgb_bc648faa, my_xgb_bc6b4322, my_xgb_bc71646e, my_xgb_bc7733e4, my_xgb_bc7d2420, my_xgb_bc838540, my_xgb_bc893116, my_xgb_bc8f2a94, my_xgb_bc9525de, my_xgb_bc9b6660, my_xgb_bca1cabe, my_xgb_bca8060e, my_xgb_bcae77aa, my_xgb_bcb4bb56, my_xgb_bcbad1a8, my_xgb_bcc134e4, my_xgb_bcc77f20, my_xgb_bccd8622])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTuneError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-d9e1c06bc975>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                     \u001b[0mloggers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDEFAULT_LOGGERS\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mWandbLogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                     **tune_kwargs)\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/ray/tune/tune.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, local_dir, search_alg, scheduler, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, verbose, progress_reporter, loggers, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, server_port, resume, queue_trials, reuse_actors, trial_executor, raise_on_failed_trial, ray_auto_init, run_errored_only, global_checkpoint_period, with_server, upload_dir, sync_to_cloud, sync_to_driver, sync_on_checkpoint)\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mraise_on_failed_trial\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTuneError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trials did not complete\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trials did not complete: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTuneError\u001b[0m: ('Trials did not complete', [my_xgb_ba57a242, my_xgb_ba5c05c6, my_xgb_ba5fd782, my_xgb_ba6372f2, my_xgb_ba67813a, my_xgb_ba6b9298, my_xgb_ba6fb062, my_xgb_ba717956, my_xgb_ba75293e, my_xgb_ba783f0c, my_xgb_ba7baa3e, my_xgb_ba7f7542, my_xgb_ba815312, my_xgb_ba8324e4, my_xgb_ba84e306, my_xgb_ba86beb0, my_xgb_ba888100, my_xgb_ba8a3176, my_xgb_ba8bd56c, my_xgb_ba8d753e, my_xgb_ba8f1c86, my_xgb_ba93a436, my_xgb_ba97d54c, my_xgb_ba9c3650, my_xgb_baa0c3d2, my_xgb_baa52e40, my_xgb_baa9afb0, my_xgb_baae3e0e, my_xgb_bab2a688, my_xgb_bab76cd6, my_xgb_babbf8aa, my_xgb_bac08ca8, my_xgb_bac4e820, my_xgb_bac96cd8, my_xgb_bacdfdfc, my_xgb_bad25bae, my_xgb_bad6f3c6, my_xgb_badb8fee, my_xgb_bae0772a, my_xgb_bae589ea, my_xgb_baea7734, my_xgb_baef8b20, my_xgb_baf49372, my_xgb_baf99e6c, my_xgb_bafed760, my_xgb_bb0376bc, my_xgb_bb08a484, my_xgb_bb0d9a8e, my_xgb_bb12cc0c, my_xgb_bb17fa24, my_xgb_bb1cb1ae, my_xgb_bb21a97a, my_xgb_bb26ab64, my_xgb_bb2bca54, my_xgb_bb30c9aa, my_xgb_bb36190a, my_xgb_bb3b4e8e, my_xgb_bb406194, my_xgb_bb45a0f0, my_xgb_bb4acc9c, my_xgb_bb4ff6fe, my_xgb_bb55e78a, my_xgb_bb5ba62a, my_xgb_bb610160, my_xgb_bb664738, my_xgb_bb6bc460, my_xgb_bb714ac0, my_xgb_bb769124, my_xgb_bb7be7a0, my_xgb_bb81314c, my_xgb_bb8686ba, my_xgb_bb8ba406, my_xgb_bb91299e, my_xgb_bb965aa4, my_xgb_bb9b7322, my_xgb_bba09096, my_xgb_bba5b0e4, my_xgb_bbab12fa, my_xgb_bbb0a2ec, my_xgb_bbb5ea5e, my_xgb_bbbb0e08, my_xgb_bbc080c2, my_xgb_bbc6001a, my_xgb_bbcbca86, my_xgb_bbd1880e, my_xgb_bbd7428a, my_xgb_bbdca1da, my_xgb_bbe1ddd0, my_xgb_bbe72416, my_xgb_bbecba16, my_xgb_bbf24846, my_xgb_bbf7b812, my_xgb_bbfd06fa, my_xgb_bc02707c, my_xgb_bc07da1c, my_xgb_bc0d87e6, my_xgb_bc134870, my_xgb_bc18dab0, my_xgb_bc1e982e, my_xgb_bc246d80, my_xgb_bc29ded2, my_xgb_bc2fa876, my_xgb_bc35523a, my_xgb_bc3afc44, my_xgb_bc40a874, my_xgb_bc4668d6, my_xgb_bc4c27f8, my_xgb_bc5241d8, my_xgb_bc586d88, my_xgb_bc5e3682, my_xgb_bc648faa, my_xgb_bc6b4322, my_xgb_bc71646e, my_xgb_bc7733e4, my_xgb_bc7d2420, my_xgb_bc838540, my_xgb_bc893116, my_xgb_bc8f2a94, my_xgb_bc9525de, my_xgb_bc9b6660, my_xgb_bca1cabe, my_xgb_bca8060e, my_xgb_bcae77aa, my_xgb_bcb4bb56, my_xgb_bcbad1a8, my_xgb_bcc134e4, my_xgb_bcc77f20, my_xgb_bccd8622])"
     ]
    }
   ],
   "source": [
    "NUM_SAMPLES=128\n",
    "\n",
    "start_time = datetime.now()\n",
    "print(\"%-20s %s\" % (\"Start Time\", start_time))\n",
    "\n",
    "algo = HyperOptSearch(random_state_seed=RANDOMSTATE)\n",
    "# to limit number of cores, uncomment and set max_concurrent \n",
    "# algo = ConcurrencyLimiter(algo, max_concurrent=10)\n",
    "scheduler = AsyncHyperBandScheduler()\n",
    "\n",
    "tune_kwargs = {\n",
    "    \"num_samples\": NUM_SAMPLES,\n",
    "    \"config\": {\n",
    "        \"n_estimators\": tune.loguniform(100, 10000),\n",
    "        \"max_depth\": tune.randint(0, 6),\n",
    "        'min_child_weight': tune.randint(0, 6),\n",
    "        \"subsample\": tune.quniform(0.4, 0.9, 0.05),\n",
    "        \"colsample_bytree\": tune.quniform(0.05, 0.8, 0.05),\n",
    "        \"reg_alpha\": tune.loguniform(1e-04, 1),\n",
    "        \"reg_lambda\": tune.loguniform(1e-04, 100),\n",
    "        \"gamma\": 0,\n",
    "        \"learning_rate\": tune.loguniform(0.001, 0.1),\n",
    "        \"wandb\": {\n",
    "            \"project\": \"iowa\",\n",
    "            \"api_key_file\": \"secrets/wandb.txt\",\n",
    "            \"log_config\": True\n",
    "        }    \n",
    "    }\n",
    "}\n",
    "\n",
    "analysis = tune.run(my_xgb,\n",
    "                    name=\"xgb_hyperopt\",\n",
    "                    metric=\"mse\",\n",
    "                    mode=\"min\",\n",
    "                    search_alg=algo,\n",
    "                    scheduler=scheduler,\n",
    "                    verbose=1,\n",
    "                    loggers=DEFAULT_LOGGERS + (WandbLogger, ),\n",
    "                    **tune_kwargs)\n",
    "\n",
    "end_time = datetime.now()\n",
    "print(\"%-20s %s\" % (\"Start Time\", start_time))\n",
    "print(\"%-20s %s\" % (\"End Time\", end_time))\n",
    "print(str(timedelta(seconds=(end_time-start_time).seconds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.results_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_results_df = analysis.results_df[['mse', 'date', 'time_this_iter_s',\n",
    "       'config.n_estimators', 'config.max_depth', 'config.min_child_weight', 'config.subsample',\n",
    "       'config.colsample_bytree', 'config.reg_alpha', 'config.reg_lambda', 'config.gamma',\n",
    "       'config.learning_rate']].sort_values('mse')\n",
    "analysis_results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = analysis_results_df.iloc[0]['config.max_depth']\n",
    "min_child_weight = analysis_results_df.iloc[0]['config.min_child_weight']\n",
    "subsample = analysis_results_df.iloc[0]['config.subsample']\n",
    "colsample_bytree = analysis_results_df.iloc[0]['config.colsample_bytree']\n",
    "reg_alpha = analysis_results_df.iloc[0]['config.reg_alpha']\n",
    "reg_lambda = analysis_results_df.iloc[0]['config.reg_lambda']\n",
    "reg_gamma = analysis_results_df.iloc[0]['config.gamma']\n",
    "learning_rate = analysis_results_df.iloc[0]['config.learning_rate']\n",
    "N_ESTIMATORS = analysis_results_df.iloc[0]['config.n_estimators']    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_config = {\n",
    "    'max_depth': max_depth,\n",
    "    'min_child_weight': min_child_weight,\n",
    "    'subsample': subsample,\n",
    "    'colsample_bytree': colsample_bytree,\n",
    "    'reg_alpha': reg_alpha,\n",
    "    'reg_lambda': reg_lambda,\n",
    "    'gamma': reg_gamma,\n",
    "    'learning_rate': learning_rate,\n",
    "    'n_estimators':  N_ESTIMATORS\n",
    "}\n",
    "\n",
    "xgb = XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    random_state=RANDOMSTATE,    \n",
    "    verbosity=1,\n",
    "    n_jobs=-1,\n",
    "    **best_config\n",
    ")\n",
    "print(xgb)\n",
    "\n",
    "scores = -cross_val_score(xgb, df[predictors], df[response],\n",
    "                          scoring=\"neg_root_mean_squared_error\",\n",
    "                          cv=kfolds)\n",
    "\n",
    "raw_scores = [cv_to_raw(x) for x in scores]\n",
    "print()\n",
    "print(\"Log1p CV RMSE %.06f (STD %.04f)\" % (np.mean(scores), np.std(scores)))\n",
    "print(\"Raw CV RMSE %.0f (STD %.0f)\" % (np.mean(raw_scores), np.std(raw_scores)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optuna\n",
    "\n",
    "HyperOpt is a Bayesian optimization algorithm by [Takuya Akiba, et al.](https://arxiv.org/abs/1907.10902)\n",
    "\n",
    " - [Home](https://optuna.org/)\n",
    " - [Using Optuna to Optimize XGBoost Hyperparameters](https://medium.com/optuna/using-optuna-to-optimize-xgboost-hyperparameters-63bfcdfd3407), Crissman Loomis, 2020\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optuna\n",
    "NUM_SAMPLES=256\n",
    "\n",
    "start_time = datetime.now()\n",
    "print(\"%-20s %s\" % (\"Start Time\", start_time))\n",
    "\n",
    "algo = OptunaSearch()\n",
    "# uncomment and set max_concurrent to limit number of cores\n",
    "# algo = ConcurrencyLimiter(algo, max_concurrent=10)\n",
    "scheduler = AsyncHyperBandScheduler()\n",
    "\n",
    "# identical tune args\n",
    "tune_kwargs = {\n",
    "    \"num_samples\": NUM_SAMPLES,\n",
    "    \"config\": {\n",
    "        \"n_estimators\": tune.loguniform(100, 10000),\n",
    "        \"max_depth\": tune.quniform(0, 6, 1),\n",
    "        'min_child_weight': tune.quniform(0, 6, 1),\n",
    "        \"subsample\": tune.quniform(0.4, 0.9, 0.05),\n",
    "        \"colsample_bytree\": tune.quniform(0.05, 0.8, 0.05),\n",
    "        \"reg_alpha\": tune.loguniform(1e-04, 1),\n",
    "        \"reg_lambda\": tune.loguniform(1e-04, 100),\n",
    "        \"gamma\": 0,\n",
    "        \"learning_rate\": tune.loguniform(0.001, 0.1),\n",
    "        \"wandb\": {\n",
    "            \"project\": \"iowa\",\n",
    "            \"api_key_file\": \"secrets/wandb.txt\",\n",
    "            \"log_config\": True,\n",
    "            \"name\": get_random_tag(6)\n",
    "        }           \n",
    "    }\n",
    "}\n",
    "\n",
    "analysis = tune.run(my_xgb,\n",
    "                    name=\"xgb_optuna\",\n",
    "                    metric=\"mse\",\n",
    "                    mode=\"min\",\n",
    "                    search_alg=algo,\n",
    "                    scheduler=scheduler,\n",
    "                    verbose=1,\n",
    "                    loggers=DEFAULT_LOGGERS + (WandbLogger, ),                    \n",
    "                    **tune_kwargs)\n",
    "\n",
    "end_time = datetime.now()\n",
    "print(\"%-20s %s\" % (\"Start Time\", start_time))\n",
    "print(\"%-20s %s\" % (\"End Time\", end_time))\n",
    "print(str(timedelta(seconds=(end_time-start_time).seconds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_results_df = analysis.results_df[['mse', 'date', 'time_this_iter_s',\n",
    "       'config.n_estimators', 'config.max_depth', 'config.min_child_weight', 'config.subsample',\n",
    "       'config.colsample_bytree', 'config.reg_alpha', 'config.reg_lambda', 'config.gamma',\n",
    "       'config.learning_rate']].sort_values('mse')\n",
    "analysis_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = analysis_results_df.iloc[0]['config.max_depth']\n",
    "min_child_weight = analysis_results_df.iloc[0]['config.min_child_weight']\n",
    "subsample = analysis_results_df.iloc[0]['config.subsample']\n",
    "colsample_bytree = analysis_results_df.iloc[0]['config.colsample_bytree']\n",
    "reg_alpha = analysis_results_df.iloc[0]['config.reg_alpha']\n",
    "reg_lambda = analysis_results_df.iloc[0]['config.reg_lambda']\n",
    "reg_gamma = analysis_results_df.iloc[0]['config.gamma']\n",
    "learning_rate = analysis_results_df.iloc[0]['config.learning_rate']\n",
    "N_ESTIMATORS = analysis_results_df.iloc[0]['config.n_estimators']    \n",
    "\n",
    "best_config = {\n",
    "    'max_depth': max_depth,\n",
    "    'min_child_weight': min_child_weight,\n",
    "    'subsample': subsample,\n",
    "    'colsample_bytree': colsample_bytree,\n",
    "    'reg_alpha': reg_alpha,\n",
    "    'reg_lambda': reg_lambda,\n",
    "    'gamma': reg_gamma,\n",
    "    'learning_rate': learning_rate,\n",
    "    'n_estimators':  N_ESTIMATORS\n",
    "}\n",
    "\n",
    "xgb = XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    random_state=RANDOMSTATE,    \n",
    "    verbosity=1,\n",
    "    n_jobs=-1,\n",
    "    **best_config\n",
    ")\n",
    "print(xgb)\n",
    "\n",
    "scores = -cross_val_score(xgb, df[predictors], df[response],\n",
    "                          scoring=\"neg_root_mean_squared_error\",\n",
    "                          cv=kfolds)\n",
    "\n",
    "raw_scores = [cv_to_raw(x) for x in scores]\n",
    "print()\n",
    "print(\"Log1p CV RMSE %.06f (STD %.04f)\" % (np.mean(scores), np.std(scores)))\n",
    "print(\"Raw CV RMSE %.0f (STD %.0f)\" % (np.mean(raw_scores), np.std(raw_scores)))\n",
    "raw_scores = [cv_to_raw(x) for x in scores]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM with HyperOpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_lgbm(config):\n",
    "    \n",
    "    # fix these configs \n",
    "    config['n_estimators'] = int(config['n_estimators'])   # pass float eg loguniform distribution, use int\n",
    "    config['num_leaves'] = 2 + int(config['num_leaves'])\n",
    "    \n",
    "    lgbm = LGBMRegressor(objective='regression',\n",
    "                         num_leaves=config['num_leaves'],\n",
    "                         learning_rate=config['learning_rate'],\n",
    "                         n_estimators=config['n_estimators'],\n",
    "                         max_bin=200,\n",
    "                         bagging_fraction=config['bagging_fraction'],\n",
    "                         feature_fraction=config['feature_fraction'],\n",
    "                         feature_fraction_seed=7,\n",
    "                         min_data_in_leaf=2,\n",
    "                         # these are to suppress warnings\n",
    "                         colsample_bytree=None,\n",
    "                         min_child_samples=None,\n",
    "                         subsample=None,\n",
    "                         verbose=-1,\n",
    "                         # early stopping params, maybe in fit\n",
    "                         #early_stopping_rounds=early_stopping_rounds,\n",
    "                         #valid_sets=[xgtrain, xgvalid], valid_names=['train','valid'], evals_result=evals_results\n",
    "                         #num_boost_round=num_boost_round,\n",
    "                         )\n",
    "    \n",
    "    scores = np.sqrt(-cross_val_score(lgbm, df[predictors], df[response],\n",
    "                                      scoring=\"neg_mean_squared_error\",\n",
    "                                      cv=kfolds))\n",
    "    tune.report(mse=np.mean(scores))\n",
    "    return {'mse': np.mean(scores)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tune LightGBM\n",
    "print(\"LightGBM\")\n",
    "#!conda install -y -c conda-forge lightgbm\n",
    "\n",
    "NUM_SAMPLES=256\n",
    "\n",
    "start_time = datetime.now()\n",
    "print(\"%-20s %s\" % (\"Start Time\", start_time))\n",
    "\n",
    "algo = HyperOptSearch(random_state_seed=RANDOMSTATE)\n",
    "# uncomment and set max_concurrent to limit number of cores\n",
    "# algo = ConcurrencyLimiter(algo, max_concurrent=10)\n",
    "scheduler = AsyncHyperBandScheduler()\n",
    "\n",
    "tune_kwargs = {\n",
    "    \"num_samples\": NUM_SAMPLES,\n",
    "    \"config\": {\n",
    "        \"n_estimators\": tune.loguniform(100, 10000),\n",
    "        'num_leaves': tune.randint(0, 10),\n",
    "        \"bagging_fraction\": tune.uniform(0.5, 0.8),\n",
    "        \"feature_fraction\": tune.uniform(0.01, 0.8),\n",
    "        \"learning_rate\": tune.loguniform(0.001, 0.1),\n",
    "        \"wandb\": {\n",
    "            \"project\": \"iowa\",\n",
    "            \"api_key_file\": \"secrets/wandb.txt\",\n",
    "            \"log_config\": True\n",
    "        }    \n",
    "    }\n",
    "}\n",
    "\n",
    "analysis = tune.run(my_lgbm,\n",
    "                    name=\"lgbm_hyperopt\",\n",
    "                    metric=\"mse\",\n",
    "                    mode=\"min\",\n",
    "                    search_alg=algo,\n",
    "                    scheduler=scheduler,\n",
    "                    verbose=1,\n",
    "                    loggers=DEFAULT_LOGGERS + (WandbLogger, ),\n",
    "                    **tune_kwargs)\n",
    "\n",
    "end_time = datetime.now()\n",
    "print(\"%-20s %s\" % (\"Start Time\", start_time))\n",
    "print(\"%-20s %s\" % (\"End Time\", end_time))\n",
    "print(str(timedelta(seconds=(end_time-start_time).seconds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_results_df = analysis.results_df[['mse', 'date', 'time_this_iter_s',\n",
    "       'config.n_estimators', 'config.num_leaves', 'config.bagging_fraction',\n",
    "       'config.feature_fraction', 'config.learning_rate']].sort_values('mse')\n",
    "analysis_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_leaves = analysis_results_df.iloc[0]['config.num_leaves']\n",
    "bagging_fraction = analysis_results_df.iloc[0]['config.bagging_fraction']\n",
    "feature_fraction = analysis_results_df.iloc[0]['config.feature_fraction']\n",
    "learning_rate = analysis_results_df.iloc[0]['config.learning_rate']\n",
    "N_ESTIMATORS = analysis_results_df.iloc[0]['config.n_estimators']    \n",
    "\n",
    "best_config = {\n",
    "    'num_leaves': num_leaves,\n",
    "    'bagging_fraction': bagging_fraction,\n",
    "    'feature_fraction': feature_fraction,\n",
    "    'learning_rate': learning_rate,\n",
    "    'n_estimators':  N_ESTIMATORS\n",
    "}\n",
    "\n",
    "lgbm = LGBMRegressor(objective='regression',\n",
    "                     num_leaves=best_config['num_leaves'],\n",
    "                     learning_rate=best_config['learning_rate'],\n",
    "                     n_estimators=best_config['n_estimators'],\n",
    "                     max_bin=200,\n",
    "                     bagging_fraction=best_config['bagging_fraction'],\n",
    "                     feature_fraction=best_config['feature_fraction'],\n",
    "                     feature_fraction_seed=7,\n",
    "                     min_data_in_leaf=2,\n",
    "                     verbose=-1,\n",
    "                     # early stopping params, maybe in fit\n",
    "                     #early_stopping_rounds=early_stopping_rounds,\n",
    "                     #valid_sets=[xgtrain, xgvalid], valid_names=['train','valid'], evals_result=evals_results\n",
    "                     #num_boost_round=num_boost_round,\n",
    "                     )\n",
    " \n",
    "print(lgbm)\n",
    "\n",
    "scores = -cross_val_score(lgbm, df[predictors], df[response],\n",
    "                          scoring=\"neg_root_mean_squared_error\",\n",
    "                          cv=kfolds)\n",
    "\n",
    "raw_scores = [cv_to_raw(x) for x in scores]\n",
    "print()\n",
    "print(\"Log1p CV RMSE %.06f (STD %.04f)\" % (np.mean(scores), np.std(scores)))\n",
    "print(\"Raw CV RMSE %.0f (STD %.0f)\" % (np.mean(raw_scores), np.std(raw_scores)))\n",
    "raw_scores = [cv_to_raw(x) for x in scores]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tune LightGBM\n",
    "print(\"LightGBM\")\n",
    "#!conda install -y -c conda-forge lightgbm\n",
    "\n",
    "NUM_SAMPLES=256\n",
    "\n",
    "start_time = datetime.now()\n",
    "print(\"%-20s %s\" % (\"Start Time\", start_time))\n",
    "\n",
    "algo = OptunaSearch()\n",
    "# uncomment and set max_concurrent to limit number of cores\n",
    "# algo = ConcurrencyLimiter(algo, max_concurrent=10)\n",
    "scheduler = AsyncHyperBandScheduler()\n",
    "\n",
    "tune_kwargs = {\n",
    "    \"num_samples\": NUM_SAMPLES,\n",
    "    \"config\": {\n",
    "        \"n_estimators\": tune.loguniform(100, 10000),\n",
    "        'num_leaves': tune.randint(0, 10),\n",
    "        \"bagging_fraction\": tune.uniform(0.5, 0.8),\n",
    "        \"feature_fraction\": tune.uniform(0.01, 0.8),\n",
    "        \"learning_rate\": tune.loguniform(0.001, 0.1),\n",
    "        \"wandb\": {\n",
    "            \"project\": \"iowa\",\n",
    "            \"api_key_file\": \"secrets/wandb.txt\",\n",
    "            \"log_config\": True\n",
    "        }    \n",
    "    }\n",
    "}\n",
    "\n",
    "analysis = tune.run(my_lgbm,\n",
    "                    name=\"lgbm_hyperopt\",\n",
    "                    metric=\"mse\",\n",
    "                    mode=\"min\",\n",
    "                    search_alg=algo,\n",
    "                    scheduler=scheduler,\n",
    "                    verbose=1,\n",
    "                    loggers=DEFAULT_LOGGERS + (WandbLogger, ),\n",
    "                    **tune_kwargs)\n",
    "\n",
    "end_time = datetime.now()\n",
    "print(\"%-20s %s\" % (\"Start Time\", start_time))\n",
    "print(\"%-20s %s\" % (\"End Time\", end_time))\n",
    "print(str(timedelta(seconds=(end_time-start_time).seconds)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_results_df = analysis.results_df[['mse', 'date', 'time_this_iter_s',\n",
    "       'config.n_estimators', 'config.num_leaves', 'config.bagging_fraction',\n",
    "       'config.feature_fraction', 'config.learning_rate']].sort_values('mse')\n",
    "analysis_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_leaves = analysis_results_df.iloc[0]['config.num_leaves']\n",
    "bagging_fraction = analysis_results_df.iloc[0]['config.bagging_fraction']\n",
    "feature_fraction = analysis_results_df.iloc[0]['config.feature_fraction']\n",
    "learning_rate = analysis_results_df.iloc[0]['config.learning_rate']\n",
    "N_ESTIMATORS = analysis_results_df.iloc[0]['config.n_estimators']    \n",
    "\n",
    "best_config = {\n",
    "    'num_leaves': num_leaves,\n",
    "    'bagging_fraction': bagging_fraction,\n",
    "    'feature_fraction': feature_fraction,\n",
    "    'learning_rate': learning_rate,\n",
    "    'n_estimators':  N_ESTIMATORS\n",
    "}\n",
    "\n",
    "lgbm = LGBMRegressor(objective='regression',\n",
    "                     num_leaves=best_config['num_leaves'],\n",
    "                     learning_rate=best_config['learning_rate'],\n",
    "                     n_estimators=best_config['n_estimators'],\n",
    "                     max_bin=200,\n",
    "                     bagging_fraction=best_config['bagging_fraction'],\n",
    "                     feature_fraction=best_config['feature_fraction'],\n",
    "                     feature_fraction_seed=7,\n",
    "                     min_data_in_leaf=2,\n",
    "                     verbose=-1,\n",
    "                     # early stopping params, maybe in fit\n",
    "                     #early_stopping_rounds=early_stopping_rounds,\n",
    "                     #valid_sets=[xgtrain, xgvalid], valid_names=['train','valid'], evals_result=evals_results\n",
    "                     #num_boost_round=num_boost_round,\n",
    "                     )\n",
    " \n",
    "print(lgbm)\n",
    "\n",
    "scores = -cross_val_score(lgbm, df[predictors], df[response],\n",
    "                          scoring=\"neg_root_mean_squared_error\",\n",
    "                          cv=kfolds)\n",
    "\n",
    "raw_scores = [cv_to_raw(x) for x in scores]\n",
    "print()\n",
    "print(\"Log1p CV RMSE %.06f (STD %.04f)\" % (np.mean(scores), np.std(scores)))\n",
    "print(\"Raw CV RMSE %.0f (STD %.0f)\" % (np.mean(raw_scores), np.std(raw_scores)))\n",
    "raw_scores = [cv_to_raw(x) for x in scores]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ray Cluster\n",
    "\n",
    "- Clusters are defined in `ray1.1.yaml`\n",
    "- boto3 and AWS CLI configured credentials are used, so [install and configure AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html)\n",
    "- Edit `ray1.1.yaml` file with your region, availability zone, subnet, imageid information\n",
    "    - to get those variables launch the latest Deep Learning AMI (Ubuntu 18.04) Version 35.0 into a small instance in your favorite region/zone\n",
    "    - test that it works\n",
    "    - note those 4 variables: region, availability zone, subnet, AMI imageid\n",
    "    - terminate the instance and edit `ray1.1.yaml` accordingly\n",
    "    - in future you can create your own image with everything pre-installed and specify its AMI imageid, instead of using the generic image and installing everything at launch.\n",
    "- To run the cluster: \n",
    "`ray up ray1.1.yaml`\n",
    "    - Creates head instance using image specified.\n",
    "    - Installs ray and related requirements\n",
    "    - Clones this Iowa repo\n",
    "    - Launches worker nodes per auto-scaling parameters (currently we fix the number of nodes because we're not benching the time the cluster will take to auto-scale)\n",
    "- After cluster starts you can check AWS console and note that several instances launched.\n",
    "- Check `ray monitor ray1.1.yaml` for any error messages\n",
    "- Run Jupyter on the cluster with port forwarding\n",
    " `ray exec ray1.1.yaml --port-forward=8899 'source ~/anaconda3/bin/activate tensorflow_p36 && jupyter notebook --port=8899'`\n",
    "- Open the notebook on the generated URL e.g. http://localhost:8899/?token=5f46d4355ae7174524ba71f30ef3f0633a20b19a204b93b4\n",
    "- Make sure to hoose the default kernel to make sure it runs in the conda environment with all installs\n",
    "- Make sure to use the ray.init() command given in the startup messages.\n",
    "- You can also run a terminal on the head node of the cluster with\n",
    " `ray attach /Users/drucev/projects/iowa/ray1.1.yaml`\n",
    "- You can also ssh explicitly with the IP address and the generated private key\n",
    " `ssh -o IdentitiesOnly=yes -i ~/.ssh/ray-autoscaler_1_us-east-1.pem ubuntu@54.161.200.54`\n",
    "- run port forwarding to the Ray dashboard with   \n",
    "`ray dashboard ray1.1.yaml`\n",
    "and then open\n",
    " http://localhost:8265/\n",
    "- the cluster will incur AWS charges so `ray down ray1.1.yaml` when complete\n",
    "- Other than connecting to Ray cluster, runs identically\n",
    "- see hyperparameter_optimization.ipynb, separated out so each notebook can be run end-to-end with/without cluster setup\n",
    "\n",
    "see https://docs.ray.io/en/latest/cluster/launcher.html for additional info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concluding remarks\n",
    "\n",
    "We observe a modest but non-negligeable improvement in the target metric with a less manual process vs. sequential tuning.\n",
    "hyoperopt.optuna yes\n",
    "ray maybe, depends on further work compare w/optuna and hyperopt online. depends on integration between ray, early stopping and search algo\n",
    "cluster , in general no need and costs add up. MacBook Pro w/16 threads and desktop with GPU are plenty.\n",
    "\n",
    "I intend to use HyperOpt and Optuna for xgboost going forward, no more grid search for me! In every case I've applied them, I've gotten at least a small improvement in the best metrics I found using grid search methods. Additionally, it's fire and forget (although with a little elbow grease the 4-pass sequential grid search could be made fire and forget.)\n",
    "\n",
    "These two algorithms seem to be the most popular but I may try the other algos systematically. \n",
    "\n",
    "I am surprised that Elasticnet, i.e. regularized linear regression outperforms boosting. \n",
    "This dataset has been heavily engineered so that linear methods work well. Predictors were chosen using lasso/elasticnet and we used log and Box-Cox transforms to force predictors to follow assumptions of least-squares.  \n",
    "\n",
    "This tends to validate one of the [critiques of machine learning](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3624052), that the most powerful ML methods don't necessarily converge all the way to the best solution. If you have a ground truth that is linear plus noise, a complex XGBoost or neural network algorithm should get arbitrarily close to the closed-form optimal solution but will never match it exactly. XGBoost is piecewise constant and the complex neural network is subject to the vagaries of stochastic gradient descent. \n",
    "\n",
    "But Elasticnet with L1 + L2 regularization plus gradient descent and hyperparameter optimization is still machine learning. It's simply the form best matched to the problem. In the real world where things don't match assumptions of least-squares, boosting performs extremely well. And even on this dataset, engineered for the linear models, SVR and KernelRidge performed better than Elasticnet (not shown) and ensembling Elasticnet with XGBoost, LightGBM, SVR, neural networks worked best of all. \n",
    "\n",
    "To paraphrase Casey Stengel, clever feature engineering will always outperform clever model algorithms and vice-versa<sup>*</sup>. But improving your hyperparameters with these best practices will always improve your results.\n",
    "\n",
    "<sup>*</sup>This is not intended to make sense.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tune LightGBM\n",
    "print(\"LightGBM\")\n",
    "#!conda install -y -c conda-forge lightgbm\n",
    "\n",
    "from ray.tune.suggest.ax import AxSearch\n",
    "\n",
    "NUM_SAMPLES=256\n",
    "\n",
    "start_time = datetime.now()\n",
    "print(\"%-20s %s\" % (\"Start Time\", start_time))\n",
    "\n",
    "algo = AxSearch(\n",
    "#    max_concurrent=4,\n",
    "    metric=\"mse\",\n",
    "    mode=\"min\",\n",
    ")\n",
    "\n",
    "# uncomment and set max_concurrent to limit number of cores\n",
    "# algo = ConcurrencyLimiter(algo, max_concurrent=10)\n",
    "scheduler = AsyncHyperBandScheduler()\n",
    "    \n",
    "tune_kwargs = {\n",
    "    \"num_samples\": NUM_SAMPLES,\n",
    "    \"config\": {\n",
    "        \"n_estimators\": tune.loguniform(100, 10000),\n",
    "        'num_leaves': tune.randint(0, 10),\n",
    "        \"bagging_fraction\": tune.uniform(0.5, 0.8),\n",
    "        \"feature_fraction\": tune.uniform(0.01, 0.8),\n",
    "        \"learning_rate\": tune.loguniform(0.001, 0.1),\n",
    "        \"wandb\": {\n",
    "            \"project\": \"iowa\",\n",
    "            \"api_key_file\": \"secrets/wandb.txt\",\n",
    "            \"log_config\": True\n",
    "        }    \n",
    "    }\n",
    "}\n",
    "\n",
    "analysis = tune.run(my_lgbm,\n",
    "                    name=\"lgbm_lightgbm\",\n",
    "                    metric=\"mse\",\n",
    "                    mode=\"min\",\n",
    "                    search_alg=algo,\n",
    "                    scheduler=scheduler,\n",
    "                    verbose=1,\n",
    "                    loggers=DEFAULT_LOGGERS + (WandbLogger, ),\n",
    "                    **tune_kwargs)\n",
    "\n",
    "end_time = datetime.now()\n",
    "print(\"%-20s %s\" % (\"Start Time\", start_time))\n",
    "print(\"%-20s %s\" % (\"End Time\", end_time))\n",
    "print(str(timedelta(seconds=(end_time-start_time).seconds)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune import suggest\n",
    "dir(suggest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.suggest.dragonfly import DragonflySearch\n",
    "\n",
    "start_time = datetime.now()\n",
    "print(\"%-20s %s\" % (\"Start Time\", start_time))\n",
    "\n",
    "algo = DragonflySearch(\n",
    "    optimizer=\"bandit\",\n",
    "    domain=\"euclidean\",\n",
    "    # space=space,  # If you want to set the space manually\n",
    ")\n",
    "\n",
    "scheduler = AsyncHyperBandScheduler()\n",
    "\n",
    "NUM_SAMPLES=256\n",
    "\n",
    "tune_kwargs = {\n",
    "    \"num_samples\": NUM_SAMPLES,\n",
    "    \"config\": {\n",
    "        \"n_estimators\": tune.loguniform(100, 10000),\n",
    "        'num_leaves': 8,\n",
    "        \"bagging_fraction\": tune.uniform(0.5, 0.8),\n",
    "        \"feature_fraction\": tune.uniform(0.01, 0.8),\n",
    "        \"learning_rate\": tune.loguniform(0.001, 0.1),\n",
    "        \"wandb\": {\n",
    "            \"project\": \"iowa\",\n",
    "            \"api_key_file\": \"secrets/wandb.txt\",\n",
    "            \"log_config\": True\n",
    "        }    \n",
    "    }\n",
    "}\n",
    "\n",
    "analysis = tune.run(my_lgbm,\n",
    "                    name=\"lgbm_dragonfly\",\n",
    "                    metric=\"mse\",\n",
    "                    mode=\"min\",\n",
    "                    search_alg=algo,\n",
    "                    scheduler=scheduler,\n",
    "                    verbose=1,\n",
    "                    loggers=DEFAULT_LOGGERS + (WandbLogger, ),\n",
    "                    **tune_kwargs)\n",
    "\n",
    "end_time = datetime.now()\n",
    "print(\"%-20s %s\" % (\"Start Time\", start_time))\n",
    "print(\"%-20s %s\" % (\"End Time\", end_time))\n",
    "print(str(timedelta(seconds=(end_time-start_time).seconds)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.suggest.skopt import SkOptSearch\n",
    "\n",
    "start_time = datetime.now()\n",
    "print(\"%-20s %s\" % (\"Start Time\", start_time))\n",
    "\n",
    "\n",
    "algo = SkOptSearch(\n",
    "        # parameter_names=space.keys(),  # If you want to set the space\n",
    "        # parameter_ranges=space.values(), # If you want to set the space\n",
    "        # points_to_evaluate=previously_run_params,\n",
    "        # evaluated_rewards=known_rewards\n",
    ")\n",
    "\n",
    "scheduler = AsyncHyperBandScheduler()\n",
    "\n",
    "NUM_SAMPLES=256\n",
    "\n",
    "tune_kwargs = {\n",
    "    \"num_samples\": NUM_SAMPLES,\n",
    "    \"config\": {\n",
    "        \"n_estimators\": tune.loguniform(100, 10000),\n",
    "        'num_leaves': 8,\n",
    "        \"bagging_fraction\": tune.uniform(0.5, 0.8),\n",
    "        \"feature_fraction\": tune.uniform(0.01, 0.8),\n",
    "        \"learning_rate\": tune.loguniform(0.001, 0.1),\n",
    "        \"wandb\": {\n",
    "            \"project\": \"iowa\",\n",
    "            \"api_key_file\": \"secrets/wandb.txt\",\n",
    "            \"log_config\": True\n",
    "        }    \n",
    "    }\n",
    "}\n",
    "\n",
    "analysis = tune.run(my_lgbm,\n",
    "                    name=\"lgbm_skopt\",\n",
    "                    metric=\"mse\",\n",
    "                    mode=\"min\",\n",
    "                    search_alg=algo,\n",
    "                    scheduler=scheduler,\n",
    "                    verbose=1,\n",
    "                    loggers=DEFAULT_LOGGERS + (WandbLogger, ),\n",
    "                    **tune_kwargs)\n",
    "\n",
    "end_time = datetime.now()\n",
    "print(\"%-20s %s\" % (\"Start Time\", start_time))\n",
    "print(\"%-20s %s\" % (\"End Time\", end_time))\n",
    "print(str(timedelta(seconds=(end_time-start_time).seconds)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.suggest.bohb import TuneBOHB\n",
    "\n",
    "start_time = datetime.now()\n",
    "print(\"%-20s %s\" % (\"Start Time\", start_time))\n",
    "\n",
    "algo = TuneBOHB(max_concurrent=4, metric=\"mse\", mode=\"min\")\n",
    "\n",
    "scheduler = HyperBandForBOHB(\n",
    "    time_attr=\"training_iteration\",\n",
    "    metric=\"mse\",\n",
    "    mode=\"min\",\n",
    "    max_t=100)\n",
    "\n",
    "NUM_SAMPLES=256\n",
    "\n",
    "tune_kwargs = {\n",
    "    \"num_samples\": NUM_SAMPLES,\n",
    "    \"config\": {\n",
    "        \"n_estimators\": tune.loguniform(100, 10000),\n",
    "        'num_leaves': 8,\n",
    "        \"bagging_fraction\": tune.uniform(0.5, 0.8),\n",
    "        \"feature_fraction\": tune.uniform(0.01, 0.8),\n",
    "        \"learning_rate\": tune.loguniform(0.001, 0.1),\n",
    "        \"wandb\": {\n",
    "            \"project\": \"iowa\",\n",
    "            \"api_key_file\": \"secrets/wandb.txt\",\n",
    "            \"log_config\": True\n",
    "        }    \n",
    "    }\n",
    "}\n",
    "\n",
    "analysis = tune.run(my_lgbm,\n",
    "                    name=\"lgbm_bohb\",\n",
    "                    metric=\"mse\",\n",
    "                    mode=\"min\",\n",
    "                    search_alg=algo,\n",
    "                    scheduler=scheduler,\n",
    "                    verbose=1,\n",
    "                    loggers=DEFAULT_LOGGERS + (WandbLogger, ),\n",
    "                    **tune_kwargs)\n",
    "\n",
    "end_time = datetime.now()\n",
    "print(\"%-20s %s\" % (\"Start Time\", start_time))\n",
    "print(\"%-20s %s\" % (\"End Time\", end_time))\n",
    "print(str(timedelta(seconds=(end_time-start_time).seconds)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.suggest.nevergrad import NevergradSearch\n",
    "import ray.tune.suggest.nevergrad as ng\n",
    "\n",
    "start_time = datetime.now()\n",
    "print(\"%-20s %s\" % (\"Start Time\", start_time))\n",
    "\n",
    "algo = NevergradSearch(\n",
    "    optimizer=ng.optimizers.OnePlusOne,\n",
    "    # space=space,  # If you want to set the space manually\n",
    ")\n",
    "\n",
    "scheduler = AsyncHyperBandScheduler()\n",
    "\n",
    "NUM_SAMPLES=256\n",
    "\n",
    "tune_kwargs = {\n",
    "    \"num_samples\": NUM_SAMPLES,\n",
    "    \"config\": {\n",
    "        \"n_estimators\": tune.loguniform(100, 10000),\n",
    "        'num_leaves': 8,\n",
    "        \"bagging_fraction\": tune.uniform(0.5, 0.8),\n",
    "        \"feature_fraction\": tune.uniform(0.01, 0.8),\n",
    "        \"learning_rate\": tune.loguniform(0.001, 0.1),\n",
    "        \"wandb\": {\n",
    "            \"project\": \"iowa\",\n",
    "            \"api_key_file\": \"secrets/wandb.txt\",\n",
    "            \"log_config\": True\n",
    "        }    \n",
    "    }\n",
    "}\n",
    "\n",
    "analysis = tune.run(my_lgbm,\n",
    "                    name=\"lgbm_bohb\",\n",
    "                    metric=\"mse\",\n",
    "                    mode=\"min\",\n",
    "                    search_alg=algo,\n",
    "                    scheduler=scheduler,\n",
    "                    verbose=1,\n",
    "                    loggers=DEFAULT_LOGGERS + (WandbLogger, ),\n",
    "                    **tune_kwargs)\n",
    "\n",
    "end_time = datetime.now()\n",
    "print(\"%-20s %s\" % (\"Start Time\", start_time))\n",
    "print(\"%-20s %s\" % (\"End Time\", end_time))\n",
    "print(str(timedelta(seconds=(end_time-start_time).seconds)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
